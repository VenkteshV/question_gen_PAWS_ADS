iir_1	documents,collections,database-style searching,unstructured data,relational database,latent linguistic structure,clustering,web search,indexing,personal information retrieval,text classification,term-document matrix,centralized file systems,inverted index,rankings,terms,personal search
iir_1_1	information retrieval,document retrieval,grepping,wildcard pattern matching,terms,querying,retrieval system,corpus,ad hoc retrieval,information need,relevance,precision,recall,inverted index,inverted file,dictionary,lexicon,vocabulary,document collections,ranked retrieval,effectiveness,term-document matrix,postings list,inverted list,binary term-document incidence matrix,ir,relevant documents
iir_1_2	inverted index,normalized tokens,document identifier,sorting,documents,dictionary,document frequency,query,ranked retrieval models,data structure,document collection,index construction,ad hoc text search,postings list,indexing terms,sort-based indexing,efficiency,singly linked lists,skip lists,explicit pointers
iir_1_3	simple conjunctive query,dictionary,intersection,postings lists,documents,merge algorithm,query processing,terms,binary searches,queries,querying,inverted index,document frequency,pointers,indexing,collection,merging
iir_1_4	ranked retrieval models,free text queries,free text querying,documents,terms,proximity operator,information needs,phrase search,inverted index,dictionary,postings lists,term frequency,unstructured search,vector space model,query optimization,ad hoc searching,wildcard query,recall,precision,query models,index structures,unstructured information,indexing
iir_2_1	postings lists,tokens,tokenization,terms,documents,linguistic preprocessing,proximity queries,character sequence decoding,heuristic methods,document unit,indexing granularity,precision,recall,document collection,vocabulary,inverted index construction,machine learning classification problem,querying,multibyte encoding schemes,metadata,binary representation,markup,character encodings,digital representation,relevant information,implicit proximity search,index granularity,information needs,usage patterns
iir_2_2	vocabulary,tokenizer,documents,normalization processes,database,hyphenation,hyphens,phrase queries,phrase query,retrieval systems,compounds,word segmentation,conditional random fields,segmentations,word-based indexing,stop words,collection frequency,stop lists,document rankings,postings lists,normalizations,equivalence classes,equivalence classing,case-folding,truecasing,document collections,language identification classifier,stemming,lemmatization,lemmatizer,recall,precision,type/token distinction,ir,semantic identifiers,tokenization phase,free text queries,linguistic processing,character k-grams,phrase searches,unnormalized tokens,morphological analysis,document unit,index terms,classifiers,classification problem,phrase index,compound-splitter module,machine learning sequence models,keyword searches,web search engine,compression techniques,impact-sorted indexes,token list,query expansion list,query term,query expansion dictionary,phonetic equivalents,stemmers,linguistic morphology
iir_2_3	postings list intersection,skip pointers,postings lists,skip lists,query terms,search engine,skip list pointer,skip span,index construction,postings list data structures,inverted index,encoding,disk postings list
iir_2_4	phrase query,ranked retrieval,biword indexes,documents,part-of-speech-tagging,phrase index,vocabulary,positional indexes,token index,merge operation,document collection,next word index,implicit phrase queries,term proximity weighting,biwords,biword indexing model,tokenize,tokens,biword dictionary,term frequency,inverted index,k word proximity searches,postings intersection,positional postings lists,search engines,query terms,part-of-speech patterns,non-positional index,compressed positional index
iir_3_1	dictionaries,dictionary,inverted indexes,proximity queries,data structures,wildcard query,documents,query terms,standard inverted index,postings list,hashing,search trees,keys,search engines,pointers,binary tree,binary search tree,document collection,vocabulary terms,hash function,balance property,binary test,vocabulary lookup operation,hash collisions,node,children,root,sub-trees,rebalancing,branch,disk blocks
iir_3_2	documents,stemming,search engine,trailing wildcard query,trailing wildcard queries,search tree,leading wildcard queries,inverted index,vocabulary terms,permuterm index,permuterm vocabulary,rotated wildcard query,document retrieval,k-gram indexes,postings list,k-grams,post-filtering,search string,query term,root-to-leaf path,dictionary terms,string-matching operation
iir_3_3	documents,edit distance,k-gram overlap,spelling correction algorithms,queries,search engines,isolated-term correction,context-sensitive correction,edit operations,dynamic programming algorithm,decoding problem,permuterm index,dictionary terms,k-gram indexes,vocabulary terms,query terms,k-grams,postings,bigrams,coefficient,spell-correction,spelling suggestion,collection,phonetic similarity,spelling correction problem,query string,query logs,biwords
iir_3_4	phonetic correction,tolerant retrieval,phonetic hashing,inverted index,vocabulary terms,soundex codes,soundex index,soundex algorithms,soundex map,equivalence classes,searches,query terms,soundex match,string
iir_4_1	index construction,inverted index,indexer,indexes,indexing algorithms,blocked sort-based indexing,static collections,single-pass in-memory indexing,vocabulary,computer clusters,dynamic indexing,ranked retrieval,documents,local file system,content management systems,databases,seek time,buffer,system bus,sort-based indexing algorithm,enterprise search,ir,clock cycles,megabytes,web search,http,information retrieval,main memory caching,mb,blocks,decompression algorithm,gigabytes
iir_4_2	nonpositional index,dominant key,secondary key,terms,postings lists,documents,collections,vocabulary,inverted index,index construction algorithms,tokens,postings file,external sorting algorithm,random disk seeks,blocked sort-based indexing algorithm,data structure,merge,document frequency,strings,secondary storage,multipass algorithms,compression techniques,priority queue,disk space,bytes,main memory,blocks,block files,read buffers,write buffer,time complexity
iir_4_3	single-pass in-memory indexing,scaling properties,data structures,collections,documents,tokens,token stream,postings lists,dynamically constructed index,dictionary terms,time complexity,hash,inverted index,block,disk space,algorithm,index construction
iir_4_4	collections,web index,distributed indexing algorithms,term-partitioned index,document-partitioned index,distributed computing,master node,key-value pairs,terms,map phase,parsers,segment files,reduce phases,indexing system,inverters,term frequencies,term frequency,inverted index,local disk,computer clusters,documents,single-machine indexing,sorted postings list,crawling,search engines,distributed index construction method,memory,chunks,worker nodes,mb,mapping splits,keys,network traffic,distributed environment
iir_4_5	dynamic indexing,document collection,collections,documents,terms,dictionary,postings lists,invalidation bit vector,merging operation,vocabulary,time complexity,logarithmic merging,in-memory auxiliary index,binomial heap data structure,inverted indexes,index maintenance,query processing,search engines,querying,main index,disk seeks,one-file-per-postings-list scheme,file systems,auxiliary index scheme,spelling correction algorithm,reconstruction-from-scratch strategy,index construction,search system
iir_4_6	nonpositional indexes,ranked retrieval,documents,postings lists,impact-sorted index,inverted index,access control lists,user-document matrix,tokens,algorithms,query processing,retrieval systems,information retrieval system,incremental indexing,user's access list,file system,terms
iir_5_1	index compression,inverted index,data structures,compression techniques,search systems,query term,one-term query,disk seek,decompression algorithms,uncompressed postings lists,disk-to-memory transfer,terms,collections,nonpositional postings,rule of 30,tokens,lossy compression,case folding,stemming,stop word elimination,vector space model,dimensionality reduction techniques,latent semantic indexing,short queries,vocabulary,dictionary compression,information retrieval system,ir,compression ratios,caching,cache,decompression methods,blocked storage,postings file,variable byte encoding,γ encoding,compressed index,web search,documents,collection frequency,power law,main memory,dictionary-as-a-string method,nonpositional index,stop words,stop list,bits,lemmatizer,lossy methods,log-log space
iir_5_2	dictionary data structures,compression ratios,dictionaries,disk seeks,queries,main memory,collections,enterprise search server,vocabulary,search systems,string,gigabytes,binary search,term pointers,front coding,minimal perfect hashing,postings file,onboard computers,document frequency,postings list,dictionary terms,postings pointer,term list,query evaluation,multiterabyte collection,documents,mb,linear search,uncompressed dictionary,perfect hashes,perfect hash function,compression scheme
iir_5_3	documents,tokens,postings lists,bytes,uncompressed postings file,bits,terms,variable encoding method,bytewise compression,bitwise compression,continuation bit,nibbles,bit-level code,unary code,γ encoding,entropy,prefix free,parameter free,dynamic indexing,vocabulary,compression techniques,term incidence matrix,bit-level operations,query processing,variable byte codes,disk space,index compression,cache,memory,gaps,decoding,pseudocode,uncompressed index,bit manipulation,compression ratios,bit-level encodings,γ codes,optimal encoding length,parameter-free code,term distribution model,collection frequency,compressed inverted index,case folding,encoded gap,32-bit words,16-bit words,4-bit words,variable-length encoding,offset,indexes,harmonic number,γ compression,term frequencies,space efficiency
iir_6	vector space model,indexes,documents,document collections,search engine,vector space scoring,query terms,free text queries,metadata,term-weighting
iir_6_1	parametric indexes,zone index,documents,terms,metadata,fields,postings intersections,queries,parametric search,search engine,zones,inverted indexes,weighted zone scoring,weighted zone scores,query term,accumulators,training examples,machine-learned relevance,document-query pair,field's dictionary,vocabulary stems,main memory,postings traversal,array,information retrieval,machine learning,learned scores,collection,training document,training query
iir_6_2	term frequency,term weights,query terms,documents,free text query,search interface,bag of words model,stop words,inverse document frequency,collection frequency,dictionary terms,overlap score measure,scoring,zone,document-level statistic,tf-idf weighting scheme,ranking,search operators,collection-wide statistic,cf
iir_6_3	document classification,vector space scoring,vector space scores,document collection,tf-idf weighting scheme,cosine similarity,cosine similarities,dot products,inner product,documents,queries,query terms,tf-idf weights,inverted index,free text query,accumulators,inverse document frequency,postings lists,document-at-a-time scoring,term-at-a-time scoring,information retrieval operations,term-document matrix,bag of words,query vector,dictionary term,weighting schemes,term frequencies,term frequency,search engines,vocabulary,array,priority queue data structure,heap,concurrent postings traversal
iir_6_4	documents,tf-idf,term frequencies,wf-idf,tf weights,smoothing term,classification,term weightings,information retrieval systems,vector space scoring method,query vector,document vectors,document frequency,document frequencies,log-weighted term frequency,cosine normalization,idf weighting,query terms,document collection,queries,probability of relevance,normalized term frequency,stop word,outlier term,pivot length,maximum tf normalization,query weighting schemes,text retrieval system,information needs,normalized document,relevance judgments,pivoted document length normalization,pivoted length normalization,linear pivoted normalization,machine learning based scoring approach
iir_7_1	term weighting,documents,vector space models,cosine scoring algorithm,search engines,cosine scores,indexes,query term proximity,ranking,collection,cosine similarity,inverted index,query terms,heap,pointers,phrase queries,index elimination,multi-term query,postings traversal,low-idf terms,stop words,fancy lists,top docs,tf-idf weighting,index construction,static quality scores,query-dependent scores,postings lists,global champion lists,impact ordering,cluster pruning,document vectors,free text queries,query vector,dictionary term,postings intersection algorithm,scoring function,term frequency,clustering,query operators,document unit vector,web search,high lists,concurrent traversal,document-at-a-time scoring,query processing
iir_7_2	vector spaces,vector space retrieval,query operators,vector space queries,vector space query,tiered indexes,index elimination,documents,documents,champion lists,tf,query terms,proximity-weighted scoring functions,cosine similarity,machine learning,search interfaces,query parser,phrase query,phrase queries,vector space scoring,collections,machine-learned scoring,tokens,tokenization,parsed document,spelling correction,tolerant retrieval,machine-learned ranking,search system,postings,free text queries,stop words,web search engines,static quality,proximity weighting,aggregate scoring function,document collection,field queries,stemming,document cache,metadata,positional indexes,indexers,indexers,free text user query,retrieval operators,query string,query parsing layer,zones,linguistic processing,field indexes
iir_7_3	vector space score,query operators,vector space model,free text queries,free text query,vector space scoring model,indexes,retrieval methods,documents,wildcard queries,phrase queries,phrase query,query terms,retrieved document,web search engines,document vector,evidence accumulation,postings,dictionary,biwords,idf,free text retrieval,conjunctive query,vector space index,p-norms,vector space retrieval,vector space queries,vector space query,wildcard operator,query vector,retrieval algorithms,query parsing
iir_8_1	information needs,queries,query-document pair,relevance,documents,binary classification,gold standard,ground truth,development test collections,web search engine,information retrieval system evaluation,ad hoc information retrieval effectiveness,test document collection
iir_8_2	test collections,ad hoc information retrieval system evaluation,information retrieval effectiveness,information needs,test document collections,documents,cross-language information retrieval,clef,text classification collection,queries,(query, document) pairs,topics,ntcir
iir_8_3	recall,documents,query,queries,accuracy,classifications,information retrieval system,two-class classifier,machine learning classification problems,information retrieval problems,true positives,false positives,hard disks,arithmetic mean,geometric mean,ranked retrieval
iir_8_4	ranked retrieval results,search engines,retrieved documents,precision-recall curve,interpolated precision,11-point interpolated average precision,information needs,relevant documents,test collection,arithmetic mean,queries,query,precision at k,break-even point,sensitivity,specificity,normalized discounted cumulative gain,normalization factor,set-based measures,ranked retrieval context,web search,machine learning approaches,ranking,relevance
iir_8_5	documents,test document collection,query terms,query and document pair,pooling,collections,relevance judgments,kappa statistic,marginal statistics,kappa value,test information needs,gold standard
iir_8_5_1	system evaluation,nonrelevant documents,machine learning methods,ir,scoring,parameter weights,retrieval systems,information needs,queries,marginal relevance,test collection,relevance-based assessment,document length normalization methods
iir_8_6	user utility,index,query,queries,document collection,documents,ranking algorithm,clickstream mining,multiple linear regression,information retrieval system,web search engines,intranet search engine,information security,clickthrough log analysis,implicit feedback,multivariate statistical analysis,information need,relevance
iir_8_7	documents,snippets,relevance,static summary,dynamic summary,metadata,text summarization,document frequency,collection,query terms,keyword-in-context,indexer,ranking,zones,scoring,positional index,caching,cache,crawler,information need,natural language processing,query-independent information value,static summarization,search engine,disk drives,information security,document summary
iir_9	synonymy,recall,information retrieval systems,search,query terms,automatic thesaurus generation,spelling correction,indirect relevance feedback,collections,query refinement,documents
iir_9_1	pseudo relevance feedback,queries,collections,query refinement,information need,query vector,cosine similarity,nonrelevant documents,vector space model,positive feedback,image search system,ranking,collection statistics,term distribution,information retrieval,collection vocabulary,multimodal class,document clusters,web search engines,precision-recall graph,mean average precision,residual collection,relevance judgments,blind relevance feedback,query reformulation,indirect relevance feedback,explicit feedback,clickstream mining,web search query,information filter,negative feedback,query term weights,term-weighting,spelling correction techniques,term frequency,full relevance feedback,clickstream data,web link structure,implicit feedback,relevance feedback strategies,relevance feedback techniques,ad hoc retrieval
iir_9_2	query reformulations,searches,inverted index,collection,documents,query terms,search engines,query suggestions,expanded queries,controlled vocabulary,word cooccurrence,co-occurrence thesaurus,term-term similarities,term-document matrix,dimensionality reduction,false positives,recall,precision,pseudo relevance feedback,stop lists,term weighting,library subject indexes,interactive query expansion,automatic query expansion,word co-occurrence statistics,query log mining,false negatives,dictionaries,domain-particular vocabularies,web search engine
iir_10	relational databases,unstructured text,database systems,data structures,query language,structured text search problems,precision,recall,structured documents,collection,unranked retrieval models,search system,structured search,ranked retrieval methods,semistructured retrieval,semistructured queries,database querying,zone search,parametric fields,zones,tree structure,text-centric approaches,patent databases,named entity tagging,marked up text,unranked retrieval system,markup languages,inverted index-based methods,information retrieval problem,data model,information need,structured data,unstructured retrieval,vector space model
iir_10_1	trees,leaf nodes,internal nodes,parents,children,vocabulary term,schemas,structured queries,collection,query,relational attribute constraints,information retrieval problem,relational attributes,documents,element nodes,root element,ranking constraint,closing tag,metadata,search,attribute nodes,subtree,element-node tree
iir_10_2	unstructured retrieval,collections,structured documents,structured document retrieval principle,trees,indexing units,leaves,relevance,leaf element,nested elements,query terms,structured queries,inverse document frequency,ir,schema heterogeneity,schema diversity,document structures,query interface,parent-child relationships,extended queries,extended query,document unit,ranking,idf weights,keyword queries,pseudodocuments,root node,leaf node,sparse data problems,parent node,tree structure
iir_10_3	vector space model,nodes,lexicalized subtrees,documents,documents,vocabulary terms,leaf node,vector space retrieval system,collection,paths,extended queries,query path,document path,weights,weightings,weightings,inverse document frequency,cosine measure,query length normalization,query-document similarities,sparse term statistics,matching function,unstructured retrieval,query structure,non-structural terms,query-document similarity function,query structural term,query term,context resemblance function,parent-child node pair,array normalizer,postings lists,structural queries
iir_10_4	collections,query,content-and-structure topics,structural constraints,component coverage,topical relevance,relevance-coverage combinations,binary relevance judgments,unstructured information retrieval,recall,binary relevance assessments,precision at k,unstructured bags of words,precision-oriented tasks,collection statistics,test collection,structured documents,keyword queries,evaluation scheme,marginal relevance,vector space system,language-model-based system,full-structure model,information needs,tree,algorithms,unstructured retrieval effectiveness,binary evaluation,relevant documents
iir_10_5	queries,queries,unstructured retrieval methods,relational databases,ranking,text-centric retrieval engine,text-centric structured retrieval model,query languages,information retrieval,joins,database framework,tuples,information need,data collections
iir_11	relevance feedback,nonrelevant documents,classifier,ir,probabilistic approach,term weights,information needs,query representations,document representations,index terms,probability theory,non-positional index,vector space models,probabilistic retrieval model,probabilistic language modeling
iir_11_1	probability theory,random variable,joint probability,conditional probability,conditional probabilities,conditional probability,conditional probabilities,partition rule,chain rule,prior probability,posterior probability,likelihood,odds
iir_11_2	collection of documents,query,ordered list of documents,binary notion of relevance,random variable,probabilistic model,probability of relevance,information need,nonrelevant document,1/0 loss,false positives,false negatives,ranked retrieval,reference retrieval system,accuracy,ir,utility models
iir_11_3	queries,binary term incidence vectors,vector representation,probabilistic retrieval strategy,estimated probability of relevance,document collection,prior probability,relevant documents,relevant documents,nonrelevant document,ranking function,query terms,ranking of documents,odds of relevance,relative frequency,maximum likelihood estimate,smoothing,pseudocounts,uniform distribution,vocabulary,maximum a posteriori,frequency of term occurrence,(pseudo-)relevance feedback,iterative process,relevance judgments,probability function,vector space model,information need,term frequency,document frequency,document length,probability of document relevance,log odds ratios,idf weighting,probabilistic approaches,relevance feedback weighting,algorithm,tf-idf value,beta distribution prior,prior distribution,pseudo-relevance feedback version
iir_11_4	probabilistic methods,computational linguistics,term independence,queries,document relevance,tf-idf,non-binary model,relevance feedback,documents,query terms,positive tuning parameter,tuning parameters,relevance judgments,directed graphs,probabilistic dependencies,directed acyclic graphs,document collection network,query network,probabilistic network,statistical ranked retrieval model,structured query operators,probabilistic models,term weighting models,term weighting scheme,vector space,cosine similarity,probability theory,probabilistic system,term dependencies,tree structure of dependencies,full-text search collections,document scoring,idf weighting,stop list,document term frequency scaling,length normalization,document length scaling,term weighting method,query vector,information retrieval,probabilistic graphical model,information need,query subexpressions,machine learning,grid search
iir_12_1	language models,query,ir,generative model,language theory,probabilistic finite automaton,probability distribution,document model,language of the automaton,likelihood ratio,terms,term emission probabilities,chain rule,unigram language models,grammar-based language models,spelling correction,machine translation,ir,unigram models,document,training data,bias-variance tradeoff,higher-order models,terms,probabilistic context-free grammars,data sparseness,proximity queries,multinomial distribution,unigram language model,unigram models,language models,multinomial probability,bag of words,multinomial coefficient,document,term vocabulary,likelihood ratio,speech recognition,ir,model distribution,query,terms,multinomial model,training sample
iir_12_2	query likelihood model,query likelihood language models,ir,documents,collection,multinomial coefficient,random process,query terms,document model,query,queries,maximum likelihood estimation,documents,term frequency,terms,tokens,training data,ir,vector space systems,conjunctive semantics,document language models,probability distribution,multinomial distribution,document collection,linear interpolation language model,smoothing methods,line search,expectation maximimization algorithm,zero probabilities,speech recognition application,human-computer interface,web search,term weighting,linear interpolation smoothing,unigram,prior distribution,uniform distribution,retrieval ranking,language modeling approach,information retrieval,ir,text classification,effectiveness,term weights,tf-idf weights,tf-idf based term weighting approach,natural language queries
iir_12_3	ir,language modeling approach,text retrieval,queries,probabilistic language modeling,probabilistic approaches,information needs,unigram models,tf-idf models,term frequency,document length normalization,terms,vector space retrieval system,relevance feedback,document model,query language,document generation probability,collection generation probability,speech and language processing,ranking of documents,mathematical models,natural language processing
iir_12_4	language modeling approach,ir,language models,queries,document likelihood model,pseudo-relevance feedback,document-likelihood approaches,ad hoc retrieval,topic tracking,normalized log-likelihood ratio,synonymy,translation models,conditional probability distribution,translation query generation model,document language model,bilingual dictionary,document collection,query language model,relevant documents,information theory,query-likelihood,vocabulary terms,relevance feedback models,document retrieval,cross-entropies,translation dictionary,hypertext
iir_13	ad hoc retrieval,indexing,standing queries,standing query,collection,documents,terms,recall,stemming,classes,two-class classification,routing,filtering,text categorization,topic classification,topic spotting,computer vision,word segmentation,search engine index,email sorting,vertical search engine,precision,ranking function,machine learning-based text classification,text classifier,statistical text classification,labeling,classification algorithms,efficiency,feature selection,classification methods,vector space classifiers,support vector machines,information needs,ir,preprocessing,truecasing,automatic classification,ad hoc information retrieval,topics,retrieval systems,training data,relevance feedback,evaluation of text classification
iir_13_1	document space,classes,labels,high-dimensional space,learning method,learning algorithm,classification function,classifier,supervised learning,text classification,training documents,test set,test data,classification problem,accuracy,training set,categories,training data
iir_13_2	probabilistic learning method,terms,prior probability,tokens,vocabulary,training set,maximum likelihood estimate,training data,conditional probability,conditional probabilities,conditional probability,conditional probabilities,positional independence assumption,sparseness,uniform prior,time complexity,training collection,test documents,text classification method,maximum a posteriori,term weights,supervised learning method,training documents
iir_13_3	multinomial model,vocabulary,documents,binary independence model,terms,time complexity,binary occurrence,classification,generation models,tokens,test document
iir_13_4	class membership,classes,classes,conditional distribution,terms,binary vector,text classification problem,conditional independence assumption,random variable,vocabulary,multinomial model,term distributions,positional independence,conditional probabilities,bag of words model,ad hoc retrieval,document generation model,bag-of-words model,concept drift,training data,optimal classifier,document space,probability distribution,data sparseness,token generation model,stop word,text classifier,probability estimates,accuracy,noise features,knn
iir_13_5	terms,training set,noise features,documents,documents,overfitting,statistical text classification,bias-variance tradeoff,feature selection algorithm,vocabulary,two-class classification,mutual information,mi,classifiers,accuracy,learning method
iir_13_6	text classification evaluation,classes,topic categories,two-class classifiers,training documents,test documents,test data,recall,precision,performance,computational efficiency of classification,ir system,microaveraging,decision trees,training sample,machine learning algorithm,training set,ad hoc retrieval,development set,held-out data,test set,effectiveness,true negatives,true positives,any-of problem,classification error,classification accuracy,one-of classification,document collection
iir_14	vector space classification,document representation,terms,binary vector,text classification,vector space model,tf-idf weight,document space,classification function,classification methods,real-valued vectors,contiguity hypothesis,documents,documents,classification tasks,weighted representations,length-normalized tf-idf representations,k nearest neighbor classification,test document,training set,text classifiers,linear classifiers,linear decision hyperplanes,bias-variance tradeoff,nonlinear models,training data,two-class classifiers,one-of tasks,any-of tasks,any-of problems,one-of problems,stop list,centroids,prototypes,k nearest neighbors
iir_14_1	document vectors,vector classification,length-normalized unit vectors,vector space classifiers,underlying distance measure,cosine similarity,length-normalized vectors,vector space classification,relatedness of two documents,centroids,unnormalized vectors,dot product
iir_14_2	decision boundaries,vector space classification,algorithms,classification accuracy,centroids,normalized vector,normal vector,pseudocode,cosine similarity,relevant documents,relevance feedback,text classification,prototype,multimodal class,two-class classification,training time,vector space classification,hyperplanes,hyperplanes,multimodality,false positives,2-dimensional planes,class regions,query,two-class classifiers,time complexity,collection
iir_14_3	k nearest neighbors,1nn,test document,training documents,decision boundary,convex polygons,convex region,training set,cosine similarity,contiguity hypothesis,2-dimensional space,classification problem
iir_14_4	nonlinear classifiers,text classifiers,two-class classifiers,two-dimensional vector representation,decision hyperplanes,algorithm,class boundary,text classification,noise documents,noise feature,document representation,classification error,training set,linear separability,linear separators,linearly separable problem,nonlinear problem,linear classification,decision boundary,vocabulary,linear hyperplanes,features,learning methods,effectiveness,class centroids,category,tokens,terms,log space,training data
iir_14_5	two-class linear classifiers,classes,documents,classification problem,training set,test document,one-of classification,single-label classification,classification function,(nonlinear) one-of classifier,one-of problems,text classification,two-class linear classifiers,confusion matrix,algorithm,accuracy,any-of classification,any-of problems,topics,one-of assumption,any-of classifiers,hyperplanes,linear separators,confidence
iir_14_6	bias-variance tradeoff,nonlinear classifiers,classification error,machine learning,text classification problems,nonlinear models,quadratic polynomial,complexity of learning,feature selection,regularization,margin maximization,training documents,test documents,document representation,bag of words model,mean squared error,learning error,statistical text classification,optimal learning method,conditional probability,training sets,linear methods,nonlinear class boundary,nonlinear method,overfitting,model complexity,one-feature classifier,misclassifications,text classification algorithms,regularized logistic regression,regularized linear regression,likelihood of linear separability,high-dimensional spaces,nonlinear learning methods,training data,generative models,test set,linear hyperplane,decision boundaries,memory capacity,centroids,effectiveness,positive and negative errors,class boundaries,noise documents,linearity
iir_15	support vector machines,classifier effectiveness,boosted decision trees,regularized logistic regression,neural networks,random forests,information retrieval problems,text classification,vector space based machine learning method,decision boundary,training data,non-separable data,multi-class problems,domain-specific text features,machine learning technology,ad hoc retrieval,machine learning methods,large-margin classifier,two-class data sets,linear classifier,documents,classes,outliers,noise,non-linear models,text classifiers
iir_15_1	decision boundary,learning methods,perceptron algorithm,linear separator,decision surface,decision function,support vectors,misclassification,decision hyperplane,weight vector,functional margin,unit vectors,geometric margin,quadratic function,linear constraints,quadratic optimization,quadratic programming,algorithms,classification function,dual problem,dot product,quadratic optimization,margin of the classifier,probability of classification,sigmoid,document,training data sets,test data,bias-variance tradeoff,mathematical optimization problems,vector space,memory capacity of the model,confidence threshold
iir_15_2	soft margin classification,high dimensional problems,text classification,training set,decision margin,slack variables,optimization problem,regularization,geometric margin,dual problem,support vectors,complexity of training,training data sets,training algorithm,training complexity,two-class classifiers,one-versus-one classifiers,linear classifiers,higher dimensional space,quadratic function,higher-dimensional feature space,kernel trick,dot products,kernel functions,vector space,polynomial kernels,radial basis functions,quadratic kernel,topic classification,noise documents,one-versus-rest classifiers,feature vector,linear kernel,text classifier,machine learning methods,outliers,training errors,terms,overfitting,quadratic optimization,higher-order features,multiclass classification,multiclass problems,gram matrix,string kernels,feature conjunctions,cubic kernel
iir_15_3	machine learning methods,bias,training data,documents,documents,supervised classifier,machine learning theory,semi-supervised training methods,bootstrapping,semi-supervised learning algorithm,active learning,linear classifier,machine learning classifier,machine learning models,classification effectiveness,text classification problem,classification algorithms,classification problems,hierarchical structure,hierarchical classification,scalability of classifiers,aggressive feature selection,boosting,boosting,classification accuracy,voting,bagging,ad hoc retrieval,terms,feature engineering,tokens,k-gram features,stemming,lowercasing,test collection,text summarization,feature selection methods,categorization,text classifiers,text classifiers,query languages,precision,recall,decision trees,classifier effectiveness,document collection,subvocabularies,web directories,library classification schemes,feature sparseness,data sparseness,document zones,domain-specific text features,nearest neighbor model,confidence decisions,tying parameters,upweighting
iir_15_4	cosine score,learning problem,nonrelevant documents,classification approach,two-class classifier,machine-learned scoring,machine-learned relevance,query terms,linear combination,vector space cosine similarity,queries,bag of words,training examples,coefficients,training data,thresholding,linear classifier,query/document pair,binary relevant/nonrelevant judgment,training document collections,training document collections,classification problems,regression problems,ordinal regression,training query,linear weighting of document features,idf,machine learning approaches,document weighting functions,text classification,proximity weighting,binary relevance judgments,probability of relevance,nonlinear scaling,confidence,relevance decision,document zones,training set,error minimization problem,decision boundary,document/query pair
iir_16	flat clustering,clusters,documents,unsupervised learning,cluster membership,classification,distance measure,document clustering,soft clustering algorithms,dimensionality reduction,information retrieval,hard clustering algorithm,classes,hard assignment,subsets
iir_16_1	cluster hypothesis,documents,relevance,information needs,clusters,collection,search result clustering,query,terms,hierarchical tree,keyword searching,static hierarchical clustering,user-mediated iterative clustering,inverted index,language modeling approach,ir,latent semantic indexing,search engine,user interface,clustering in information retrieval,contiguity hypothesis,search term,cluster-based navigation,recall,sparse data problems,vector space model,nearest neighbors,nearest-neighbor search
iir_16_2	hard flat clustering,objective function,centroids,similarity measures,distance metrics,topic similarity,vector space model,cosine similarity,stop words,hard clustering,partitional hierarchical clustering,soft clustering,non-exhaustive clusterings,cardinality of a clustering,clusters,set of documents,distance between documents,heuristic method
iir_16_3	objective functions,intra-cluster similarity,internal criterion,clustering algorithms,external criterion,purity,normalized mutual information,false positive,false negative,true positive,true negative,documents,documents,effectiveness,gold standard,external criteria of clustering quality,one-document clusters,evaluation benchmark,classes,maximum likelihood estimates of the probabilities,class membership,accuracy,recall,information retrieval
iir_16_4	flat clustering algorithm,centroids,documents,length-normalized vectors,residual sum of squares,seeds,global minimum,outliers,singleton cluster,seed selection,deterministic hierarchical clustering methods,time complexity,hierarchical algorithms,linear algorithm,medoid,clusters,random sample,sparse document vectors,cluster centers,mean,real-valued space,training set,suboptimal clustering,iterations,distance computations,cluster cardinality,distortion,model complexity,clusterings,clusters,negative maximum log-likelihood,vector space,text clustering,information retrieval,heuristic method,documents,hard assignment,covariance matrices,flat clustering algorithms,objective function,centroids
iir_16_5	model-based clustering,centroids,noise,maximum likelihood criterion,language modeling,text classification,document-cluster pair,soft clustering,hierarchical algorithms,terms,documents,em,expectation step,maximization step,clusters,clusterings,clustering model,document clustering,soft assignment,local optima,log-likelihood,objective function,hard clustering,probabilistic modeling,maximum likelihood estimates,normalization,probability distribution
