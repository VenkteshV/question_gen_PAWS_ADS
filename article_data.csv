topic,summary
inverted index,"In computer science, an inverted index (also referred to as a postings list, postings file, or inverted file) is a database index storing a mapping from content, such as words or numbers, to its locations in a table, or in a document or a set of documents (named in contrast to a forward index, which maps from documents to content).  The purpose of an inverted index is to allow fast full-text searches, at a cost of increased processing when a document is added to the database.  The inverted file may be the database file itself, rather than its index. It is the most popular data structure used in document retrieval systems, used on a large scale for example in search engines.  Additionally, several significant general-purpose mainframe-based database management systems have used inverted list architectures, including ADABAS, DATACOM/DB, and Model 204.
There are two main variants of inverted indexes: A record-level inverted index (or inverted file index or just inverted file) contains a list of references to documents for each word. A word-level inverted index (or full inverted index or inverted list) additionally contains the positions of each word within a document. The latter form offers more functionality (like phrase searches), but needs more processing power and space to be created."
Inverted index,"In computer science, an inverted index (also referred to as a postings list, postings file, or inverted file) is a database index storing a mapping from content, such as words or numbers, to its locations in a table, or in a document or a set of documents (named in contrast to a forward index, which maps from documents to content).  The purpose of an inverted index is to allow fast full-text searches, at a cost of increased processing when a document is added to the database.  The inverted file may be the database file itself, rather than its index. It is the most popular data structure used in document retrieval systems, used on a large scale for example in search engines.  Additionally, several significant general-purpose mainframe-based database management systems have used inverted list architectures, including ADABAS, DATACOM/DB, and Model 204.
There are two main variants of inverted indexes: A record-level inverted index (or inverted file index or just inverted file) contains a list of references to documents for each word. A word-level inverted index (or full inverted index or inverted list) additionally contains the positions of each word within a document. The latter form offers more functionality (like phrase searches), but needs more processing power and space to be created."
Search engine indexing,"Search engine indexing is the collecting, parsing, and storing of data to facilitate fast and accurate information retrieval. Index design incorporates interdisciplinary concepts from linguistics, cognitive psychology, mathematics, informatics, and computer science. An alternate name for the process, in the context of search engines designed to find web pages on the Internet, is web indexing.
Popular engines focus on the full-text indexing of online, natural language documents. Media types such as pictures, video, audio, and graphics are also searchable.
Meta search engines reuse the indices of other services and do not store a local index whereas cache-based search engines permanently store the index along with the corpus. Unlike full-text indices, partial-text services restrict the depth indexed to reduce index size. Larger services typically perform indexing at a predetermined time interval due to the required time and processing costs, while agent-based search engines index in real time.

"
Database index,"A database index is a data structure that improves the speed of data retrieval operations on a database table at the cost of additional writes and storage space to maintain the index data structure.  Indexes are used to quickly locate data without having to search every row in a database table every time a database table is accessed.  Indexes can be created using one or more columns of a database table, providing the basis for both rapid random lookups and efficient access of ordered records.
An index is a copy of selected columns of data, from a table, that is designed to enable very efficient search.  An index normally includes a ""key"" or direct link to the original row of data from which it was copied, to allow the complete row to be retrieved efficiently.  Some databases extend the power of indexing by letting developers create indexes on column values that have been transformed by functions or expressions. For example, an index could be created on upper(last_name), which would only store the upper-case versions of the last_name field in the index. Another option sometimes supported is the use of partial indices, where index entries are created only for those records that satisfy some conditional expression. A further aspect of flexibility is to permit indexing on user-defined functions, as well as expressions formed from an assortment of built-in functions."
Boolean model of information retrieval,"The (standard) Boolean model of information retrieval (BIR) is a classical information retrieval (IR) model and, at the same time, the first and most-adopted one. It is used by many IR systems to this day. The BIR is based on Boolean logic and classical set theory in that both the documents to be searched and the user's query are conceived as sets of terms (a bag-of-words model). Retrieval is based on whether or not the documents contain the query terms."
Microsoft SQL Server,"Microsoft SQL Server is a relational database management system developed by Microsoft. As a database server, it is a software product with the primary function of storing and retrieving data as requested by other software applications—which may run either on the same computer or on another computer across a network (including the Internet). Microsoft markets at least a dozen different editions of Microsoft SQL Server, aimed at different audiences and for workloads ranging from small single-machine applications to large Internet-facing applications with many concurrent users."
Database model,"A database model is a type of data model that determines the logical structure of a database. It fundamentally determines in which manner data can be stored, organized and manipulated. The most popular example of a database model is the relational model, which uses a table-based format."
Explicit semantic analysis,"In natural language processing and information retrieval, explicit semantic analysis (ESA) is a vectoral representation of text (individual words or entire documents) that uses a document corpus as a knowledge base. Specifically, in ESA, a word is represented as a column vector in the tf–idf matrix of the text corpus and a document (string of words) is represented as the centroid of the vectors representing its words. Typically, the text corpus is English Wikipedia, though other corpora including the Open Directory Project have been used.ESA was designed by Evgeniy Gabrilovich and Shaul Markovitch as a means of improving text categorization
and has been used by this pair of researchers to compute what they refer to as ""semantic relatedness"" by means of cosine similarity between the aforementioned vectors, collectively interpreted as a space of ""concepts explicitly defined and described by humans"", where Wikipedia articles (or ODP entries, or otherwise titles of documents in the knowledge base corpus) are equated with concepts. The name ""explicit semantic analysis"" contrasts with latent semantic analysis (LSA), because the use of a knowledge base makes it possible to assign human-readable labels to the concepts that make up the vector space."
BitFunnel,"BitFunnel is the search engine indexing algorithm and a set of components used in the Bing search engine, which were made open source in 2016. BitFunnel uses bit-sliced signatures instead of an inverted index in an attempt to reduce operations cost."
simple conjunctive query,"Data integration involves combining data residing in different sources and providing users with a unified view of them. This process becomes significant in a variety of situations, which include both commercial (such as when two similar companies need to merge their databases) and scientific (combining research results from different bioinformatics repositories, for example) domains.  Data integration appears with increasing frequency as the volume (that is, big data) and the need to share existing data explodes.  It has become the focus of extensive theoretical work, and numerous open problems remain unsolved. Data integration encourages collaboration between internal as well as external users. The data being integrated must be received from a heterogeneous database system and transformed to a single coherent data store that provides synchronous data across a network of files for clients. A common use of data integration is in data mining when analyzing and extracting information from existing databases that can be useful for Business information."
Data integration,"Data integration involves combining data residing in different sources and providing users with a unified view of them. This process becomes significant in a variety of situations, which include both commercial (such as when two similar companies need to merge their databases) and scientific (combining research results from different bioinformatics repositories, for example) domains.  Data integration appears with increasing frequency as the volume (that is, big data) and the need to share existing data explodes.  It has become the focus of extensive theoretical work, and numerous open problems remain unsolved. Data integration encourages collaboration between internal as well as external users. The data being integrated must be received from a heterogeneous database system and transformed to a single coherent data store that provides synchronous data across a network of files for clients. A common use of data integration is in data mining when analyzing and extracting information from existing databases that can be useful for Business information."
Logical conjunction,"In logic, mathematics and linguistics, And (
  
    
      
        ∧
      
    
    {\displaystyle \wedge }
  ) is the truth-functional operator of logical conjunction; the and of a set of operands is true if and only if all of its operands are true. The logical connective that represents this operator is typically written as 
  
    
      
        ∧
      
    
    {\displaystyle \wedge }
   or  ⋅ .
  
    
      
        A
        ∧
        B
      
    
    {\displaystyle A\land B}
   is true if and only if 
  
    
      
        A
      
    
    {\displaystyle A}
   is true and 
  
    
      
        B
      
    
    {\displaystyle B}
   is true, otherwise it is false.
An operand of a conjunction is a conjunct.
Beyond logic, the term ""conjunction"" also refers to similar concepts in other fields:

In natural language, the denotation of expressions such as English ""and"".
In programming languages, the short-circuit and control structure.
In set theory, intersection.
In lattice theory, logical conjunction (greatest lower bound).
In predicate logic, universal quantification."
RDFLib,"RDFLib is a Python library for working with RDF, a simple yet powerful language for representing information. This library contains parsers/serializers for almost all of the known RDF serializations, such as RDF/XML, Turtle, N-Triples, & JSON-LD, many of which are now supported in their updated form (e.g. Turtle 1.1). The library also contains both in-memory and persistent Graph back-ends for storing RDF information and numerous convenience functions for declaring graph namespaces, lodging SPARQL queries and so on. It is in continuous development with the most recent stable release, rdflib 6.1.1 having been released on 20 December 2021. It was originally created by Daniel Krech with the first release in November, 2002.
A number of other Python projects use rdflib for RDF manipulation, including:

OWL-RL - A simple implementation of the OWL2 RL Profile (reasoning engine)
pySHACL - a Python SHACL validator
pyLDAPI - an add-on module for the Python Flask web framework used to deliver Linked Data
pyLODE - a Web Ontology Language documentation tool"
Constraint satisfaction problem,"Constraint satisfaction problems (CSPs) are mathematical questions defined as the set of objects whose state must satisfy a number of constraints or/ limitations. CSPs represent a entities in a problem as a homogeneous collection of finite constraints over variables, which is solved by constraint satisfaction methods. CSPs are the subject of research in both artificial intelligence and operations research, since the regularity in their formulation provides a common basis to analyze and solve problems of many seemingly unrelated families. CSPs often exhibit high complexity, requiring a combination of heuristics and combinatorial search methods to be solved in a reasonable time. Constraint programming (CP) is the field of research that specifically focuses on tackling these kinds of problems. Additionally, Boolean satisfiability problem (SAT), the satisfiability modulo theories (SMT), mixed integer programming (MIP) and answer set programming (ASP) are all fields of research focusing on the resolution of particular forms of the constraint satisfaction problem.
Examples of problems that can be modeled as a constraint satisfaction problem include:

Type inference
Eight queens puzzle
Map coloring problem
Maximum cut problem
Sudoku, Crosswords, Futoshiki, Kakuro (Cross Sums), Numbrix, Hidato and many other logic puzzlesThese are often provided with tutorials of CP, ASP, Boolean SAT and SMT solvers. In the general case, constraint problems can be much harder, and may not be expressible in some of these simpler systems. ""Real life"" examples include automated planning, lexical disambiguation, musicology, product configuration and resource allocation.The existence of a solution to a CSP can be viewed as a decision problem. This can be decided by finding a solution, or failing to find a solution after exhaustive search (stochastic algorithms typically never reach an exhaustive conclusion, while directed searches often do, on sufficiently small problems). In some cases the CSP might be known to have solutions beforehand, through some other mathematical inference process.

"
Bayesian network,"A Bayesian network (also known as a Bayes network, Bayes net, belief network, or decision network) is a probabilistic graphical model that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG). Bayesian networks are ideal for taking an event that occurred and predicting the likelihood that any one of several possible known causes was the contributing factor.  For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases.
Efficient algorithms can perform inference and learning in Bayesian networks. Bayesian networks that model sequences of variables (e.g. speech signals or protein sequences) are called dynamic Bayesian networks. Generalizations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams."
Serial comma,"In English-language punctuation, a serial comma (also called a series comma, Oxford comma, or Harvard comma) is a comma placed immediately after the penultimate term (i.e., before the coordinating conjunction, such as and or or) in a series of three or more terms. For example, a list of three countries might be punctuated either as ""France, Italy and Spain"" (without the serial comma) or ""France, Italy, and Spain"" (with the serial comma).Opinions among writers and editors differ on whether to use the serial comma, and usage also differs somewhat between regional varieties of English. British English allows constructions with or without this comma, whereas in American English it is common and sometimes even considered mandatory. A majority of American style guides mandate use of the serial comma, including APA style, The Chicago Manual of Style, Garner's Modern American Usage, The MLA Style Manual, Strunk and White's The Elements of Style, and the U.S. Government Printing Office Style Manual. By contrast, the Associated Press Stylebook and The New York Times Style Book advise against it. In Canada, the stylebook published by The Canadian Press advises against it. Most British style guides do not mandate its use. The Economist Style Guide notes that most British writers use it only where necessary to avoid ambiguity. A few British style guides mandate it, most notably The Oxford Style Manual (hence the name, ""Oxford comma"").  However, the University of Oxford Style Guide (2014) advises against its use.The Oxford Companion to the English Language notes: ""Usage varies as to the inclusion of a comma before and in the last item. ... This practice is controversial and is known as the serial comma or Oxford comma, because it is part of the house style of Oxford University Press.""There are cases in which the use of the serial comma can avoid ambiguity, and also instances in which its use can introduce ambiguity."
List of algorithms,The following is a list of well-known algorithms along with one-line descriptions for each.
Boolean algebra,"In mathematics and mathematical logic, Boolean algebra is a branch of algebra. It differs from elementary algebra in two ways. First, the values of the variables are the truth values true and false, usually denoted 1 and 0, whereas in elementary algebra the values of the variables are numbers. Second, Boolean algebra uses logical operators such as conjunction (and) denoted as ∧, disjunction (or) denoted as ∨, and the negation (not) denoted as ¬. Elementary algebra, on the other hand, uses arithmetic operators such as addition, multiplication, subtraction and division. So Boolean algebra is a formal way of describing logical operations, in the same way that elementary algebra describes numerical operations.
Boolean algebra was introduced by George Boole in his first book The Mathematical Analysis of Logic (1847), and set forth more fully in his An Investigation of the Laws of Thought (1854).
According to Huntington, the term ""Boolean algebra"" was first suggested by Sheffer in 1913, although Charles Sanders Peirce gave the title ""A Boolean Algebra with One Constant"" to the first chapter of his ""The Simplest Mathematics"" in 1880.
Boolean algebra has been fundamental in the development of digital electronics, and is provided for in all modern programming languages. It is also used in set theory and statistics."
Fuzzy logic,"Fuzzy logic is a form of many-valued logic in which the truth value of variables may be any real number between 0 and 1. It is employed to handle the concept of partial truth, where the truth value may range between completely true and completely false. By contrast, in Boolean logic, the truth values of variables may only be the integer values 0 or 1.
The term fuzzy logic was introduced with the 1965 proposal of fuzzy set theory by Iranian Azerbaijani mathematician Lotfi Zadeh.  Fuzzy logic had, however,  been studied since the 1920s, as infinite-valued logic—notably by Łukasiewicz and Tarski.Fuzzy logic is based on the observation that people make decisions based on imprecise and non-numerical information. Fuzzy models or sets are mathematical means of representing vagueness and imprecise information (hence the term fuzzy). These models have the capability of recognising, representing, manipulating, interpreting, and using data and information that are vague and lack certainty.Fuzzy logic has been applied to many fields, from control theory to artificial intelligence."
Outline of logic,"Logic  is the formal science of using reason and is considered a branch of both philosophy and mathematics and to a lesser extent computer science. Logic investigates and classifies the structure of statements and arguments, both through the study of formal systems of inference and the study of arguments in natural language. The scope of logic can therefore be very large, ranging from core topics such as the study of fallacies and paradoxes, to specialized analyses of reasoning such as probability, correct reasoning, and arguments involving causality. One of the aims of logic is to identify the correct (or valid) and incorrect (or fallacious) inferences. Logicians study the criteria for the evaluation of arguments."
dictionary,"Wiktionary (UK:  WIK-shə-nər-ee, US:  WIK-shə-nerr-ee, rhyming with ""dictionary"") is a multilingual, web-based project to create a free content dictionary of terms (including words, phrases, proverbs, linguistic reconstructions, etc.) in all natural languages and in a number of artificial languages. These entries may contain definitions, images for illustration, pronunciations, etymologies, inflections, usage examples, quotations, related terms, and translations of terms into other languages, among other features. It is collaboratively edited via a wiki. Its name is a portmanteau of the words wiki and dictionary. It is available in 186 languages and in Simple English. Like its sister project Wikipedia, Wiktionary is run by the Wikimedia Foundation, and is written collaboratively by volunteers, dubbed ""Wiktionarians"". Its wiki software, MediaWiki, allows almost anyone with access to the website to create and edit entries.
Because Wiktionary is not limited by print space considerations, most of Wiktionary's language editions provide definitions and translations of terms from many languages, and some editions offer additional information typically found in thesauri.
Wiktionary's data is frequently used in various natural language processing tasks."
Dictionary,"Wiktionary (UK:  WIK-shə-nər-ee, US:  WIK-shə-nerr-ee, rhyming with ""dictionary"") is a multilingual, web-based project to create a free content dictionary of terms (including words, phrases, proverbs, linguistic reconstructions, etc.) in all natural languages and in a number of artificial languages. These entries may contain definitions, images for illustration, pronunciations, etymologies, inflections, usage examples, quotations, related terms, and translations of terms into other languages, among other features. It is collaboratively edited via a wiki. Its name is a portmanteau of the words wiki and dictionary. It is available in 186 languages and in Simple English. Like its sister project Wikipedia, Wiktionary is run by the Wikimedia Foundation, and is written collaboratively by volunteers, dubbed ""Wiktionarians"". Its wiki software, MediaWiki, allows almost anyone with access to the website to create and edit entries.
Because Wiktionary is not limited by print space considerations, most of Wiktionary's language editions provide definitions and translations of terms from many languages, and some editions offer additional information typically found in thesauri.
Wiktionary's data is frequently used in various natural language processing tasks."
Oxford English Dictionary,"The Oxford English Dictionary (OED) is the first and foundational historical dictionary of the English language, published by Oxford University Press (OUP). It traces the historical development of the English language, providing a comprehensive resource to scholars and academic researchers, as well as describing usage in its many variations throughout the world.Work began on the dictionary in 1857, but it was only in 1884 that it began to be published in unbound fascicles as work continued on the project, under the name of A New English Dictionary on Historical Principles; Founded Mainly on the Materials Collected by The Philological Society. In 1895, the title The Oxford English Dictionary was first used unofficially on the covers of the series, and in 1928 the full dictionary was republished in 10 bound volumes. In 1933, the title The Oxford English Dictionary fully replaced the former name in all occurrences in its reprinting as 12 volumes with a one-volume supplement. More supplements came over the years until 1989, when the second edition was published, comprising 21,728 pages in 20 volumes. Since 2000, compilation of a third edition of the dictionary has been underway, approximately half of which was complete by 2018.The first electronic version of the dictionary was made available in 1988. The online version has been available since 2000, and by April 2014 was receiving over two million visits per month. The third edition of the dictionary is expected to be available exclusively in electronic form; the Chief Executive of Oxford University Press has stated that it is unlikely that it will ever be printed."
Oxford dictionary,"The Oxford English Dictionary (OED) is the first and foundational historical dictionary of the English language, published by Oxford University Press (OUP). It traces the historical development of the English language, providing a comprehensive resource to scholars and academic researchers, as well as describing usage in its many variations throughout the world.Work began on the dictionary in 1857, but it was only in 1884 that it began to be published in unbound fascicles as work continued on the project, under the name of A New English Dictionary on Historical Principles; Founded Mainly on the Materials Collected by The Philological Society. In 1895, the title The Oxford English Dictionary was first used unofficially on the covers of the series, and in 1928 the full dictionary was republished in 10 bound volumes. In 1933, the title The Oxford English Dictionary fully replaced the former name in all occurrences in its reprinting as 12 volumes with a one-volume supplement. More supplements came over the years until 1989, when the second edition was published, comprising 21,728 pages in 20 volumes. Since 2000, compilation of a third edition of the dictionary has been underway, approximately half of which was complete by 2018.The first electronic version of the dictionary was made available in 1988. The online version has been available since 2000, and by April 2014 was receiving over two million visits per month. The third edition of the dictionary is expected to be available exclusively in electronic form; the Chief Executive of Oxford University Press has stated that it is unlikely that it will ever be printed."
Urban Dictionary,"Urban Dictionary is a crowdsourced English-language online dictionary for slang words and phrases, operating under the motto ""Define Your World"". The website was founded in 1999 by Aaron Peckham. Originally, Urban Dictionary was intended as a dictionary of slang or cultural words and phrases, not typically found in standard English dictionaries, but it is now used to define any word, event, or phrase (including sexually explicit content). Words or phrases on Urban Dictionary may have multiple definitions, usage examples, and tags. As of 2014, the dictionary contains over seven million definitions, while around 2,000 new entries were being added daily."
Chambers Dictionary,"The Chambers Dictionary (TCD) was first published by William and Robert Chambers as Chambers's English Dictionary in 1872. It was an expanded version of Chambers's Etymological Dictionary of 1867, compiled by James Donald. A second edition came out in 1898, and was followed in 1901 by a new compact edition called Chambers's Twentieth Century Dictionary.TCD is widely used by British crossword solvers and setters, and by Scrabble players (though it is no longer the official Scrabble dictionary). It contains many more dialectal, archaic, unconventional and eccentric words than its rivals, and is noted for its occasional wryly humorous definitions. Examples of such definitions include those for éclair (""a cake, long in shape but short in duration"") and middle-aged (""between youth and old age, variously reckoned to suit the reckoner""). These jocular definitions were removed by the publisher in the 1970s, but many of them were reinstated in 1983 because of the affection in which they were held by readers.
The twelfth edition of The Chambers Dictionary was published in August 2011 by Chambers Harrap Publishers Ltd and runs to 1936 pages with 62,500 main entries. This edition is available for mobile use as an iPhone, iPad, or Android app. That has been followed by the thirteenth edition published in 2014. Also on sale is the smaller 21st Century Dictionary of 1664 pages, where ""the focus is on the English that people use today, and definitions are given in straightforward, accessible language"". This dictionary can be accessed for free online."
The Free Dictionary,The Free Dictionary is an American online dictionary and encyclopedia that aggregates information from various sources.
Webster's Dictionary,"Webster's Dictionary is any of the English language dictionaries edited in the early 19th century by American lexicographer Noah Webster (1758–1843), as well as numerous related or unrelated dictionaries that have adopted the Webster's name in honor. ""Webster's"" has since become a genericized trademark in the United States for English dictionaries, and is widely used in dictionary titles.Merriam-Webster is the corporate heir to Noah Webster's original works, which are in the public domain."
Dictionary.com,"Dictionary.com is an online dictionary whose domain was first registered on May 14, 1995. The primary content on Dictionary.com is a proprietary dictionary based on Random House Unabridged Dictionary, with editors for the site providing new and updated definitions. Supplementary content comes from the Collins English Dictionary, American Heritage Dictionary and others."
The New Grove Dictionary of Music and Musicians,"The New Grove Dictionary of Music and Musicians is an encyclopedic dictionary of music and musicians. Along with the German-language Die Musik in Geschichte und Gegenwart, it is one of the largest reference works on the history and theory of music. Earlier editions were published under the titles A Dictionary of Music and Musicians, and Grove's Dictionary of Music and Musicians; the work has gone through several editions since the 19th century and is widely used. In recent years it has been made available as an electronic resource called Grove Music Online, which is now an important part of Oxford Music Online."
Collins English Dictionary,"The Collins English Dictionary is a printed and online dictionary of English. It is published by HarperCollins in Glasgow.The edition of the dictionary in 1979 with Patrick Hanks as editor and Laurence Urdang as editorial director, was the first British English dictionary to be typeset from the output from a computer database in a specified format. This meant that every aspect of an entry was handled by a different editor using different forms or templates. Once all the entries for an entry had been assembled, they were passed on to be keyed into the slowly assembled dictionary database which was completed for the typesetting of the first edition.In a later edition, they increasingly used the Bank of English established by John Sinclair at COBUILD to provide typical citations rather than examples composed by the lexicographer."
intersection,"In mathematics, the intersection of two or more objects is another object consisting of everything that is contained in all of the objects simultaneously. For example, in Euclidean geometry, when two lines in a plane are not parallel, their intersection is the point at which they meet. More generally, in set theory, the intersection of sets is defined to be the set of elements which belong to all of them. Unlike the Euclidean definition, this does not presume that the objects under consideration lie in a common space.
Intersection is one of the basic concepts of geometry. An intersection can have various geometric shapes, but a point is the most common in a plane geometry. Incidence geometry defines an intersection (usually, of flats) as an object of lower dimension that is incident to each of original objects. In this approach an intersection can be sometimes undefined, such as for parallel lines. In both cases the concept of intersection relies on logical conjunction. Algebraic geometry defines intersections in its own way with intersection theory."
Intersection,"In mathematics, the intersection of two or more objects is another object consisting of everything that is contained in all of the objects simultaneously. For example, in Euclidean geometry, when two lines in a plane are not parallel, their intersection is the point at which they meet. More generally, in set theory, the intersection of sets is defined to be the set of elements which belong to all of them. Unlike the Euclidean definition, this does not presume that the objects under consideration lie in a common space.
Intersection is one of the basic concepts of geometry. An intersection can have various geometric shapes, but a point is the most common in a plane geometry. Incidence geometry defines an intersection (usually, of flats) as an object of lower dimension that is incident to each of original objects. In this approach an intersection can be sometimes undefined, such as for parallel lines. In both cases the concept of intersection relies on logical conjunction. Algebraic geometry defines intersections in its own way with intersection theory."
Intersectionality,"Intersectionality is an analytical framework for understanding how aspects of a person's social and political identities combine to create different modes of discrimination and privilege. Intersectionality identifies multiple factors of advantage and disadvantage. Examples of these factors include gender, caste, sex, race, ethnicity, class, sexuality, religion, disability, weight, and physical appearance. These intersecting and overlapping social identities may be both empowering and oppressing.Intersectionality broadens the scope of the first and second waves of feminism, which largely focused on the experiences of women who were white, middle-class and cisgender, to include the different experiences of women of color, poor women, immigrant women, and other groups. Intersectional feminism aims to separate itself from white feminism by acknowledging women's differing experiences and identities.The term intersectionality was coined by Kimberlé Crenshaw in 1989.: 385  Intersectionality is a qualitative analytic framework developed in the late 20th century that identifies how interlocking systems of power affect those who are most marginalized in society. Activists use the framework to promote social and political egalitarianism. Intersectionality opposes analytical systems that treat each axis of oppression in isolation. In this framework, for instance, discrimination against black women cannot be explained as a simple combination of misogyny and racism, but as something more complicated. Intersectionality engages in similar themes as triple oppression, which is the oppression associated with being a poor or immigrant woman of color. Intersectional analysis aligns very closely with anarcha-feminist power analysis frameworks.
Criticism includes the framework's tendency to reduce individuals to specific demographic factors, and its use as an ideological tool against other feminist theories. Critics have characterized the framework as ambiguous and lacking defined goals. As it is based in standpoint theory, critics say the focus on subjective experiences can lead to contradictions and the inability to identify common causes of oppression."
Intersection (road),"An intersection or an at-grade junction is a junction where two or more roads converge, diverge, meet or cross at the same height, as opposed to an interchange, which uses bridges or tunnels to separate different roads. Major intersections are often delineated by gores and may be classified by road segments, traffic controls and lane design."
vocabulary,"A vocabulary is a set of familiar words within a person's language. A vocabulary, usually developed with age, serves as a useful and fundamental tool for communication and acquiring knowledge. Acquiring an extensive vocabulary is one of the largest challenges in learning a second language."
Vocabulary,"A vocabulary is a set of familiar words within a person's language. A vocabulary, usually developed with age, serves as a useful and fundamental tool for communication and acquiring knowledge. Acquiring an extensive vocabulary is one of the largest challenges in learning a second language."
English language,"English is a West Germanic language of the Indo-European language family, with its earliest forms spoken by the inhabitants of early medieval England. It is named after the Angles, one of the ancient Germanic peoples that migrated to the island of Great Britain. English is genealogically West Germanic, closest related to the Low Saxon and Frisian languages; however, its vocabulary is also distinctively influenced by dialects of French (about 29% of modern English words) and Latin (also about 29%), plus some grammar and a small amount of core vocabulary influenced by Old Norse (a North Germanic language). Speakers of English are called Anglophones.
The earliest forms of English, collectively known as Old English, evolved from a group of West Germanic (Ingvaeonic) dialects brought to Great Britain by Anglo-Saxon settlers in the 5th century and further mutated by Norse-speaking Viking settlers starting in the 8th and 9th centuries. Middle English began in the late 11th century after the Norman conquest of England, when considerable French (especially Old Norman) and Latin-derived vocabulary was incorporated into English over some three hundred years. Early Modern English began in the late 15th century with the start of the Great Vowel Shift and the Renaissance trend of borrowing further Latin and Greek words and roots into English, concurrent with the introduction of the printing press to London. This era notably culminated in the King James Bible and plays of William Shakespeare.Modern English grammar is the result of a gradual change from a typical Indo-European dependent-marking pattern, with a rich inflectional morphology and relatively free word order, to a mostly analytic pattern with little inflection, and a fairly fixed subject–verb–object word order. Modern English relies more on auxiliary verbs and word order for the expression of complex tenses, aspect and mood, as well as passive constructions, interrogatives and some negation.
Modern English has spread around the world since the 17th century as a consequence of the worldwide influence of the British Empire and the United States of America. Through all types of printed and electronic media of these countries, English has become the leading language of international discourse and the lingua franca in many regions and professional contexts such as science, navigation and law. English is the most spoken language in the world and the third-most spoken native language in the world, after Standard Chinese and Spanish. It is the most widely learned second language and is either the official language or one of the official languages in 59 sovereign states. There are more people who have learned English as a second language than there are native speakers. As of 2005, it was estimated that there were over 2 billion speakers of English. English is the majority native language in the United Kingdom, the United States, Canada, Australia, New Zealand and the Republic of Ireland (see Anglosphere), and is widely spoken in some areas of the Caribbean, Africa, South Asia, Southeast Asia, and Oceania. It is a co-official language of the United Nations, the European Union and many other world and regional international organisations. It is the most widely spoken Germanic language, accounting for at least 70% of speakers of this Indo-European branch."
Vocabulary development,"Vocabulary development is a process by which people acquire words. Babbling shifts towards meaningful speech as infants grow and produce their first words around the age of one year. In early word learning, infants build their vocabulary slowly. By the age of 18 months, infants can typically produce about 50 words and begin to make word combinations.
In order to build their vocabularies, infants must learn about the meanings that words carry. The mapping problem asks how infants correctly learn to attach words to referents. Constraints theories, domain-general views, social-pragmatic accounts, and an emergentist coalition model have been proposed to account for the mapping problem.
From an early age, infants use language to communicate. Caregivers and other family members use language to teach children how to act in society. In their interactions with peers, children have the opportunity to learn about unique conversational roles. Through pragmatic directions, adults often offer children cues for understanding the meaning of words.
Throughout their school years, children continue to build their vocabulary. In particular, children begin to learn abstract words. Beginning around age 3–5, word learning takes place both in conversation and through reading. Word learning often involves physical context, builds on prior knowledge, takes place in social context, and includes semantic support. The phonological loop and serial order short-term memory may both play an important role in vocabulary development."
Japanese language,"Japanese (日本語, Nihongo, [ɲihoŋɡo] (listen)) is spoken natively by about 128 million people, primarily by Japanese people and primarily in Japan, the only country where it is the national language. Japanese belongs to the Japonic or Japanese-Ryukyuan language family. There have been many attempts to group the Japonic languages with other families such as the Ainu, Austroasiatic, Koreanic, and the now-discredited Altaic, but none of these proposals has gained widespread acceptance.
Little is known of the language's prehistory, or when it first appeared in Japan. Chinese documents from the 3rd century AD recorded a few Japanese words, but substantial Old Japanese texts did not appear until the 8th century. From the Heian period (794–1185), there was a massive influx of Sino-Japanese vocabulary into the language, affecting the phonology of Early Middle Japanese. Late Middle Japanese (1185–1600) saw extensive grammatical changes and the first appearance of European loanwords. The basis of the standard dialect moved from the Kansai region to the Edo region (modern Tokyo) in the Early Modern Japanese period (early 17th century–mid 19th century). Following the end of Japan's self-imposed isolation in 1853, the flow of loanwords from European languages increased significantly, and words from English roots have proliferated.
Japanese is an agglutinative, mora-timed language with relatively simple phonotactics, a pure vowel system, phonemic vowel and consonant length, and a lexically significant pitch-accent. Word order is normally subject–object–verb with particles marking the grammatical function of words, and sentence structure is topic–comment. Sentence-final particles are used to add emotional or emphatic impact, or form questions. Nouns have no grammatical number or gender, and there are no articles. Verbs are conjugated, primarily for tense and voice, but not person. Japanese adjectives are also conjugated. Japanese has a complex system of honorifics, with verb forms and vocabulary to indicate the relative status of the speaker, the listener, and persons mentioned.
The Japanese writing system combines Chinese characters, known as kanji (漢字, 'Han characters'), with two unique syllabaries (or moraic scripts) derived by the Japanese from the more complex Chinese characters: hiragana (ひらがな or 平仮名, 'simple characters') and katakana (カタカナ or 片仮名, 'partial characters'). Latin script (rōmaji ローマ字) is also used in a limited fashion (such as for imported acronyms) in Japanese writing. The numeral system uses mostly Arabic numerals, but also traditional Chinese numerals."
Korean language,"Korean (South Korean: 한국어, hangugeo; North Korean: 조선말, chosŏnmal) is the native language for about 80 million people, mostly of Korean descent. It is the official and national language of both North Korea and South Korea (geographically Korea), but over the past 74 years of political division, the two Koreas have developed some noticeable vocabulary differences. Beyond Korea, the language is recognised as a minority language in parts of China, namely Jilin Province, and specifically Yanbian Prefecture and Changbai County. It is also spoken by Sakhalin Koreans in parts of Sakhalin, the Russian island just north of Japan, and by the Koryo-saramcode: kor promoted to code: ko  in parts of Central Asia. The language has a few extinct relatives which—along with the Jeju language (Jejuan) of Jeju Island and Korean itself—form the compact Koreanic language family. Even so, Jejuan and Korean are not mutually intelligible with each other. The linguistic homeland of Korean is suggested to be somewhere in contemporary Northeast China. The hierarchy of the society from which the language originates deeply influences the language, leading to a system of speech levels and honorifics indicative of the formality of any given situation.
Modern Korean is written in the Korean script (한글; Hangul in South Korea, 조선글; Chosŏn'gŭl in North Korea), a system developed during the 15th century for that purpose, although it did not become the primary script until the 20th century. The script uses 24 basic letters (jamo) and 27 complex letters formed from the basic ones. When first recorded in historical texts, Korean was only a spoken language; all written records were maintained in Classical Chinese, which, even when spoken, is not intelligible to someone who speaks only Korean. Later, Chinese characters adapted to the Korean language, Hanja (漢字), were used to write the language for most of Korea's history and are still used to a limited extent in South Korea, most prominently in the humanities and the study of historical texts.
Since the turn of the 21st century, aspects of Korean culture have spread to other countries through globalization and cultural exports. As such, interest in Korean language acquisition (as a foreign language) is also generated by longstanding alliances, military involvement, and diplomacy, such as between South Korea–United States, China–North Korea and North Korea–Russia since the end of World War II and the Korean War. Along with other languages such as Chinese and Arabic, Korean is ranked at the top difficulty level for English speakers by the U.S. Department of Defense.

"
French language,"French (français [fʁɑ̃sɛ] or langue française [lɑ̃ɡ fʁɑ̃sɛːz]) is a Romance language of the Indo-European family. It descended from the Vulgar Latin of the Roman Empire, as did all Romance languages. French evolved from Gallo-Romance, the Latin spoken in Gaul, and more specifically in Northern Gaul. Its closest relatives are the other langues d'oïl—languages historically spoken in northern France and in southern Belgium, which French (Francien) largely supplanted. French was also influenced by native Celtic languages of Northern Roman Gaul like Gallia Belgica and by the (Germanic) Frankish language of the post-Roman Frankish invaders. Today, owing to France's past overseas expansion, there are numerous French-based creole languages, most notably Haitian Creole. A French-speaking person or nation may be referred to as Francophone in both English and French.
French is an official language in 29 countries across multiple continents, most of which are members of the Organisation internationale de la Francophonie (OIF), the community of 84 countries which share the official use or teaching of French. French is also one of six official languages used in the United Nations. It is spoken as a first language (in descending order of the number of speakers) in France; Canada (especially in the provinces of Quebec, Ontario, and New Brunswick, as well as other Francophone regions); Belgium (Wallonia and the Brussels-Capital Region); western Switzerland (specifically the cantons forming the Romandy region); parts of Luxembourg; parts of the United States (the states of Louisiana, Maine, New Hampshire and Vermont); Monaco; the Aosta Valley region of Italy; and various communities elsewhere.In 2015, approximately 40% of the francophone population (including L2 and partial speakers) lived in Europe, 36% in sub-Saharan Africa and the Indian Ocean, 15% in North Africa and the Middle East, 8% in the Americas, and 1% in Asia and Oceania. French is the second most widely spoken mother tongue in the European Union. Of Europeans who speak other languages natively, approximately one-fifth are able to speak French as a second language. French is the second most taught foreign language in the EU. All institutions of the EU use French as a working language along with English and German; in certain institutions, French is the sole working language (e.g. at the Court of Justice of the European Union). French is also the 18th most natively spoken language in the world, fifth most spoken language by total number of speakers and the second or third most studied language worldwide (with about 120 million learners as of 2017). As a result of French and Belgian colonialism from the 16th century onward, French was introduced to new territories in the Americas, Africa and Asia. Most second-language speakers reside in Francophone Africa, in particular Gabon, Algeria, Morocco, Tunisia, Mauritius, Senegal and Ivory Coast.French is estimated to have about 76 million native speakers; about 235 million daily, fluent speakers; and another 77–110 million secondary speakers who speak it as a second language to varying degrees of proficiency, mainly in Africa. According to the OIF, approximately 321 million people worldwide are ""able to speak the language"", without specifying the criteria for this estimation or whom it encompasses. According to a demographic projection led by the Université Laval and the Réseau Démographie de l'Agence universitaire de la Francophonie, the total number of French speakers will reach approximately 500 million in 2025 and 650 million by 2050. OIF estimates 700 million by 2050, 80% of whom will be in Africa.French has a long history as an international language of literature and scientific standards and is a primary or second language of many international organisations including the United Nations, the European Union, the North Atlantic Treaty Organization, the World Trade Organization, the International Olympic Committee, and the International Committee of the Red Cross. In 2011, Bloomberg Businessweek ranked French the third most useful language for business, after English and Standard Mandarin Chinese."
Dutch language,"Dutch (Nederlands [ˈneːdərlɑnts] (listen)) is a West Germanic language spoken by about 25 million people as a first language and 5 million as a second language. It is the third most widely spoken Germanic language, after its close relatives  German and English. Afrikaans is a separate but somewhat mutually intelligible daughter language spoken, to some degree, by at least 16 million people, mainly in South Africa and Namibia, evolving from the Cape Dutch dialects of Southern Africa. The dialects used in Belgium (including Flemish) and in Suriname, meanwhile, are all guided by the Dutch Language Union.
In Europe, most of the population of the Netherlands (where it is the only official language spoken countrywide) and about 60% of the population of Belgium (as one of three official languages) speak Dutch. Outside the Low Countries, Dutch is the native language of the majority of the population of the South American country of Suriname, a former Dutch colony, where it also holds an official status, as it does in the Caribbean island countries of Aruba, Curaçao and Sint Maarten, which are constituent countries of the Kingdom of the Netherlands.  Up to half a million native speakers reside in the United States, Canada and Australia combined, and historical linguistic minorities on the verge of extinction remain in parts of France, Germany and Indonesia.Dutch is one of the closest relatives of both German and English and is colloquially said to be ""roughly in between"" them. Dutch, like English, has not undergone the High German consonant shift, does not use Germanic umlaut as a grammatical marker, has largely abandoned the use of the subjunctive, and has levelled much of its morphology, including most of its case system. Features shared with German include the survival of two to three grammatical genders—albeit with few grammatical consequences—as well as the use of modal particles, final-obstruent devoicing, and a similar word order. Dutch vocabulary is mostly Germanic; it incorporates slightly more Romance loans than German but far fewer than English."
Bengali vocabulary,"Bengali (বাংলা Bangla) is one of the Eastern Indo-Aryan (Magadhan) languages, evolved from Magadhi Prakrit and Pali languages native to the Indian subcontinent. The core of Bengali vocabulary is thus etymologically of Magadhi Prakrit and Pali languages. However, centuries of major borrowing and reborrowing from Arabic, Persian, Turkish, Sanskrit, Austroasiatic languages and other languages has led to the adoption of a wide range of words with foreign origins; thus making the origins of borrowed and re-borrowed words in the Bengali vocabulary numerous and diverse, due to centuries of contact with various languages."
Singlish vocabulary,"Singlish is the English-based creole or patois spoken colloquially in Singapore. English is one of Singapore's official languages, along with Malay (which is also the National Language), Mandarin, and Tamil. Although English is the lexifier language, Singlish has its unique slang and syntax, which are more pronounced in informal speech. It is usually a mixture of English, Mandarin, Tamil, Malay, and other local dialects like Hokkien, Cantonese or Teochew. There are a few loan words from these languages i.e. pek chek is often taken as being annoyed or frustrated and originate from the Hokkien dialect. It is used in casual contexts between Singaporeans, but is avoided in formal events when certain Singlish phrases may be considered unedifying. Singapore English can be broken into two subcategories. Standard Singapore English (SSE) and Colloquial Singapore English (CSE) or Singlish as many locals call it. The relationship between SSE and Singlish is viewed as a diglossia, in which SSE is restricted to be used in situations of formality where Singlish/CSE is used in most other circumstances.Some of the most popular Singlish terms have been added to the Oxford English Dictionary (OED) since 2000, including wah, sabo, lepak, shiok and hawker centre. On 11 February 2015, kiasu was chosen as OED's Word of the Day."
XML,"Extensible Markup Language (XML) is a markup language and file format for storing, transmitting, and reconstructing arbitrary data. It defines a set of rules for encoding documents in a format that is both human-readable and machine-readable. The World Wide Web Consortium's XML 1.0 Specification of 1998 and several other related specifications—all of them free open standards—define XML.The design goals of XML emphasize simplicity, generality, and usability across the Internet. It is a textual data format with strong support via Unicode for different human languages. Although the design of XML focuses on documents, the language is widely used for the representation of arbitrary data structures such as those used in web services.
Several schema systems exist to aid in the definition of XML-based languages, while programmers have developed many application programming interfaces (APIs) to aid the processing of XML data."
dictionaries,"A dictionary is a listing of lexemes from the lexicon of one or more specific languages, often arranged alphabetically (or by radical and stroke for ideographic languages), which may include information on definitions, usage, etymologies, pronunciations, translation, etc. It is a lexicographical reference that shows inter-relationships among the data.A broad distinction is made between general and specialized dictionaries. Specialized dictionaries include words in specialist fields, rather than a complete range of words in the language. Lexical items that describe concepts in specific fields are usually called terms instead of words, although there is no consensus whether lexicology and terminology are two different fields of study. In theory, general dictionaries are supposed to be semasiological, mapping word to definition, while specialized dictionaries are supposed to be onomasiological, first identifying concepts and then establishing the terms used to designate them. In practice, the two approaches are used for both types. There are other types of dictionaries that do not fit neatly into the above distinction, for instance bilingual (translation) dictionaries, dictionaries of synonyms (thesauri), and rhyming dictionaries. The word dictionary (unqualified) is usually understood to refer to a general purpose monolingual dictionary.There is also a contrast between prescriptive or descriptive dictionaries; the former reflect what is seen as correct use of the language while the latter reflect recorded actual use. Stylistic indications (e.g. ""informal"" or ""vulgar"") in many modern dictionaries are also considered by some to be less than objectively descriptive.The first recorded dictionaries date back to Sumerian times around 2300 BCE, in the form of bilingual dictionaries, and the oldest surviving monolingual dictionaries are Chinese dictionaries c. 3rd century BCE. The first purely English alphabetical dictionary was A Table Alphabeticall, written in 1604, and monolingual dictionaries in other languages also began appearing in Europe at around this time. The systematic study of dictionaries as objects of scientific interest arose as a 20th-century enterprise, called lexicography, and largely initiated by Ladislav Zgusta. The birth of the new discipline was not without controversy, with the practical dictionary-makers being sometimes accused by others of having an ""astonishing"" lack of method and critical-self reflection."
Dictionary,"Wiktionary (UK:  WIK-shə-nər-ee, US:  WIK-shə-nerr-ee, rhyming with ""dictionary"") is a multilingual, web-based project to create a free content dictionary of terms (including words, phrases, proverbs, linguistic reconstructions, etc.) in all natural languages and in a number of artificial languages. These entries may contain definitions, images for illustration, pronunciations, etymologies, inflections, usage examples, quotations, related terms, and translations of terms into other languages, among other features. It is collaboratively edited via a wiki. Its name is a portmanteau of the words wiki and dictionary. It is available in 186 languages and in Simple English. Like its sister project Wikipedia, Wiktionary is run by the Wikimedia Foundation, and is written collaboratively by volunteers, dubbed ""Wiktionarians"". Its wiki software, MediaWiki, allows almost anyone with access to the website to create and edit entries.
Because Wiktionary is not limited by print space considerations, most of Wiktionary's language editions provide definitions and translations of terms from many languages, and some editions offer additional information typically found in thesauri.
Wiktionary's data is frequently used in various natural language processing tasks."
Lexico,"A lexicon is the vocabulary of a language or branch of knowledge (such as nautical or medical).  In linguistics, a lexicon is a language's inventory of lexemes. The word lexicon derives from Greek word λεξικόν (lexikon), neuter of λεξικός (lexikos) meaning 'of or for words'.Linguistic theories generally regard human languages as consisting of two parts: a lexicon, essentially a catalogue of a language's words (its wordstock); and a grammar, a system of rules which allow for the combination of those words into meaningful sentences. The lexicon is also thought to include bound morphemes, which cannot stand alone as words (such as most affixes). In some analyses, compound words and certain classes of idiomatic expressions, collocations and other phrases are also considered to be part of the lexicon. Dictionaries are lists of the lexicon, in alphabetical order, of a given language; usually, however, bound morphemes are not included."
Oxford dictionary,"The Oxford English Dictionary (OED) is the first and foundational historical dictionary of the English language, published by Oxford University Press (OUP). It traces the historical development of the English language, providing a comprehensive resource to scholars and academic researchers, as well as describing usage in its many variations throughout the world.Work began on the dictionary in 1857, but it was only in 1884 that it began to be published in unbound fascicles as work continued on the project, under the name of A New English Dictionary on Historical Principles; Founded Mainly on the Materials Collected by The Philological Society. In 1895, the title The Oxford English Dictionary was first used unofficially on the covers of the series, and in 1928 the full dictionary was republished in 10 bound volumes. In 1933, the title The Oxford English Dictionary fully replaced the former name in all occurrences in its reprinting as 12 volumes with a one-volume supplement. More supplements came over the years until 1989, when the second edition was published, comprising 21,728 pages in 20 volumes. Since 2000, compilation of a third edition of the dictionary has been underway, approximately half of which was complete by 2018.The first electronic version of the dictionary was made available in 1988. The online version has been available since 2000, and by April 2014 was receiving over two million visits per month. The third edition of the dictionary is expected to be available exclusively in electronic form; the Chief Executive of Oxford University Press has stated that it is unlikely that it will ever be printed."
documents,"A document is a written, drawn, presented, or memorialized representation of thought, often the manifestation of non-fictional, as well as fictional, content.  The word originates from the Latin Documentum, which denotes a ""teaching"" or ""lesson"": the verb doceō denotes ""to teach"". In the past, the word was usually used to denote written proof useful as evidence of a truth or fact. In the computer age, ""document"" usually denotes a primarily textual computer file, including its structure and format, e.g. fonts, colors, and images. Contemporarily, ""document"" is not defined by its transmission medium, e.g., paper, given the existence of electronic documents. ""Documentation"" is distinct because it has more denotations than ""document"". Documents are also distinguished from ""realia"", which are three-dimensional objects that would otherwise satisfy the definition of ""document"" because they memorialize or represent thought; documents are considered more as 2-dimensional representations. While documents can have large varieties of customization, all documents can be shared freely and have the right to do so, creativity can be represented by documents, also. History, events, examples, opinions, etc. all can be expressed in documents."
Document,"A document is a written, drawn, presented, or memorialized representation of thought, often the manifestation of non-fictional, as well as fictional, content.  The word originates from the Latin Documentum, which denotes a ""teaching"" or ""lesson"": the verb doceō denotes ""to teach"". In the past, the word was usually used to denote written proof useful as evidence of a truth or fact. In the computer age, ""document"" usually denotes a primarily textual computer file, including its structure and format, e.g. fonts, colors, and images. Contemporarily, ""document"" is not defined by its transmission medium, e.g., paper, given the existence of electronic documents. ""Documentation"" is distinct because it has more denotations than ""document"". Documents are also distinguished from ""realia"", which are three-dimensional objects that would otherwise satisfy the definition of ""document"" because they memorialize or represent thought; documents are considered more as 2-dimensional representations. While documents can have large varieties of customization, all documents can be shared freely and have the right to do so, creativity can be represented by documents, also. History, events, examples, opinions, etc. all can be expressed in documents."
False document,A false document is a technique by which an author aims to increase verisimilitude in a work of fiction by inventing and inserting or mentioning documents that appear to be factual. The goal of a false document is to convince an audience that what is being presented is factual.
Book of Documents,"The Book of Documents (Shūjīng, earlier Shu King) or Classic of History, also known as the Shangshu (“Venerated Documents”), is one of the Five Classics of ancient Chinese literature.  It is a collection of rhetorical prose attributed to figures of ancient China, and served as the foundation of Chinese political philosophy for over 2,000 years.
The Book of Documents was the subject of one of China's oldest literary controversies, between proponents of different versions of the text. A version was preserved from Qin Shi Huang's burning of books and burying of scholars by scholar Fu Sheng, in 29 sections (pian 篇). This group of texts were referred to as ""Modern Script"" jinwen 今文, because written with the script in use at the beginning of the Western Han dyansty. According to Western Han dynasty documents, new textual material was  discovered in the wall of Confucius' family estate in Qufu by his descendant Kong Anguo in the late 2nd century BC. This new material was referred to as ""Old Script"" guwen 古文, because written in the script that predated the standardization of Chinese script enforced during the Qing dynasty. Compared to the ""Modern Script"" texts, the ""Old Script"" material had 16 more texts. However, this seems to have been lost at the end of the Eastern Han dynasty, while the ""Modern Script"" text enjoyed circulation, in particular in Ouyang Gao’s 歐陽高 (a. 136 BCE) study of it, the Ouyang Shangshu 歐陽尚書. This was the basis of studies by Ma Rong and Zheng Xuan in the Easter Han Dynasty. By the end of the second century CE, there was knowledge that the Shangshu at some point included more than the ""Modern Script"" text. This likely prompted scholars to recreate the ""Old Script"" texts said to have once belonged to the Shangshu, a process that culminated with the presentation of a 58 section (59 if the preface is included in the count) Shangshu to the Eastern Jin court, in 317 CE, by Mei Ze 梅頤.
This version was accepted, among doubts, and later was canonized as part of Kong Yingda's project. It was only in the 17th century that Qing dynasty scholar Yan Ruoqu demonstrated that the ""Old Script"" were actually fabrications ""reconstructed"" in the 3rd or 4th centuries AD.
In the transmitted edition, texts are grouped into four sections representing different eras: the semi-mythical reign of Yu the Great, and the Xia, Shang and Zhou dynasties.  The Zhou section accounts for over half the text.  Some of its New Text chapters are among the earliest examples of Chinese prose, recording speeches from the early years of the Zhou dynasty in the late 11th century BC.  Although the other three sections purport to record earlier material, most scholars believe that even the New Script chapters in these sections were composed later than those in the Zhou section, with chapters relating to the earliest periods being as recent as the 4th or 3rd centuries BC."
Halloween documents,"The Halloween documents comprise a series of confidential Microsoft memoranda on potential strategies relating to free software, open-source software, and to Linux in particular, and a series of media responses to these memoranda. Both the leaked documents and the responses were published by Eric S. Raymond in 1998.The documents are associated with Halloween because many of them were originally leaked close to October 31 in different years.

"
Electronic document,"An electronic document is any electronic media content (other than computer programs or system files) that is intended to be used in either an electronic form or as printed output. Originally, any computer data were considered as something internal — the final data output was always on paper. However, the development of computer networks has made it so that in most cases it is much more convenient to distribute electronic documents than printed ones. The improvements in electronic visual display technologies made it possible to view documents on screen instead of printing them (thus saving paper and the space required to store the printed copies).However, using electronic documents for final presentation instead of paper has created the problem of multiple incompatible file formats. Even plain text computer files are not free from this problem — e.g. under MS-DOS, most programs could not work correctly with UNIX-style text files (see newline), and for non-English speakers, the different code pages always have been a source of trouble.
Even more problems are connected with complex file formats of various word processors, spreadsheets, and graphics software. To alleviate the problem, many software companies distribute free file viewers for their proprietary file formats (one example is Adobe's Acrobat Reader). The other solution is the development of standardized non-proprietary file formats (such as HTML and OpenDocument), and electronic documents for specialized uses have specialized formats – the specialized electronic articles in physics use TeX or PostScript."
Historical document,"Historical documents are original documents that contain important historical information about a person, place, or event and can thus serve as primary sources as important ingredients of the historical methodology.
Significant historical documents can be deeds, laws, accounts of battles (often given by the victors or persons sharing their viewpoint), or the exploits of the powerful. Though these documents are of historical interest, they do not detail the daily lives of ordinary people, or the way society functioned. Anthropologists, historians and archeologists generally are more interested in documents that describe the day-to-day lives of ordinary people, indicating what they ate, their interaction with other members of their households and social groups, and their states of mind. It is this information that allows them to try to understand and describe the way society was functioning at any particular time in history. Greek ostraka provide good examples of historical documents from ""among the common people"".
Many documents that are produced today, such as personal letters, pictures, contracts, newspapers, and medical records, would be considered valuable historical documents in the future. However most of these will be lost in the future since they are either printed on ordinary paper which has a limited lifespan, or even stored in digital formats, then lost track over time.
Some companies and government entities are attempting to increase the number of documents that will survive the passage of time, by taking into account the preservation issues, and either printing documents in a manner that would increase the likelihood of them surviving indefinitely, or placing selected documents in time capsules or other special storage environments."
Document management system,"A document management system (DMS) is usually a computerized system used to store, share, track and manage files or documents. Some systems include history tracking where a log of the various versions created and modified by different users is recorded. The term has some overlap with the concepts of content management systems. It is often viewed as a component of enterprise content management (ECM) systems and related to digital asset management, document imaging, workflow systems and records management systems."
Constitution,"Institutions are humanly devised structures of rules and norms that shape and constrain individual behavior. All definitions of institutions generally entail that there is a level of persistence and continuity. Laws, rules, social conventions and norms are all examples of institutions. Institutions vary in their level of formality and informality.Institutions are a principal object of study in social sciences such as political science, anthropology, economics, and sociology (the latter described by Émile Durkheim as the ""science of institutions, their genesis and their functioning""). Primary or meta-institutions are institutions such as the family or money that are broad enough to encompass sets of related institutions. Institutions are also a central concern for law, the formal mechanism for political rule-making and enforcement. Historians study and document the founding, growth, decay and development of institutions as part of political, economic and cultural history."
Identity document,
Google Docs,"Google Docs is an online word processor included as part of the free, web-based Google Docs Editors suite offered by Google, which also includes: Google Sheets, Google Slides, Google Drawings, Google Forms, Google Sites and Google Keep. Google Docs is accessible via an internet browser as a web-based application and is also available as a mobile app on Android and iOS and as a desktop application on Google's ChromeOS.
Google Docs allows users to create and edit documents online while collaborating with other users in real time. Edits are tracked by the user making the edit, with a revision history presenting changes. An editor's position is highlighted with an editor-specific color and cursor, and a permissions system regulates what users can do. Updates have introduced features using machine learning, including ""Explore"", offering search results based on the contents of a document, and ""Action items"", allowing users to assign tasks to other users.Google Docs supports opening and saving documents in the standard OpenDocument format as well as in Rich text format, plain Unicode text, zipped HTML, and Microsoft Word. Exporting to PDF and EPUB formats are implemented."
stemming,"In linguistic morphology and information retrieval, stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form—generally a written word form. The stem need not be identical to the morphological root of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root. Algorithms for stemming have been studied in computer science since the 1960s. Many search engines treat words with the same stem as synonyms as a kind of query expansion, a process called conflation.
A computer program or subroutine that stems word may be called a stemming program, stemming algorithm, or stemmer."
Stemming,"In linguistic morphology and information retrieval, stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form—generally a written word form. The stem need not be identical to the morphological root of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root. Algorithms for stemming have been studied in computer science since the 1960s. Many search engines treat words with the same stem as synonyms as a kind of query expansion, a process called conflation.
A computer program or subroutine that stems word may be called a stemming program, stemming algorithm, or stemmer."
documents,"A document is a written, drawn, presented, or memorialized representation of thought, often the manifestation of non-fictional, as well as fictional, content.  The word originates from the Latin Documentum, which denotes a ""teaching"" or ""lesson"": the verb doceō denotes ""to teach"". In the past, the word was usually used to denote written proof useful as evidence of a truth or fact. In the computer age, ""document"" usually denotes a primarily textual computer file, including its structure and format, e.g. fonts, colors, and images. Contemporarily, ""document"" is not defined by its transmission medium, e.g., paper, given the existence of electronic documents. ""Documentation"" is distinct because it has more denotations than ""document"". Documents are also distinguished from ""realia"", which are three-dimensional objects that would otherwise satisfy the definition of ""document"" because they memorialize or represent thought; documents are considered more as 2-dimensional representations. While documents can have large varieties of customization, all documents can be shared freely and have the right to do so, creativity can be represented by documents, also. History, events, examples, opinions, etc. all can be expressed in documents."
Document,"A document is a written, drawn, presented, or memorialized representation of thought, often the manifestation of non-fictional, as well as fictional, content.  The word originates from the Latin Documentum, which denotes a ""teaching"" or ""lesson"": the verb doceō denotes ""to teach"". In the past, the word was usually used to denote written proof useful as evidence of a truth or fact. In the computer age, ""document"" usually denotes a primarily textual computer file, including its structure and format, e.g. fonts, colors, and images. Contemporarily, ""document"" is not defined by its transmission medium, e.g., paper, given the existence of electronic documents. ""Documentation"" is distinct because it has more denotations than ""document"". Documents are also distinguished from ""realia"", which are three-dimensional objects that would otherwise satisfy the definition of ""document"" because they memorialize or represent thought; documents are considered more as 2-dimensional representations. While documents can have large varieties of customization, all documents can be shared freely and have the right to do so, creativity can be represented by documents, also. History, events, examples, opinions, etc. all can be expressed in documents."
False document,A false document is a technique by which an author aims to increase verisimilitude in a work of fiction by inventing and inserting or mentioning documents that appear to be factual. The goal of a false document is to convince an audience that what is being presented is factual.
Book of Documents,"The Book of Documents (Shūjīng, earlier Shu King) or Classic of History, also known as the Shangshu (“Venerated Documents”), is one of the Five Classics of ancient Chinese literature.  It is a collection of rhetorical prose attributed to figures of ancient China, and served as the foundation of Chinese political philosophy for over 2,000 years.
The Book of Documents was the subject of one of China's oldest literary controversies, between proponents of different versions of the text. A version was preserved from Qin Shi Huang's burning of books and burying of scholars by scholar Fu Sheng, in 29 sections (pian 篇). This group of texts were referred to as ""Modern Script"" jinwen 今文, because written with the script in use at the beginning of the Western Han dyansty. According to Western Han dynasty documents, new textual material was  discovered in the wall of Confucius' family estate in Qufu by his descendant Kong Anguo in the late 2nd century BC. This new material was referred to as ""Old Script"" guwen 古文, because written in the script that predated the standardization of Chinese script enforced during the Qing dynasty. Compared to the ""Modern Script"" texts, the ""Old Script"" material had 16 more texts. However, this seems to have been lost at the end of the Eastern Han dynasty, while the ""Modern Script"" text enjoyed circulation, in particular in Ouyang Gao’s 歐陽高 (a. 136 BCE) study of it, the Ouyang Shangshu 歐陽尚書. This was the basis of studies by Ma Rong and Zheng Xuan in the Easter Han Dynasty. By the end of the second century CE, there was knowledge that the Shangshu at some point included more than the ""Modern Script"" text. This likely prompted scholars to recreate the ""Old Script"" texts said to have once belonged to the Shangshu, a process that culminated with the presentation of a 58 section (59 if the preface is included in the count) Shangshu to the Eastern Jin court, in 317 CE, by Mei Ze 梅頤.
This version was accepted, among doubts, and later was canonized as part of Kong Yingda's project. It was only in the 17th century that Qing dynasty scholar Yan Ruoqu demonstrated that the ""Old Script"" were actually fabrications ""reconstructed"" in the 3rd or 4th centuries AD.
In the transmitted edition, texts are grouped into four sections representing different eras: the semi-mythical reign of Yu the Great, and the Xia, Shang and Zhou dynasties.  The Zhou section accounts for over half the text.  Some of its New Text chapters are among the earliest examples of Chinese prose, recording speeches from the early years of the Zhou dynasty in the late 11th century BC.  Although the other three sections purport to record earlier material, most scholars believe that even the New Script chapters in these sections were composed later than those in the Zhou section, with chapters relating to the earliest periods being as recent as the 4th or 3rd centuries BC."
Halloween documents,"The Halloween documents comprise a series of confidential Microsoft memoranda on potential strategies relating to free software, open-source software, and to Linux in particular, and a series of media responses to these memoranda. Both the leaked documents and the responses were published by Eric S. Raymond in 1998.The documents are associated with Halloween because many of them were originally leaked close to October 31 in different years.

"
Electronic document,"An electronic document is any electronic media content (other than computer programs or system files) that is intended to be used in either an electronic form or as printed output. Originally, any computer data were considered as something internal — the final data output was always on paper. However, the development of computer networks has made it so that in most cases it is much more convenient to distribute electronic documents than printed ones. The improvements in electronic visual display technologies made it possible to view documents on screen instead of printing them (thus saving paper and the space required to store the printed copies).However, using electronic documents for final presentation instead of paper has created the problem of multiple incompatible file formats. Even plain text computer files are not free from this problem — e.g. under MS-DOS, most programs could not work correctly with UNIX-style text files (see newline), and for non-English speakers, the different code pages always have been a source of trouble.
Even more problems are connected with complex file formats of various word processors, spreadsheets, and graphics software. To alleviate the problem, many software companies distribute free file viewers for their proprietary file formats (one example is Adobe's Acrobat Reader). The other solution is the development of standardized non-proprietary file formats (such as HTML and OpenDocument), and electronic documents for specialized uses have specialized formats – the specialized electronic articles in physics use TeX or PostScript."
Historical document,"Historical documents are original documents that contain important historical information about a person, place, or event and can thus serve as primary sources as important ingredients of the historical methodology.
Significant historical documents can be deeds, laws, accounts of battles (often given by the victors or persons sharing their viewpoint), or the exploits of the powerful. Though these documents are of historical interest, they do not detail the daily lives of ordinary people, or the way society functioned. Anthropologists, historians and archeologists generally are more interested in documents that describe the day-to-day lives of ordinary people, indicating what they ate, their interaction with other members of their households and social groups, and their states of mind. It is this information that allows them to try to understand and describe the way society was functioning at any particular time in history. Greek ostraka provide good examples of historical documents from ""among the common people"".
Many documents that are produced today, such as personal letters, pictures, contracts, newspapers, and medical records, would be considered valuable historical documents in the future. However most of these will be lost in the future since they are either printed on ordinary paper which has a limited lifespan, or even stored in digital formats, then lost track over time.
Some companies and government entities are attempting to increase the number of documents that will survive the passage of time, by taking into account the preservation issues, and either printing documents in a manner that would increase the likelihood of them surviving indefinitely, or placing selected documents in time capsules or other special storage environments."
Document management system,"A document management system (DMS) is usually a computerized system used to store, share, track and manage files or documents. Some systems include history tracking where a log of the various versions created and modified by different users is recorded. The term has some overlap with the concepts of content management systems. It is often viewed as a component of enterprise content management (ECM) systems and related to digital asset management, document imaging, workflow systems and records management systems."
Constitution,"Institutions are humanly devised structures of rules and norms that shape and constrain individual behavior. All definitions of institutions generally entail that there is a level of persistence and continuity. Laws, rules, social conventions and norms are all examples of institutions. Institutions vary in their level of formality and informality.Institutions are a principal object of study in social sciences such as political science, anthropology, economics, and sociology (the latter described by Émile Durkheim as the ""science of institutions, their genesis and their functioning""). Primary or meta-institutions are institutions such as the family or money that are broad enough to encompass sets of related institutions. Institutions are also a central concern for law, the formal mechanism for political rule-making and enforcement. Historians study and document the founding, growth, decay and development of institutions as part of political, economic and cultural history."
Identity document,
Google Docs,"Google Docs is an online word processor included as part of the free, web-based Google Docs Editors suite offered by Google, which also includes: Google Sheets, Google Slides, Google Drawings, Google Forms, Google Sites and Google Keep. Google Docs is accessible via an internet browser as a web-based application and is also available as a mobile app on Android and iOS and as a desktop application on Google's ChromeOS.
Google Docs allows users to create and edit documents online while collaborating with other users in real time. Edits are tracked by the user making the edit, with a revision history presenting changes. An editor's position is highlighted with an editor-specific color and cursor, and a permissions system regulates what users can do. Updates have introduced features using machine learning, including ""Explore"", offering search results based on the contents of a document, and ""Action items"", allowing users to assign tasks to other users.Google Docs supports opening and saving documents in the standard OpenDocument format as well as in Rich text format, plain Unicode text, zipped HTML, and Microsoft Word. Exporting to PDF and EPUB formats are implemented."
edit distance,"In computational linguistics and computer science, edit distance is a string metric, i.e. a way of quantifying how dissimilar two strings (e.g., words) are to one another, that is measured by counting the minimum number of operations required to transform one string into the other. Edit distances find applications in natural language processing, where automatic spelling correction can determine candidate corrections for a misspelled word by selecting words from a dictionary that have a low distance to the word in question. In bioinformatics, it can be used to quantify the similarity of DNA sequences, which can be viewed as strings of the letters A, C, G and T.
Different definitions of an edit distance use different sets of string operations. Levenshtein distance operations are the removal, insertion, or substitution of a character in the string. Being the most common metric, the term Levenshtein distance is often used interchangeably with edit distance."
Edit distance,"In computational linguistics and computer science, edit distance is a string metric, i.e. a way of quantifying how dissimilar two strings (e.g., words) are to one another, that is measured by counting the minimum number of operations required to transform one string into the other. Edit distances find applications in natural language processing, where automatic spelling correction can determine candidate corrections for a misspelled word by selecting words from a dictionary that have a low distance to the word in question. In bioinformatics, it can be used to quantify the similarity of DNA sequences, which can be viewed as strings of the letters A, C, G and T.
Different definitions of an edit distance use different sets of string operations. Levenshtein distance operations are the removal, insertion, or substitution of a character in the string. Being the most common metric, the term Levenshtein distance is often used interchangeably with edit distance."
Graph edit distance,"In mathematics and computer science, graph edit distance (GED) is a measure of similarity (or dissimilarity) between two graphs.
The concept of graph edit distance was first formalized mathematically by Alberto Sanfeliu and King-Sun Fu in 1983.
A major application of graph edit distance is in inexact graph matching, such
as error-tolerant pattern recognition in machine learning.The graph edit distance between two graphs is related to the
string edit distance between strings.
With the interpretation of strings as 
connected, directed acyclic graphs of 
maximum degree one, classical definitions
of edit distance such as Levenshtein distance,Hamming distance
and Jaro–Winkler distance may be interpreted as graph edit distances
between suitably constrained graphs.  Likewise, graph edit distance is
also a generalization of tree edit distance between
rooted trees."
Levenshtein distance,"In information theory, linguistics, and computer science, the Levenshtein distance is a string metric for measuring the difference between two sequences. Informally, the Levenshtein distance between two words is the minimum number of single-character edits (insertions, deletions or substitutions) required to change one word into the other. It is named after the Soviet mathematician Vladimir Levenshtein, who considered this distance in 1965.Levenshtein distance may also be referred to as edit distance, although that term may also denote a larger family of distance metrics known collectively as edit distance.: 32  It is closely related to pairwise string alignments.

"
Damerau–Levenshtein distance,"In information theory and computer science, the Damerau–Levenshtein distance (named after Frederick J. Damerau and Vladimir I. Levenshtein) is a string metric for measuring the edit distance between two sequences. Informally, the Damerau–Levenshtein distance between two words is the minimum number of operations (consisting of insertions, deletions or substitutions of a single character, or transposition of two adjacent characters) required to change one word into the other.
The Damerau–Levenshtein distance differs from the classical Levenshtein distance by including transpositions among its allowable operations in addition to the three classical single-character edit operations (insertions, deletions and substitutions).In his seminal paper, Damerau stated that in an investigation of spelling errors for an information-retrieval system, more than 80% were a result of a single error of one of the four types. Damerau's paper considered only misspellings that could be corrected with at most one edit operation. While the original motivation was to measure distance between human misspellings to improve applications such as spell checkers, Damerau–Levenshtein distance has also seen uses in biology to measure the variation between protein sequences.

"
Distance,"Distance is a numerical or occasionally qualitative measurement of how far apart objects or points are. In physics or everyday usage, distance may refer to a physical length or an estimation based on other criteria (e.g. ""two counties over"").  Since spatial cognition is a rich source of conceptual metaphors in human thought, the term is also frequently used metaphorically to mean a measurement of the amount of difference between two similar objects (such as statistical distance between probability distributions or edit distance between strings of text) or a degree of separation (as exemplified by distance between people in a social network).  Most such notions of distance, both physical and metaphorical, are formalized in mathematics using the notion of a metric space.
In the social sciences, distance can refer to a qualitative measurement of separation, such as social distance or psychological distance."
Jaro–Winkler distance,"In computer science and statistics, the Jaro–Winkler distance is a string metric measuring an edit distance between two sequences. It is a variant proposed in 1990 by William E. Winkler of the Jaro distance metric (1989, Matthew A. Jaro).
The Jaro–Winkler distance uses a prefix scale 
  
    
      
        p
      
    
    {\displaystyle p}
   which gives more favourable ratings to strings that match from the beginning for a set prefix length 
  
    
      
        ℓ
      
    
    {\displaystyle \ell }
  .
The higher the Jaro–Winkler distance for two strings is, the less similar the strings are. The score is normalized such that 0 means an exact match and 1 means there is no similarity. The original paper actually defined the metric in terms of similarity, so the distance is defined as the inversion of that value (distance = 1 − similarity).
Although often referred to as a distance metric, the Jaro–Winkler distance is not a metric in the mathematical sense of that term because it does not obey the triangle inequality.

"
Wagner–Fischer algorithm,"In computer science, the Wagner–Fischer algorithm is a dynamic programming algorithm that computes the edit distance between two strings of characters."
Hamming distance,"In information theory, the Hamming distance between two strings of equal length is the number of positions at which the corresponding symbols are different. In other words, it measures the minimum number of substitutions required to change one string into the other, or the minimum number of errors that could have transformed one string into the other. In a more general context, the Hamming distance is one of several string metrics for measuring the edit distance between two sequences. It is named after the American mathematician Richard Hamming.
A major application is in coding theory, more specifically to block codes, in which the equal-length strings are vectors over a finite field."
Time Warp Edit Distance,"Time Warp Edit Distance (TWED) is a measure of similarity (or dissimilarity) for discrete time series matching with time 'elasticity'. In comparison to other distance measures, (e.g. DTW (dynamic time warping) or  LCS (longest common subsequence problem)), TWED is a metric. Its computational time complexity is 
  
    
      
        O
        (
        
          n
          
            2
          
        
        )
      
    
    {\displaystyle O(n^{2})}
  , but can be drastically reduced in some specific situations by using a corridor to reduce the search space. Its memory space complexity can be reduced to 
  
    
      
        O
        (
        n
        )
      
    
    {\displaystyle O(n)}
  . It was first proposed in 2009 by P.-F. Marteau."
Word error rate,"Word error rate (WER) is a common metric of the performance of a speech recognition or machine translation system.
The general difficulty of measuring performance lies in the fact that the recognized word sequence can have a different length from the reference word sequence (supposedly the correct one). The WER is derived from the Levenshtein distance, working at the word level instead of the phoneme level. The WER is a valuable tool for comparing different systems as well as for evaluating improvements within one system. This kind of measurement, however, provides no details on the nature of translation errors and further work is therefore required to identify the main source(s) of error and to focus any research effort.
This problem is solved by first aligning the recognized word sequence with the reference (spoken) word sequence using dynamic string alignment. Examination of this issue is seen through a theory called the power law that states the correlation between perplexity and word error rate.Word error rate can then be computed as:

  
    
      
        
          
            W
            E
            R
          
        
        =
        
          
            
              S
              +
              D
              +
              I
            
            N
          
        
        =
        
          
            
              S
              +
              D
              +
              I
            
            
              S
              +
              D
              +
              C
            
          
        
      
    
    {\displaystyle {\mathit {WER}}={\frac {S+D+I}{N}}={\frac {S+D+I}{S+D+C}}}
  where

S is the number of substitutions,
D is the number of deletions,
I is the number of insertions,
C is the number of correct words,
N is the number of words in the reference (N=S+D+C)The intuition behind 'deletion' and 'insertion' is how to get from the reference to the hypothesis. So if we have the reference ""This is wikipedia"" and hypothesis ""This _ wikipedia"", we call it a deletion.
When reporting the performance of a speech recognition system, sometimes word accuracy (WAcc) is used instead:

  
    
      
        
          
            W
            A
            c
            c
          
        
        =
        1
        −
        
          
            W
            E
            R
          
        
        =
        
          
            
              N
              −
              S
              −
              D
              −
              I
            
            N
          
        
        =
        
          
            
              C
              −
              I
            
            N
          
        
      
    
    {\displaystyle {\mathit {WAcc}}=1-{\mathit {WER}}={\frac {N-S-D-I}{N}}={\frac {C-I}{N}}}
  Note that since N is the number of words in the reference, the word error rate can be larger than 1.0, and thus, the word accuracy can be smaller than 0.0."
index construction,"The Russell 1000 Index is a stock market index that tracks the highest-ranking 1,000 stocks in the Russell 3000 Index, which represent about 93% of the total market capitalization of that index. As of 31 December 2021, the stocks of the Russell 1000 Index had a weighted average market capitalization of $608.1 billion and a median market capitalization of $15.1 billion. As of 8 May 2020, components ranged in market capitalization from $1.8 billion to $1.4 trillion. The index, which was launched on January 1, 1984, is maintained by FTSE Russell, a subsidiary of the London Stock Exchange Group.
The ticker symbol is ^RUI. There are several exchange-traded funds and mutual funds that track the index."
Construction,"Construction is a general term meaning the art and science to form objects, systems, or organizations, and comes from Latin constructio (from com- ""together"" and struere ""to pile up"") and Old French construction. To construct is the verb: the act of building, and the noun is construction: how something is built, the nature of its structure.
In its most widely used context, construction covers the processes involved in delivering buildings, infrastructure, industrial facilities and associated activities through to the end of their life. It typically starts with planning, financing, and design, and continues until the asset is built and ready for use; construction also covers repairs and maintenance work, any works to expand, extend and improve the asset, and its eventual demolition, dismantling or decommissioning.
The construction industry contributes significantly to many countries’ gross domestic products (GDP). Global expenditure on construction activities was about $4 trillion in 2012. Today, expenditure on the construction industry exceeds $11 trillion a year, equivalent to about 13 percent of global GDP. This spending was forecast to rise to around $14.8 trillion in 2030.Although the construction industry promotes economic development and brings many non-monetary benefits to many countries, it is one of the most hazardous industries. For example, about 20% (1,061) of US industry fatalities in 2019 happened in construction."
dynamic indexing,"Search engine indexing is the collecting, parsing, and storing of data to facilitate fast and accurate information retrieval. Index design incorporates interdisciplinary concepts from linguistics, cognitive psychology, mathematics, informatics, and computer science. An alternate name for the process, in the context of search engines designed to find web pages on the Internet, is web indexing.
Popular engines focus on the full-text indexing of online, natural language documents. Media types such as pictures, video, audio, and graphics are also searchable.
Meta search engines reuse the indices of other services and do not store a local index whereas cache-based search engines permanently store the index along with the corpus. Unlike full-text indices, partial-text services restrict the depth indexed to reduce index size. Larger services typically perform indexing at a predetermined time interval due to the required time and processing costs, while agent-based search engines index in real time.

"
Search engine indexing,"Search engine indexing is the collecting, parsing, and storing of data to facilitate fast and accurate information retrieval. Index design incorporates interdisciplinary concepts from linguistics, cognitive psychology, mathematics, informatics, and computer science. An alternate name for the process, in the context of search engines designed to find web pages on the Internet, is web indexing.
Popular engines focus on the full-text indexing of online, natural language documents. Media types such as pictures, video, audio, and graphics are also searchable.
Meta search engines reuse the indices of other services and do not store a local index whereas cache-based search engines permanently store the index along with the corpus. Unlike full-text indices, partial-text services restrict the depth indexed to reduce index size. Larger services typically perform indexing at a predetermined time interval due to the required time and processing costs, while agent-based search engines index in real time.

"
Web server directory index,"When an HTTP client (generally a web browser) requests a URL that points to a directory structure instead of an actual web page within the directory structure, the web server will generally serve a default page, which is often referred to as a main or ""index"" page.
A common filename for such a page is index.html, but most modern HTTP servers offer a configurable list of filenames that the server can use as an index. If a server is configured to support server-side scripting, the list will usually include entries allowing dynamic content to be used as the index page (e.g. index.cgi, index.pl, index.php, index.shtml, index.jsp, default.asp) even though it may be more appropriate to still specify the HTML output (index.html.php or index.html.aspx), as this should not be taken for granted. An example is the popular open source web server Apache, where the list of filenames is controlled by the DirectoryIndex directive in the main server configuration file or in the configuration file for that directory. It is possible to not use file extensions at all, and be neutral to content delivery methods, and set the server to automatically pick the best file through content negotiation.
If the server is unable to find a file with any of the names listed in its configuration, it may either return an error (usually 403 Index Listing Forbidden or 404 Not Found) or generate its own index page listing the files in the directory. Usually this option, often named autoindex, is also configurable."
Dynamic web page,"A server-side dynamic web page is a web page whose construction is controlled by an application server processing server-side scripts. In server-side scripting, parameters determine how the assembly of every new web page proceeds, and including the setting up of more client-side processing.
A client-side dynamic web page processes the web page using JavaScript running in the browser as it loads. JavaScript can interact with the page via Document Object Model, or DOM, to query page state and modify it. Even though a web page can be dynamic on the client-side, it can still be hosted on a static hosting service such as GitHub Pages or Amazon S3 as long as there isn't any server-side code included.
A dynamic web page is then reloaded by the user or by a computer program to change some variable content. The updating information could come from the server, or from changes made to that page's DOM. This may or may not truncate the browsing history or create a saved version to go back to, but a dynamic web page update using AJAX technologies will neither create a page to go back to, nor truncate the web browsing history forward of the displayed page. Using AJAX, the end user gets one dynamic page managed as a single page in the web browser while the actual web content rendered on that page can vary. The AJAX engine sits only on the browser requesting parts of its DOM, the DOM, for its client, from an application server. A particular application server could offer a standardized REST style interface to offer services to the web application.DHTML is the umbrella term for technologies and methods used to create web pages that are not static web pages, though it has fallen out of common use since the popularization of AJAX, a term which is now itself rarely used. Client-side-scripting, server-side scripting, or a combination of these make for the dynamic web experience in a browser.

"
Dynamic array,"In computer science, a dynamic array, growable array, resizable array, dynamic table, mutable array, or array list is a random access, variable-size list data structure that allows elements to be added or removed. It is supplied with standard libraries in many modern mainstream programming languages. Dynamic arrays overcome a limit of static arrays, which have a fixed capacity that needs to be specified at allocation.
A dynamic array is not the same thing as a dynamically allocated array or variable-length array, either of which is an array whose size is fixed when the array is allocated, although a dynamic array may use such a fixed-size array as a back end."
Dynamic programming,"Dynamic programming is both a mathematical optimization method and a computer programming method. The method was developed by Richard Bellman in the 1950s and has found applications in numerous fields, from aerospace engineering to economics.
In both contexts it refers to simplifying a complicated problem by breaking it down into simpler sub-problems in a recursive manner. While some decision problems cannot be taken apart this way, decisions that span several points in time do often break apart recursively. Likewise, in computer science, if a problem can be solved optimally by breaking it into sub-problems and then recursively finding the optimal solutions to the sub-problems, then it is said to have optimal substructure.
If sub-problems can be nested recursively inside larger problems, so that dynamic programming methods are applicable, then there is a relation between the value of the larger problem and the values of the sub-problems. In the optimization literature this relationship is called the Bellman equation.

"
Array (data type),"In computer science, array is a data type that represents a collection of elements (values or variables), each selected by one or more indices (identifying keys) that can be computed at run time during program execution.  Such a collection is usually called an array variable or array value.  By analogy with the mathematical concepts vector and matrix, array types with one and two indices are often called vector type and matrix type, respectively. More generally, a multidimensional array type can be called a tensor type, by anology with the physical concept, tensor.Language support for array types may include certain built-in array data types, some syntactic constructions (array type constructors) that the programmer may use to define such types and declare array variables, and special notation for indexing array elements.  For example, in the Pascal programming language, the declaration type MyTable = array [1..4,1..2] of integer, defines a new array data type called MyTable. The declaration var A: MyTable then defines a variable A of that type, which is an aggregate of eight elements, each being an integer variable identified by two indices. In the Pascal program, those elements are denoted A[1,1], A[1,2], A[2,1], …, A[4,2].  Special array types are often defined by the language's standard libraries.
Dynamic lists are also more common and easier to implement than dynamic arrays. Array types are distinguished from record types mainly because they allow the element indices to be computed at run time, as in the Pascal assignment A[I,J] := A[N-I,2*J].  Among other things, this feature allows a single iterative statement to process arbitrarily many elements of an array variable.
In more theoretical contexts, especially in type theory and in the description of abstract algorithms, the terms ""array"" and ""array type"" sometimes refer to an abstract data type (ADT) also called abstract array or may refer to an associative array, a mathematical model with the basic operations and behavior of a typical array type in most languages — basically, a collection of elements that are selected by indices computed at run-time.
Depending on the language, array types may overlap (or be identified with) other data types that describe aggregates of values, such as lists and strings.  Array types are often implemented by array data structures, but sometimes by other means, such as hash tables, linked lists, or search trees."
Dynamic time warping,"In time series analysis, dynamic time warping (DTW) is an algorithm for measuring similarity between two temporal sequences, which may vary in speed.  For instance, similarities in walking could be detected using DTW, even if one person was walking faster than the other, or if there were accelerations and decelerations during the course of an observation. DTW has been applied to temporal sequences of video, audio, and graphics data — indeed, any data that can be turned into a linear sequence can be analyzed with DTW. A well-known application has been automatic speech recognition, to cope with different speaking speeds. Other applications include speaker recognition and online signature recognition. It can also be used in partial shape matching applications.
In general, DTW is a method that calculates an optimal match between two given sequences (e.g. time series) with certain restriction and rules:

Every index from the first sequence must be matched with one or more indices from the other sequence, and vice versa
The first index from the first sequence must be matched with the first index from the other sequence (but it does not have to be its only match)
The last index from the first sequence must be matched with the last index from the other sequence (but it does not have to be its only match)
The mapping of the indices from the first sequence to indices from the other sequence must be monotonically increasing, and vice versa, i.e. if 
  
    
      
        j
        >
        i
      
    
    {\displaystyle j>i}
   are indices from the first sequence, then there must not be two indices 
  
    
      
        l
        >
        k
      
    
    {\displaystyle l>k}
   in the other sequence, such that index 
  
    
      
        i
      
    
    {\displaystyle i}
   is matched with index 
  
    
      
        l
      
    
    {\displaystyle l}
   and index 
  
    
      
        j
      
    
    {\displaystyle j}
   is matched with index 
  
    
      
        k
      
    
    {\displaystyle k}
  , and vice versaThe optimal match is denoted by the match that satisfies all the restrictions and the rules and that has the minimal cost, where the cost is computed as the sum of absolute differences, for each matched pair of indices, between their values.
The sequences are ""warped"" non-linearly in the time dimension to determine a measure of their similarity independent of certain non-linear variations in the time dimension. This sequence alignment method is often used in time series classification. Although DTW measures a distance-like quantity between two given sequences, it doesn't guarantee the triangle inequality to hold.
In addition to a similarity measure between the two sequences, a so called ""warping path"" is produced, by warping according to this path the two signals may be aligned in time. The signal with an original set of points X(original), Y(original) is transformed to X(warped), Y(warped). This finds applications in genetic sequence and audio synchronisation. In a related technique sequences of varying speed may be averaged using this technique see the average sequence section.
This is conceptually very similar to the Needleman–Wunsch algorithm."
Array (data structure),"In computer science, an array is a data structure consisting of a collection of elements (values or variables), each identified by at least one array index or key. An array is stored such that the position of each element can be computed from its index tuple by a mathematical formula. The simplest type of data structure is a linear array, also called one-dimensional array.
For example, an array of ten 32-bit (4-byte) integer variables, with indices 0 through 9, may be stored as ten words at memory addresses 2000, 2004, 2008, ..., 2036, (in hexadecimal: 0x7D0, 0x7D4, 0x7D8, ..., 0x7F4) so that the element with index i has the address 2000 + (i × 4).
The memory address of the first element of an array is called first address, foundation address, or base address.
Because the mathematical concept of a matrix can be represented as a two-dimensional grid, two-dimensional arrays are also sometimes called ""matrices"". In some cases the term ""vector"" is used in computing to refer to an array, although tuples rather than vectors are the more mathematically correct equivalent. Tables are often implemented in the form of arrays, especially lookup tables; the word ""table"" is sometimes used as a synonym of array.
Arrays are among the oldest and most important data structures, and are used by almost every program. They are also used to implement many other data structures, such as lists and strings. They effectively exploit the addressing logic of computers. In most modern computers and many external storage devices, the memory is a one-dimensional array of words, whose indices are their addresses. Processors, especially vector processors, are often optimized for array operations.
Arrays are useful mostly because the element indices can be computed at run time. Among other things, this feature allows a single iterative statement to process arbitrarily many elements of an array. For that reason, the elements of an array data structure are required to have the same size and should use the same data representation. The set of valid index tuples and the addresses of the elements (and hence the element addressing formula) are usually, but not always, fixed while the array is in use.
The term ""array"" may also refer to an array data type, a kind of data type provided by most high-level programming languages that consists of a collection of values or variables that can be selected by one or more indices computed at run-time. Array types are often implemented by array structures; however, in some languages they may be implemented by hash tables, linked lists, search trees, or other data structures.
The term is also used, especially in the description of algorithms, to mean associative array or ""abstract array"", a theoretical computer science model (an abstract data type or ADT) intended to capture the essential properties of arrays."
Enhanced indexing,"In finance, enhanced indexing is any indexing strategy employed with the intention of outperforming strict indexing. Enhanced indexing attempts to generate modest excess returns compared to traditional index funds and other passive management techniques."
Dynamic response index,"The Dynamic Response Index (DRI) is a measure of the likelihood of spinal damage arising from a vertical shock load such as might be encountered in a military environment (i.e., during a mine blast, or in an ejection seat).  The DRI is a dimensionless number which is proportional to the maximum spinal compression suffered during the event.
The DRI is derived as the solution to an equation which models the human spine as a lumped single-degree-of-freedom spring-shock absorber system.  The model uses an ordinary linear second-order differential equation with constant coefficients with spinal compression as the variable.  The forcing function in the equation is the accelerative shock load delivered to the pelvis by the event.  The equation is given below:
  
    
      
        
          
            
              
                d
                
                  2
                
              
              X
            
            
              d
              
                t
                
                  2
                
              
            
          
        
        +
        2
        ⋅
        ζ
        ⋅
        ω
        ⋅
        
          
            
              d
              X
            
            
              d
              t
            
          
        
        +
        
          ω
          
            2
          
        
        ⋅
        X
        =
        
          
            
              
                d
                
                  2
                
              
              z
            
            
              d
              
                t
                
                  2
                
              
            
          
        
      
    
    {\displaystyle {d^{2}X \over dt^{2}}+2\cdot \zeta \cdot \omega \cdot {dX \over dt}+\omega ^{2}\cdot X={d^{2}z \over dt^{2}}}
  
In this equation, X denotes the spinal compression, and d2z/dt2 denotes the time-dependent shock acceleration (the input).  The coefficients ω and ζ  are the lumped spinal frequency and damping, ω = 52.90 radians/second, ζ = 0.224.
The DRI is defined in terms of the maximum spinal compression (Xmax) calculated from the differential equation, DRI = (ω2/G)·Xmax, where G is the acceleration of gravity (9.806 m/s2).
The DRI measure was derived from research on cadavers, as well as from “willing volunteers” (essentially, aviators who activated the ejection seat mechanism).The limiting DRI value according to NATO STANAG 4569 is 17.7 with a 10% chance of serious injury.  This corresponds to a maximum spinal compression of about 62 mm.
A simple rule of thumb for short duration shocks (much less than about a quarter of a cycle, which means much less than about 30 ms) is DRI = 3.96·ΔV, where ΔV is the total impulse delivered by the shock in units of meters per second.  ΔV is simply the integral under the shock acceleration curve, equivalent to the “liftoff” velocity of the object undergoing shock loading.  For a constant impulse, the DRI decreases as the duration of the pulse increases beyond about 30 ms."
document collection,"An archive is an accumulation of historical records or materials – in any medium – or the physical facility in which they are located.Archives contain primary source documents that have accumulated over the course of an individual or organization's lifetime, and are kept to show the function of that person or organization. Professional archivists and historians generally understand archives to be records that have been naturally and necessarily generated as a product of regular legal, commercial, administrative, or social activities. They have been metaphorically defined as ""the secretions of an organism"", and are distinguished from documents that have been consciously written or created to communicate a particular message to posterity.
In general, archives consist of records that have been selected for permanent or long-term preservation on grounds of their enduring cultural, historical, or evidentiary value. Archival records are normally unpublished and almost always unique, unlike books or magazines of which many identical copies may exist. This means that archives are quite distinct from libraries with regard to their functions and organization, although archival collections can often be found within library buildings.A person who works in archives is called an archivist. The study and practice of organizing, preserving, and providing access to information and materials in archives is called archival science. The physical place of storage can be referred to as an archive (more usual in the United Kingdom), an archives (more usual in the United States), or a repository.The computing use of the term ""archive"" should not be confused with the record-keeping meaning of the term."
Archive,"An archive is an accumulation of historical records or materials – in any medium – or the physical facility in which they are located.Archives contain primary source documents that have accumulated over the course of an individual or organization's lifetime, and are kept to show the function of that person or organization. Professional archivists and historians generally understand archives to be records that have been naturally and necessarily generated as a product of regular legal, commercial, administrative, or social activities. They have been metaphorically defined as ""the secretions of an organism"", and are distinguished from documents that have been consciously written or created to communicate a particular message to posterity.
In general, archives consist of records that have been selected for permanent or long-term preservation on grounds of their enduring cultural, historical, or evidentiary value. Archival records are normally unpublished and almost always unique, unlike books or magazines of which many identical copies may exist. This means that archives are quite distinct from libraries with regard to their functions and organization, although archival collections can often be found within library buildings.A person who works in archives is called an archivist. The study and practice of organizing, preserving, and providing access to information and materials in archives is called archival science. The physical place of storage can be referred to as an archive (more usual in the United Kingdom), an archives (more usual in the United States), or a repository.The computing use of the term ""archive"" should not be confused with the record-keeping meaning of the term."
Internet Archive,"The Internet Archive is an American digital library with the stated mission of ""universal access to all knowledge"". It provides free public access to collections of digitized materials, including websites, software applications/games, music, movies/videos, moving images, and millions of books. In addition to its archiving function, the Archive is an activist organization, advocating a free and open Internet. As of September 10, 2022, the Internet Archive holds over 35 million books and texts, 8.5 million movies, videos and TV shows, 894 thousand software programs, 14 million audio files, 4.4 million images, 2.4 million TV clips, 241 thousand concerts, and over 734 billion web pages in the Wayback Machine.
The Internet Archive allows the public to upload and download digital material to its data cluster, but the bulk of its data is collected automatically by its web crawlers, which work to preserve as much of the public web as possible. Its web archive, the Wayback Machine, contains hundreds of billions of web captures. The Archive also oversees one of the world's largest book digitization projects."
Document,"A document is a written, drawn, presented, or memorialized representation of thought, often the manifestation of non-fictional, as well as fictional, content.  The word originates from the Latin Documentum, which denotes a ""teaching"" or ""lesson"": the verb doceō denotes ""to teach"". In the past, the word was usually used to denote written proof useful as evidence of a truth or fact. In the computer age, ""document"" usually denotes a primarily textual computer file, including its structure and format, e.g. fonts, colors, and images. Contemporarily, ""document"" is not defined by its transmission medium, e.g., paper, given the existence of electronic documents. ""Documentation"" is distinct because it has more denotations than ""document"". Documents are also distinguished from ""realia"", which are three-dimensional objects that would otherwise satisfy the definition of ""document"" because they memorialize or represent thought; documents are considered more as 2-dimensional representations. While documents can have large varieties of customization, all documents can be shared freely and have the right to do so, creativity can be represented by documents, also. History, events, examples, opinions, etc. all can be expressed in documents."
Document management system,"A document management system (DMS) is usually a computerized system used to store, share, track and manage files or documents. Some systems include history tracking where a log of the various versions created and modified by different users is recorded. The term has some overlap with the concepts of content management systems. It is often viewed as a component of enterprise content management (ECM) systems and related to digital asset management, document imaging, workflow systems and records management systems."
Khalili Collection of Aramaic Documents,"The Khalili Collection of Aramaic Documents is a private collection of letters and documents from the Bactria region in present-day Afghanistan, assembled by the British-Iranian collector and philanthropist Nasser D. Khalili. It is one of the Khalili Collections: eight collections of artifacts assembled, conserved, published and exhibited by Khalili. 
The documents, written in Imperial Aramaic, likely originated from the historical city of Balkh and all are dated between 353 BC to 324 BC, mostly during the reign of Artaxerxes III. The most recent of the documents was written during the early part of Alexander the Great's reign in the region. These letters use in Aramaic the original Greek form Alexandros (spelled Lksndrs) instead of the Eastern variant Iskandar (spelled Lksndr). The collection also includes eighteen tally sticks recording transfers of goods during the reign of Darius III. The collection's letters, administrative records, and military documents are significant for the linguistic study of the Official Aramaic language and of daily life in the Achaemenid empire."
Tf–idf,"In information retrieval, tf–idf (also TF*IDF, TFIDF, TF–IDF, or Tf–idf), short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling.
The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. tf–idf is one of the most popular term-weighting schemes today. A survey conducted in 2015 showed that 83% of text-based recommender systems in digital libraries use tf–idf.Variations of the tf–idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query. tf–idf can be successfully used for stop-words filtering in various subject fields, including text summarization and classification.
One of the simplest ranking functions is computed by summing the tf–idf for each query term; many more sophisticated ranking functions are variants of this simple model."
Martha Jefferson,"Martha Skelton Jefferson (née Wayles; October 30, 1748 – September 6, 1782) was the wife of Thomas Jefferson. She served as First Lady of Virginia during Jefferson's term as governor from 1779 to 1781. She died in 1782, 19 years before he became president.Of the six children born to Thomas and Martha, only two survived to adulthood, Martha and Mary. Martha died four months after the birth of her last child. The couple's letters to one another were burned, though by whom is unknown, and Thomas rarely spoke of her, so she remains a somewhat enigmatic figure. (Similarly, Jefferson did not speak much of his mother, Jane Randolph Jefferson.)It is widely held that as a widower, Thomas had a long-standing relationship and children with Martha's half-sister, Sally Hemings, a favored slave who was three-quarters white."
Identity document,
Spencer C. Tucker,"Spencer C. Tucker is a Fulbright scholar, retired university professor, and author of works on military history. He taught history at Texas Christian University for 30 years and held the John Biggs Chair of Military History at the Virginia Military Institute for six years."
Latent Dirichlet allocation,"In natural language processing, Latent Dirichlet Allocation (LDA) is a generative statistical model that explains a set of observations through unobserved groups, and each group explains why some parts of the data are similar. The LDA is an example of a topic model. In this, observations (e.g., words) are collected into documents, and each word's presence is attributable to one of the document's topics. Each document will contain a small number of topics."
index compression,"A bitmap index is a special kind of database index that uses bitmaps.
Bitmap indexes have traditionally been considered to work well for low-cardinality columns, which have a modest number of distinct values, either absolutely, or relative to the number of records that contain the data. The extreme case of low cardinality is Boolean data (e.g., does a resident in a city have internet access?), which has two values, True and False. Bitmap indexes use bit arrays (commonly called bitmaps) and answer queries by performing bitwise logical operations on these bitmaps. Bitmap indexes have a significant space and performance advantage over other structures for query of such data. Their drawback is they are less efficient than the traditional B-tree indexes for columns whose data is frequently updated: consequently, they are more often employed in read-only systems that are specialized for fast query - e.g., data warehouses, and generally unsuitable for online transaction processing applications.
Some researchers argue that bitmap indexes are also useful for moderate or even high-cardinality data (e.g., unique-valued data) which is accessed in a read-only manner, and queries access multiple bitmap-indexed columns using the AND, OR or XOR operators extensively.Bitmap indexes are also useful in data warehousing applications for joining a large fact table to smaller dimension tables such as those arranged in a star schema."
Bitmap index,"A bitmap index is a special kind of database index that uses bitmaps.
Bitmap indexes have traditionally been considered to work well for low-cardinality columns, which have a modest number of distinct values, either absolutely, or relative to the number of records that contain the data. The extreme case of low cardinality is Boolean data (e.g., does a resident in a city have internet access?), which has two values, True and False. Bitmap indexes use bit arrays (commonly called bitmaps) and answer queries by performing bitwise logical operations on these bitmaps. Bitmap indexes have a significant space and performance advantage over other structures for query of such data. Their drawback is they are less efficient than the traditional B-tree indexes for columns whose data is frequently updated: consequently, they are more often employed in read-only systems that are specialized for fast query - e.g., data warehouses, and generally unsuitable for online transaction processing applications.
Some researchers argue that bitmap indexes are also useful for moderate or even high-cardinality data (e.g., unique-valued data) which is accessed in a read-only manner, and queries access multiple bitmap-indexed columns using the AND, OR or XOR operators extensively.Bitmap indexes are also useful in data warehousing applications for joining a large fact table to smaller dimension tables such as those arranged in a star schema."
Inverted index,"In computer science, an inverted index (also referred to as a postings list, postings file, or inverted file) is a database index storing a mapping from content, such as words or numbers, to its locations in a table, or in a document or a set of documents (named in contrast to a forward index, which maps from documents to content).  The purpose of an inverted index is to allow fast full-text searches, at a cost of increased processing when a document is added to the database.  The inverted file may be the database file itself, rather than its index. It is the most popular data structure used in document retrieval systems, used on a large scale for example in search engines.  Additionally, several significant general-purpose mainframe-based database management systems have used inverted list architectures, including ADABAS, DATACOM/DB, and Model 204.
There are two main variants of inverted indexes: A record-level inverted index (or inverted file index or just inverted file) contains a list of references to documents for each word. A word-level inverted index (or full inverted index or inverted list) additionally contains the positions of each word within a document. The latter form offers more functionality (like phrase searches), but needs more processing power and space to be created."
Lossless compression,"Lossless compression is a class of data compression that allows the original data to be perfectly reconstructed from the compressed data with no loss of information. Lossless compression is possible because most real-world data exhibits statistical redundancy.  By contrast, lossy compression permits reconstruction only of an approximation of the original data, though usually with greatly improved compression rates (and therefore reduced media sizes).
By operation of the pigeonhole principle, no lossless compression algorithm can efficiently compress all possible data. For this reason, many different algorithms exist that are designed either with a specific type of input data in mind or with specific assumptions about what kinds of redundancy the uncompressed data are likely to contain. Therefore, compression ratios tend to be stronger on human- and machine-readable documents and code in comparison to entropic binary data (random bytes).Lossless data compression is used in many applications. For example, it is used in the ZIP file format and in the GNU tool gzip. It is also often used as a component within lossy data compression technologies (e.g. lossless mid/side joint stereo preprocessing by MP3 encoders and other lossy audio encoders).Lossless compression is used in cases where it is important that the original and the decompressed data be identical, or where deviations from the original data would be unfavourable. Typical examples are executable programs, text documents, and source code. Some image file formats, like PNG or GIF, use only lossless compression, while others like TIFF and MNG may use either lossless or lossy methods. Lossless audio formats are most often used for archiving or production purposes, while smaller lossy audio files are typically used on portable players and in other cases where storage space is limited or exact replication of the audio is unnecessary."
LZ77 and LZ78,"LZ77 and LZ78 are the two lossless data compression algorithms published in papers by Abraham Lempel and Jacob Ziv in 1977 and 1978.
They are also known as LZ1 and LZ2 respectively. These two algorithms form the basis for many variations including LZW, LZSS, LZMA and others. Besides their academic influence, these algorithms formed the basis of several ubiquitous compression schemes, including GIF and the DEFLATE algorithm used in PNG and ZIP.
They are both theoretically dictionary coders. LZ77 maintains a sliding window during compression. This was later shown to be equivalent to the explicit dictionary constructed by LZ78—however, they are only equivalent when the entire data is intended to be decompressed.
Since LZ77 encodes and decodes from a sliding window over previously seen characters, decompression must always start at the beginning of the input.  Conceptually, LZ78 decompression could allow random access to the input if the entire dictionary were known in advance.  However, in practice the dictionary is created during encoding and decoding by creating a new phrase whenever a token is output.The algorithms were named an IEEE Milestone in 2004. In 2021 Jacob Ziv was awarded the IEEE Medal of Honor for his involvement in their development.

"
Image compression,"Image compression is a type of data compression applied to digital images, to reduce their cost for storage or transmission. Algorithms may take advantage of visual perception and the statistical properties of image data to provide superior results compared with generic data compression methods which are used for other digital data."
IBM Informix,"IBM Informix is a product family within IBM's Information Management division that is centered on several relational database management system (RDBMS) offerings.  The Informix products were originally developed by Informix Corporation, whose Informix Software subsidiary was acquired by IBM in 2001.  In April 2017, IBM delegated active development and support to HCL Technologies for 15 years while keeping part of the marketing responsibilities.The current version of Informix is 14.10 and forms the basis of several product editions with variation in capacity and functionality.   The Informix database has been used in many high transaction rate OLTP applications in the retail, finance, energy and utilities, manufacturing and transportation sectors. More recently the server has been enhanced to improve its support for data warehouse workloads. Through extensions, Informix supports data types that are not a part of the SQL standard."
Ankle–brachial pressure index,"The ankle-brachial pressure index (ABPI) or ankle-brachial index (ABI) is the ratio of the blood pressure at the ankle to the blood pressure in the upper arm (brachium). Compared to the arm, lower blood pressure in the leg suggests blocked arteries due to peripheral artery disease (PAD). The ABPI is calculated by dividing the systolic blood pressure at the ankle by the systolic blood pressure in the arm."
Search engine indexing,"Search engine indexing is the collecting, parsing, and storing of data to facilitate fast and accurate information retrieval. Index design incorporates interdisciplinary concepts from linguistics, cognitive psychology, mathematics, informatics, and computer science. An alternate name for the process, in the context of search engines designed to find web pages on the Internet, is web indexing.
Popular engines focus on the full-text indexing of online, natural language documents. Media types such as pictures, video, audio, and graphics are also searchable.
Meta search engines reuse the indices of other services and do not store a local index whereas cache-based search engines permanently store the index along with the corpus. Unlike full-text indices, partial-text services restrict the depth indexed to reduce index size. Larger services typically perform indexing at a predetermined time interval due to the required time and processing costs, while agent-based search engines index in real time.

"
Compression stockings,"Compression stockings (Flight Socks, Support Bandage) are a specialized hosiery designed to help prevent the occurrence of, and guard against further progression of, venous disorders such as edema, phlebitis and thrombosis. Compression stockings are elastic compression garments worn around the leg, compressing the limb. This reduces the diameter of distended veins and increases venous blood flow velocity and valve effectiveness. Compression therapy helps decrease venous pressure, prevents venous stasis and impairments of venous walls, and relieves heavy and aching legs.
Knee-high compression stockings are used not only to help increase circulation, but also to help prevent the formation of blood clots in the lower legs. They also aid in the treatment of ulcers of the lower legs.
Unlike traditional dress or athletic stockings and socks, compression stockings use stronger elastics to create significant pressure on the legs, ankles and feet. Compression stockings are tightest at the ankles and gradually become less constrictive toward the knees and thighs. By compressing the surface veins, arteries and muscles, they force circulating blood through narrower channels. As a result, the arterial pressure is increased, which causes more blood to return to the heart and less blood to pool in the feet.
There are two types of compression stockings, gradient and anti-embolism."
Bit array,"A bit array (also known as bitmask,  bit map, bit set, bit string, or bit vector) is an array data structure that compactly stores bits. It can be used to implement a simple set data structure. A bit array is effective at exploiting bit-level parallelism in hardware to perform operations quickly. A typical bit array stores kw bits, where w is the number of bits in the unit of storage, such as a byte or word, and k is some nonnegative integer. If w does not divide the number of bits to be stored, some space is wasted due to internal fragmentation."
inverted index,"In computer science, an inverted index (also referred to as a postings list, postings file, or inverted file) is a database index storing a mapping from content, such as words or numbers, to its locations in a table, or in a document or a set of documents (named in contrast to a forward index, which maps from documents to content).  The purpose of an inverted index is to allow fast full-text searches, at a cost of increased processing when a document is added to the database.  The inverted file may be the database file itself, rather than its index. It is the most popular data structure used in document retrieval systems, used on a large scale for example in search engines.  Additionally, several significant general-purpose mainframe-based database management systems have used inverted list architectures, including ADABAS, DATACOM/DB, and Model 204.
There are two main variants of inverted indexes: A record-level inverted index (or inverted file index or just inverted file) contains a list of references to documents for each word. A word-level inverted index (or full inverted index or inverted list) additionally contains the positions of each word within a document. The latter form offers more functionality (like phrase searches), but needs more processing power and space to be created."
Inverted index,"In computer science, an inverted index (also referred to as a postings list, postings file, or inverted file) is a database index storing a mapping from content, such as words or numbers, to its locations in a table, or in a document or a set of documents (named in contrast to a forward index, which maps from documents to content).  The purpose of an inverted index is to allow fast full-text searches, at a cost of increased processing when a document is added to the database.  The inverted file may be the database file itself, rather than its index. It is the most popular data structure used in document retrieval systems, used on a large scale for example in search engines.  Additionally, several significant general-purpose mainframe-based database management systems have used inverted list architectures, including ADABAS, DATACOM/DB, and Model 204.
There are two main variants of inverted indexes: A record-level inverted index (or inverted file index or just inverted file) contains a list of references to documents for each word. A word-level inverted index (or full inverted index or inverted list) additionally contains the positions of each word within a document. The latter form offers more functionality (like phrase searches), but needs more processing power and space to be created."
Search engine indexing,"Search engine indexing is the collecting, parsing, and storing of data to facilitate fast and accurate information retrieval. Index design incorporates interdisciplinary concepts from linguistics, cognitive psychology, mathematics, informatics, and computer science. An alternate name for the process, in the context of search engines designed to find web pages on the Internet, is web indexing.
Popular engines focus on the full-text indexing of online, natural language documents. Media types such as pictures, video, audio, and graphics are also searchable.
Meta search engines reuse the indices of other services and do not store a local index whereas cache-based search engines permanently store the index along with the corpus. Unlike full-text indices, partial-text services restrict the depth indexed to reduce index size. Larger services typically perform indexing at a predetermined time interval due to the required time and processing costs, while agent-based search engines index in real time.

"
Database index,"A database index is a data structure that improves the speed of data retrieval operations on a database table at the cost of additional writes and storage space to maintain the index data structure.  Indexes are used to quickly locate data without having to search every row in a database table every time a database table is accessed.  Indexes can be created using one or more columns of a database table, providing the basis for both rapid random lookups and efficient access of ordered records.
An index is a copy of selected columns of data, from a table, that is designed to enable very efficient search.  An index normally includes a ""key"" or direct link to the original row of data from which it was copied, to allow the complete row to be retrieved efficiently.  Some databases extend the power of indexing by letting developers create indexes on column values that have been transformed by functions or expressions. For example, an index could be created on upper(last_name), which would only store the upper-case versions of the last_name field in the index. Another option sometimes supported is the use of partial indices, where index entries are created only for those records that satisfy some conditional expression. A further aspect of flexibility is to permit indexing on user-defined functions, as well as expressions formed from an assortment of built-in functions."
Boolean model of information retrieval,"The (standard) Boolean model of information retrieval (BIR) is a classical information retrieval (IR) model and, at the same time, the first and most-adopted one. It is used by many IR systems to this day. The BIR is based on Boolean logic and classical set theory in that both the documents to be searched and the user's query are conceived as sets of terms (a bag-of-words model). Retrieval is based on whether or not the documents contain the query terms."
Microsoft SQL Server,"Microsoft SQL Server is a relational database management system developed by Microsoft. As a database server, it is a software product with the primary function of storing and retrieving data as requested by other software applications—which may run either on the same computer or on another computer across a network (including the Internet). Microsoft markets at least a dozen different editions of Microsoft SQL Server, aimed at different audiences and for workloads ranging from small single-machine applications to large Internet-facing applications with many concurrent users."
Database model,"A database model is a type of data model that determines the logical structure of a database. It fundamentally determines in which manner data can be stored, organized and manipulated. The most popular example of a database model is the relational model, which uses a table-based format."
Explicit semantic analysis,"In natural language processing and information retrieval, explicit semantic analysis (ESA) is a vectoral representation of text (individual words or entire documents) that uses a document corpus as a knowledge base. Specifically, in ESA, a word is represented as a column vector in the tf–idf matrix of the text corpus and a document (string of words) is represented as the centroid of the vectors representing its words. Typically, the text corpus is English Wikipedia, though other corpora including the Open Directory Project have been used.ESA was designed by Evgeniy Gabrilovich and Shaul Markovitch as a means of improving text categorization
and has been used by this pair of researchers to compute what they refer to as ""semantic relatedness"" by means of cosine similarity between the aforementioned vectors, collectively interpreted as a space of ""concepts explicitly defined and described by humans"", where Wikipedia articles (or ODP entries, or otherwise titles of documents in the knowledge base corpus) are equated with concepts. The name ""explicit semantic analysis"" contrasts with latent semantic analysis (LSA), because the use of a knowledge base makes it possible to assign human-readable labels to the concepts that make up the vector space."
BitFunnel,"BitFunnel is the search engine indexing algorithm and a set of components used in the Bing search engine, which were made open source in 2016. BitFunnel uses bit-sliced signatures instead of an inverted index in an attempt to reduce operations cost."
dictionary data structures,"In computer science, a data structure is a data organization, management, and storage format that is usually chosen for efficient access to data. More precisely, a data structure is a collection of data values, the relationships among them, and the functions or operations that can be applied to the data, i.e., it is an algebraic structure about data.

"
Data structure,"In computer science, a data structure is a data organization, management, and storage format that is usually chosen for efficient access to data. More precisely, a data structure is a collection of data values, the relationships among them, and the functions or operations that can be applied to the data, i.e., it is an algebraic structure about data.

"
Data dictionary,"A data dictionary, or metadata repository, as defined in the IBM Dictionary of Computing, is a ""centralized repository of information about data such as meaning, relationships to other data, origin, usage, and format"". Oracle defines it as a collection of tables with metadata. The term can have one of several closely related meanings pertaining to databases and database management systems (DBMS):

A document describing a database or collection of databases
An integral component of a DBMS that is required to determine its structure
A piece of middleware that extends or supplants the native data dictionary of a DBMS"
Associative array,"In computer science, an associative array, map, symbol table, or dictionary is an abstract data type that stores a collection of (key, value) pairs, such that each possible key appears at most once in the collection. In mathematical terms an associative array is a function with finite domain. It supports 'lookup', 'remove', and 'insert' operations. 
The dictionary problem is the classic problem of designing efficient data structures that implement associative arrays.
The two major solutions to the dictionary problem are hash tables and search trees.
In some cases it is also possible to solve the problem using directly addressed arrays, binary search trees, or other more specialized structures.
Many programming languages include associative arrays as primitive data types, and they are available in software libraries for many others. Content-addressable memory is a form of direct hardware-level support for associative arrays.
Associative arrays have many applications including such fundamental programming patterns as memoization and the decorator pattern.The name does not come from the associative property known in mathematics. Rather, it arises from the fact that values are associated with keys. It is not to be confused with associative processors."
Array (data structure),"In computer science, an array is a data structure consisting of a collection of elements (values or variables), each identified by at least one array index or key. An array is stored such that the position of each element can be computed from its index tuple by a mathematical formula. The simplest type of data structure is a linear array, also called one-dimensional array.
For example, an array of ten 32-bit (4-byte) integer variables, with indices 0 through 9, may be stored as ten words at memory addresses 2000, 2004, 2008, ..., 2036, (in hexadecimal: 0x7D0, 0x7D4, 0x7D8, ..., 0x7F4) so that the element with index i has the address 2000 + (i × 4).
The memory address of the first element of an array is called first address, foundation address, or base address.
Because the mathematical concept of a matrix can be represented as a two-dimensional grid, two-dimensional arrays are also sometimes called ""matrices"". In some cases the term ""vector"" is used in computing to refer to an array, although tuples rather than vectors are the more mathematically correct equivalent. Tables are often implemented in the form of arrays, especially lookup tables; the word ""table"" is sometimes used as a synonym of array.
Arrays are among the oldest and most important data structures, and are used by almost every program. They are also used to implement many other data structures, such as lists and strings. They effectively exploit the addressing logic of computers. In most modern computers and many external storage devices, the memory is a one-dimensional array of words, whose indices are their addresses. Processors, especially vector processors, are often optimized for array operations.
Arrays are useful mostly because the element indices can be computed at run time. Among other things, this feature allows a single iterative statement to process arbitrarily many elements of an array. For that reason, the elements of an array data structure are required to have the same size and should use the same data representation. The set of valid index tuples and the addresses of the elements (and hence the element addressing formula) are usually, but not always, fixed while the array is in use.
The term ""array"" may also refer to an array data type, a kind of data type provided by most high-level programming languages that consists of a collection of values or variables that can be selected by one or more indices computed at run-time. Array types are often implemented by array structures; however, in some languages they may be implemented by hash tables, linked lists, search trees, or other data structures.
The term is also used, especially in the description of algorithms, to mean associative array or ""abstract array"", a theoretical computer science model (an abstract data type or ADT) intended to capture the essential properties of arrays."
documents,"A document is a written, drawn, presented, or memorialized representation of thought, often the manifestation of non-fictional, as well as fictional, content.  The word originates from the Latin Documentum, which denotes a ""teaching"" or ""lesson"": the verb doceō denotes ""to teach"". In the past, the word was usually used to denote written proof useful as evidence of a truth or fact. In the computer age, ""document"" usually denotes a primarily textual computer file, including its structure and format, e.g. fonts, colors, and images. Contemporarily, ""document"" is not defined by its transmission medium, e.g., paper, given the existence of electronic documents. ""Documentation"" is distinct because it has more denotations than ""document"". Documents are also distinguished from ""realia"", which are three-dimensional objects that would otherwise satisfy the definition of ""document"" because they memorialize or represent thought; documents are considered more as 2-dimensional representations. While documents can have large varieties of customization, all documents can be shared freely and have the right to do so, creativity can be represented by documents, also. History, events, examples, opinions, etc. all can be expressed in documents."
Document,"A document is a written, drawn, presented, or memorialized representation of thought, often the manifestation of non-fictional, as well as fictional, content.  The word originates from the Latin Documentum, which denotes a ""teaching"" or ""lesson"": the verb doceō denotes ""to teach"". In the past, the word was usually used to denote written proof useful as evidence of a truth or fact. In the computer age, ""document"" usually denotes a primarily textual computer file, including its structure and format, e.g. fonts, colors, and images. Contemporarily, ""document"" is not defined by its transmission medium, e.g., paper, given the existence of electronic documents. ""Documentation"" is distinct because it has more denotations than ""document"". Documents are also distinguished from ""realia"", which are three-dimensional objects that would otherwise satisfy the definition of ""document"" because they memorialize or represent thought; documents are considered more as 2-dimensional representations. While documents can have large varieties of customization, all documents can be shared freely and have the right to do so, creativity can be represented by documents, also. History, events, examples, opinions, etc. all can be expressed in documents."
False document,A false document is a technique by which an author aims to increase verisimilitude in a work of fiction by inventing and inserting or mentioning documents that appear to be factual. The goal of a false document is to convince an audience that what is being presented is factual.
Book of Documents,"The Book of Documents (Shūjīng, earlier Shu King) or Classic of History, also known as the Shangshu (“Venerated Documents”), is one of the Five Classics of ancient Chinese literature.  It is a collection of rhetorical prose attributed to figures of ancient China, and served as the foundation of Chinese political philosophy for over 2,000 years.
The Book of Documents was the subject of one of China's oldest literary controversies, between proponents of different versions of the text. A version was preserved from Qin Shi Huang's burning of books and burying of scholars by scholar Fu Sheng, in 29 sections (pian 篇). This group of texts were referred to as ""Modern Script"" jinwen 今文, because written with the script in use at the beginning of the Western Han dyansty. According to Western Han dynasty documents, new textual material was  discovered in the wall of Confucius' family estate in Qufu by his descendant Kong Anguo in the late 2nd century BC. This new material was referred to as ""Old Script"" guwen 古文, because written in the script that predated the standardization of Chinese script enforced during the Qing dynasty. Compared to the ""Modern Script"" texts, the ""Old Script"" material had 16 more texts. However, this seems to have been lost at the end of the Eastern Han dynasty, while the ""Modern Script"" text enjoyed circulation, in particular in Ouyang Gao’s 歐陽高 (a. 136 BCE) study of it, the Ouyang Shangshu 歐陽尚書. This was the basis of studies by Ma Rong and Zheng Xuan in the Easter Han Dynasty. By the end of the second century CE, there was knowledge that the Shangshu at some point included more than the ""Modern Script"" text. This likely prompted scholars to recreate the ""Old Script"" texts said to have once belonged to the Shangshu, a process that culminated with the presentation of a 58 section (59 if the preface is included in the count) Shangshu to the Eastern Jin court, in 317 CE, by Mei Ze 梅頤.
This version was accepted, among doubts, and later was canonized as part of Kong Yingda's project. It was only in the 17th century that Qing dynasty scholar Yan Ruoqu demonstrated that the ""Old Script"" were actually fabrications ""reconstructed"" in the 3rd or 4th centuries AD.
In the transmitted edition, texts are grouped into four sections representing different eras: the semi-mythical reign of Yu the Great, and the Xia, Shang and Zhou dynasties.  The Zhou section accounts for over half the text.  Some of its New Text chapters are among the earliest examples of Chinese prose, recording speeches from the early years of the Zhou dynasty in the late 11th century BC.  Although the other three sections purport to record earlier material, most scholars believe that even the New Script chapters in these sections were composed later than those in the Zhou section, with chapters relating to the earliest periods being as recent as the 4th or 3rd centuries BC."
Halloween documents,"The Halloween documents comprise a series of confidential Microsoft memoranda on potential strategies relating to free software, open-source software, and to Linux in particular, and a series of media responses to these memoranda. Both the leaked documents and the responses were published by Eric S. Raymond in 1998.The documents are associated with Halloween because many of them were originally leaked close to October 31 in different years.

"
Electronic document,"An electronic document is any electronic media content (other than computer programs or system files) that is intended to be used in either an electronic form or as printed output. Originally, any computer data were considered as something internal — the final data output was always on paper. However, the development of computer networks has made it so that in most cases it is much more convenient to distribute electronic documents than printed ones. The improvements in electronic visual display technologies made it possible to view documents on screen instead of printing them (thus saving paper and the space required to store the printed copies).However, using electronic documents for final presentation instead of paper has created the problem of multiple incompatible file formats. Even plain text computer files are not free from this problem — e.g. under MS-DOS, most programs could not work correctly with UNIX-style text files (see newline), and for non-English speakers, the different code pages always have been a source of trouble.
Even more problems are connected with complex file formats of various word processors, spreadsheets, and graphics software. To alleviate the problem, many software companies distribute free file viewers for their proprietary file formats (one example is Adobe's Acrobat Reader). The other solution is the development of standardized non-proprietary file formats (such as HTML and OpenDocument), and electronic documents for specialized uses have specialized formats – the specialized electronic articles in physics use TeX or PostScript."
Historical document,"Historical documents are original documents that contain important historical information about a person, place, or event and can thus serve as primary sources as important ingredients of the historical methodology.
Significant historical documents can be deeds, laws, accounts of battles (often given by the victors or persons sharing their viewpoint), or the exploits of the powerful. Though these documents are of historical interest, they do not detail the daily lives of ordinary people, or the way society functioned. Anthropologists, historians and archeologists generally are more interested in documents that describe the day-to-day lives of ordinary people, indicating what they ate, their interaction with other members of their households and social groups, and their states of mind. It is this information that allows them to try to understand and describe the way society was functioning at any particular time in history. Greek ostraka provide good examples of historical documents from ""among the common people"".
Many documents that are produced today, such as personal letters, pictures, contracts, newspapers, and medical records, would be considered valuable historical documents in the future. However most of these will be lost in the future since they are either printed on ordinary paper which has a limited lifespan, or even stored in digital formats, then lost track over time.
Some companies and government entities are attempting to increase the number of documents that will survive the passage of time, by taking into account the preservation issues, and either printing documents in a manner that would increase the likelihood of them surviving indefinitely, or placing selected documents in time capsules or other special storage environments."
Document management system,"A document management system (DMS) is usually a computerized system used to store, share, track and manage files or documents. Some systems include history tracking where a log of the various versions created and modified by different users is recorded. The term has some overlap with the concepts of content management systems. It is often viewed as a component of enterprise content management (ECM) systems and related to digital asset management, document imaging, workflow systems and records management systems."
Constitution,"Institutions are humanly devised structures of rules and norms that shape and constrain individual behavior. All definitions of institutions generally entail that there is a level of persistence and continuity. Laws, rules, social conventions and norms are all examples of institutions. Institutions vary in their level of formality and informality.Institutions are a principal object of study in social sciences such as political science, anthropology, economics, and sociology (the latter described by Émile Durkheim as the ""science of institutions, their genesis and their functioning""). Primary or meta-institutions are institutions such as the family or money that are broad enough to encompass sets of related institutions. Institutions are also a central concern for law, the formal mechanism for political rule-making and enforcement. Historians study and document the founding, growth, decay and development of institutions as part of political, economic and cultural history."
Identity document,
Google Docs,"Google Docs is an online word processor included as part of the free, web-based Google Docs Editors suite offered by Google, which also includes: Google Sheets, Google Slides, Google Drawings, Google Forms, Google Sites and Google Keep. Google Docs is accessible via an internet browser as a web-based application and is also available as a mobile app on Android and iOS and as a desktop application on Google's ChromeOS.
Google Docs allows users to create and edit documents online while collaborating with other users in real time. Edits are tracked by the user making the edit, with a revision history presenting changes. An editor's position is highlighted with an editor-specific color and cursor, and a permissions system regulates what users can do. Updates have introduced features using machine learning, including ""Explore"", offering search results based on the contents of a document, and ""Action items"", allowing users to assign tasks to other users.Google Docs supports opening and saving documents in the standard OpenDocument format as well as in Rich text format, plain Unicode text, zipped HTML, and Microsoft Word. Exporting to PDF and EPUB formats are implemented."
vector space model,"Vector space model or term vector model is an algebraic model for representing text documents (and any objects, in general) as vectors of identifiers (such as index terms). It is used in information filtering, information retrieval, indexing and relevancy rankings.  Its first use was in the SMART Information Retrieval System.

"
Vector space model,"Vector space model or term vector model is an algebraic model for representing text documents (and any objects, in general) as vectors of identifiers (such as index terms). It is used in information filtering, information retrieval, indexing and relevancy rankings.  Its first use was in the SMART Information Retrieval System.

"
Vector space,"In mathematics and physics, a vector space (also called a linear space) is a set whose elements, often called vectors, may be added together and multiplied (""scaled"") by numbers called scalars. Scalars are often real numbers, but can be  complex numbers or, more generally, elements of any field. The operations of vector addition and scalar multiplication must satisfy certain requirements, called vector axioms. The terms real vector space and complex vector space are often used to specify the nature of the scalars: real coordinate space or complex coordinate space.
Vector spaces generalize Euclidean vectors, which allow modeling of physical quantities, such as forces and velocity, that have not only a magnitude, but also a direction. The concept of vector spaces is fundamental for linear algebra, together with the concept of matrix, which allows computing in vector spaces. This provides a concise and synthetic way for manipulating and studying systems of linear equations.
Vector spaces are characterized by their dimension, which, roughly speaking, specifies the number of independent directions in the space. This means that, for two vector spaces with the same dimension, the properties that depend only on the vector-space structure are exactly the same (technically the vector spaces are isomorphic). A vector space is finite-dimensional if its dimension is a natural number. Otherwise, it is infinite-dimensional, and its dimension is an infinite cardinal. Finite-dimensional vector spaces occur naturally in geometry and related areas. Infinite-dimensional vector spaces occur in many areas of mathematics. For example, polynomial rings are countably infinite-dimensional vector spaces, and many function spaces have the cardinality of the continuum as a dimension.
Many vector spaces that are considered in mathematics are also endowed with other structures. This is the case of algebras, which include field extensions, polynomial rings, associative algebras and Lie algebras. This is also the case of topological vector spaces, which include function spaces, inner product spaces, normed spaces, Hilbert spaces and Banach spaces."
Ranking (information retrieval),"Ranking of query is one of the fundamental problems in information retrieval (IR), the scientific/engineering discipline behind search engines. Given a query q and a collection D of documents that match the query, the problem is to rank, that is, sort, the documents in D according to some criterion so that the ""best"" results appear early in the result list displayed to the user. Ranking in terms of information retrieval is an important concept in computer science and is used in many different applications such as search engine queries and recommender systems. A majority of search engines use ranking algorithms to provide users with accurate and relevant results."
Word embedding,"In natural language processing (NLP), word embedding is a term used for the representation of words for text analysis, typically in the form of a real-valued vector that encodes the meaning of the word such that the words that are closer in the vector space are expected to be similar in meaning. Word embeddings can be obtained using a set of language modeling and feature learning techniques where words or phrases from the vocabulary are mapped to vectors of real numbers.
Methods to generate this mapping include neural networks, dimensionality reduction on the word co-occurrence matrix, probabilistic models, explainable knowledge base method, and explicit representation in terms of the context in which words appear.Word and phrase embeddings, when used as the underlying input representation, have been shown to boost the performance in NLP tasks such as syntactic parsing and sentiment analysis."
Bag-of-words model,"The bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity. The bag-of-words model has also been used for computer vision.The bag-of-words model is commonly used in methods of document classification where the (frequency of) occurrence of each word is used as a feature for training a classifier.An early reference to ""bag of words"" in a linguistic context can be found in Zellig Harris's 1954 article on Distributional Structure.The Bag-of-words model is one example of a Vector space model."
term frequency,"In information retrieval, tf–idf (also TF*IDF, TFIDF, TF–IDF, or Tf–idf), short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling.
The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. tf–idf is one of the most popular term-weighting schemes today. A survey conducted in 2015 showed that 83% of text-based recommender systems in digital libraries use tf–idf.Variations of the tf–idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query. tf–idf can be successfully used for stop-words filtering in various subject fields, including text summarization and classification.
One of the simplest ranking functions is computed by summing the tf–idf for each query term; many more sophisticated ranking functions are variants of this simple model."
Tf–idf,"In information retrieval, tf–idf (also TF*IDF, TFIDF, TF–IDF, or Tf–idf), short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling.
The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. tf–idf is one of the most popular term-weighting schemes today. A survey conducted in 2015 showed that 83% of text-based recommender systems in digital libraries use tf–idf.Variations of the tf–idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query. tf–idf can be successfully used for stop-words filtering in various subject fields, including text summarization and classification.
One of the simplest ranking functions is computed by summing the tf–idf for each query term; many more sophisticated ranking functions are variants of this simple model."
Frequency,"In grammar, a frequentative form (abbreviated FREQ or FR) of a word is one that indicates repeated action but is not to be confused with iterative aspect. The frequentative form can be considered a separate but not completely independent word called a frequentative. The frequentative is no longer productive in English, unlike in some language groups, such as Finno-Ugric, Balto-Slavic, and Turkic."
document classification,"Document classification or document categorization is a problem in library science, information science and computer science. The task is to assign a document to one or more classes or categories. This may be done ""manually"" (or ""intellectually"") or algorithmically. The intellectual classification of documents has mostly been the province of library science, while the algorithmic classification of documents is mainly in information science and computer science. The problems are overlapping, however, and there is therefore interdisciplinary research on document classification.
The documents to be classified may be texts, images, music, etc. Each kind of document possesses its special classification problems. When not otherwise specified, text classification is implied.
Documents may be classified according to their subjects or according to other attributes (such as document type, author, printing year etc.). In the rest of this article only subject classification is considered. There are two main philosophies of subject classification of documents: the content-based approach and the request-based approach."
Document classification,"Document classification or document categorization is a problem in library science, information science and computer science. The task is to assign a document to one or more classes or categories. This may be done ""manually"" (or ""intellectually"") or algorithmically. The intellectual classification of documents has mostly been the province of library science, while the algorithmic classification of documents is mainly in information science and computer science. The problems are overlapping, however, and there is therefore interdisciplinary research on document classification.
The documents to be classified may be texts, images, music, etc. Each kind of document possesses its special classification problems. When not otherwise specified, text classification is implied.
Documents may be classified according to their subjects or according to other attributes (such as document type, author, printing year etc.). In the rest of this article only subject classification is considered. There are two main philosophies of subject classification of documents: the content-based approach and the request-based approach."
Classification,"Classification is a process related to categorization, the process in which ideas and objects are recognized, differentiated and understood. 
Classification is the grouping of related facts into classes. 
It may also refer to:"
Naive Bayes classifier,"In statistics, naive Bayes classifiers are a family of simple ""probabilistic classifiers"" based on applying Bayes' theorem with strong (naive) independence assumptions between the features (see Bayes classifier). They are among the simplest Bayesian network models, but coupled with kernel density estimation, they can achieve high accuracy levels.Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features/predictors) in a learning problem. Maximum-likelihood training can be done by evaluating a closed-form expression,: 718  which takes linear time, rather than by expensive iterative approximation as used for many other types of classifiers.
In the statistics literature, naive Bayes models are known under a variety of names, including simple Bayes and independence Bayes. All these names reference the use of Bayes' theorem in the classifier's decision rule, but naive Bayes is not (necessarily) a Bayesian method."
Superintendent of Documents Classification,"Superintendent of Documents Classification, commonly called as SuDocs or SuDoc, is a system of library classification developed and maintained by the United States Government Publishing Office. Unlike Library of Congress Classification, Dewey Decimal Classification, or Universal Decimal Classification, SuDocs is not a universal system. Rather, it is intended for use only with publications of the Federal Government of the United States.SuDocs call numbers are assigned by the Government Publishing Office as new publications are produced. Many libraries that participate in the Federal Depository Library Program employ SuDocs to classify their collections."
Classified information,"Classified information is material that a government body deems to be sensitive information that must be protected. Access is restricted by law or regulation to particular groups of people with the necessary security clearance and need to know, and mishandling of the material can incur criminal penalties.
A formal security clearance is required to view or handle classified material. The clearance process requires a satisfactory background investigation. Documents and other information must be properly marked ""by the author"" with one of several (hierarchical) levels of sensitivity—e.g. restricted, confidential, secret, and top secret. The choice of level is based on an impact assessment; governments have their own criteria, including how to determine the classification of an information asset and rules on how to protect information classified at each level. This process often includes security clearances for personnel handling the information.
Some corporations and non-government organizations also assign levels of protection to their private information, either from a desire to protect trade secrets, or because of laws and regulations governing various matters such as personal privacy, sealed legal proceedings and the timing of financial information releases.
With the passage of time much classified information can become less sensitive, and may be declassified and made public. Since the late twentieth century there has been freedom of information legislation in some countries, whereby the public is deemed to have the right to all information that is not considered to be damaging if released. Sometimes documents are released with information still considered confidential obscured (redacted), as in the adjacent example.
The question exists among some political science and legal experts whether the definition of classified ought to be information that would cause injury to the cause of justice, human rights, etc., rather than information that would cause injury to the national interest; to distinguish when classifying information is in the collective best interest of a just society, or merely the best interest of a society acting unjustly to protect its people, government, or administrative officials from legitimate recourses consistent with a fair and just social contract."
Library classification,"A library classification is a system of organization of knowledge by which library resources are arranged and ordered systematically. Library classifications are a notational system that represents the order of topics in the classification and allows items to be stored in that order. Library classification systems group related materials together, typically arranged as a hierarchical tree structure. A different kind of classification system, called a faceted classification system, is also widely used, which allows the assignment of multiple classifications to an object, enabling the classifications to be ordered in many ways.

"
Bag-of-words model,"The bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity. The bag-of-words model has also been used for computer vision.The bag-of-words model is commonly used in methods of document classification where the (frequency of) occurrence of each word is used as a feature for training a classifier.An early reference to ""bag of words"" in a linguistic context can be found in Zellig Harris's 1954 article on Distributional Structure.The Bag-of-words model is one example of a Vector space model."
Document-term matrix,"A document-term matrix is a mathematical matrix that describes the frequency of terms that occur in a collection of documents. In a document-term matrix, rows correspond to documents in the collection and columns correspond to terms. This matrix is a specific instance of a document-feature matrix where ""features"" may refer to other properties of a document besides terms. It is also common to encounter the transpose, or term-document matrix where documents are the columns and terms are the rows. They are useful in the field of natural language processing and computational text analysis.While the value of the cells is commonly the raw count of a given term, there are various schemes for weighting the raw counts such as, row normalizing (i.e. relative frequency/proportions) and tf-idf.
Terms are commonly single words separated by whitespace or punctuation on either side (a.k.a. unigrams). In such a case, this is also referred to as ""bag of words"" representation because the counts of individual words is retained, but not the order of the words in the document."
Document retrieval,"Document retrieval is defined as the matching of some stated user query against a set of free-text records. These records could be any type of mainly unstructured text, such as newspaper articles, real estate records or paragraphs in a manual. User queries can range from multi-sentence full descriptions of an information need to a few words.
Document retrieval is sometimes referred to as, or as a branch of, text retrieval. Text retrieval is a branch of information retrieval where the information is stored primarily in the form of text. Text databases became decentralized thanks to the personal computer. Text retrieval is a critical area of study today, since it is the fundamental basis of all internet search engines."
Statistical classification,"In statistics, classification is the problem of identifying which of a set of categories (sub-populations) an observation (or observations) belongs to.  Examples are assigning a given email to the ""spam"" or ""non-spam"" class, and assigning a diagnosis to a given patient based on observed characteristics of the patient (sex, blood pressure, presence or absence of certain symptoms, etc.).
Often, the individual observations are analyzed into a set of quantifiable properties, known variously as explanatory variables or features.  These properties may variously be categorical (e.g. ""A"", ""B"", ""AB"" or ""O"", for blood type), ordinal (e.g. ""large"", ""medium"" or ""small""), integer-valued (e.g. the number of occurrences of a particular word in an email) or real-valued (e.g. a measurement of blood pressure). Other classifiers work by comparing observations to previous observations by means of a similarity or distance function.
An algorithm that implements classification, especially in a concrete implementation, is known as a classifier.  The term ""classifier"" sometimes also refers to the mathematical function, implemented by a classification algorithm, that maps input data to a category.
Terminology across fields is quite varied. In statistics, where classification is often done with logistic regression or a similar procedure, the properties of observations are termed explanatory variables (or independent variables, regressors, etc.), and the categories to be predicted are known as outcomes, which are considered to be possible values of the dependent variable.  In machine learning, the observations are often known as instances, the explanatory variables are termed features (grouped into a feature vector), and the possible categories to be predicted are classes.  Other fields may use different terminology: e.g. in community ecology, the term ""classification"" normally refers to cluster analysis."
vector space scoring,"In natural language processing (NLP), word embedding is a term used for the representation of words for text analysis, typically in the form of a real-valued vector that encodes the meaning of the word such that the words that are closer in the vector space are expected to be similar in meaning. Word embeddings can be obtained using a set of language modeling and feature learning techniques where words or phrases from the vocabulary are mapped to vectors of real numbers.
Methods to generate this mapping include neural networks, dimensionality reduction on the word co-occurrence matrix, probabilistic models, explainable knowledge base method, and explicit representation in terms of the context in which words appear.Word and phrase embeddings, when used as the underlying input representation, have been shown to boost the performance in NLP tasks such as syntactic parsing and sentiment analysis."
Word embedding,"In natural language processing (NLP), word embedding is a term used for the representation of words for text analysis, typically in the form of a real-valued vector that encodes the meaning of the word such that the words that are closer in the vector space are expected to be similar in meaning. Word embeddings can be obtained using a set of language modeling and feature learning techniques where words or phrases from the vocabulary are mapped to vectors of real numbers.
Methods to generate this mapping include neural networks, dimensionality reduction on the word co-occurrence matrix, probabilistic models, explainable knowledge base method, and explicit representation in terms of the context in which words appear.Word and phrase embeddings, when used as the underlying input representation, have been shown to boost the performance in NLP tasks such as syntactic parsing and sentiment analysis."
Support vector machine,"In machine learning, support vector machines (SVMs, also support vector networks) are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis. Developed at AT&T Bell Laboratories by Vladimir Vapnik with colleagues (Boser et al., 1992, Guyon et al., 1993, Cortes and Vapnik, 1995, Vapnik et al., 1997) SVMs are one of the most robust prediction methods, being based on statistical learning frameworks or VC theory proposed by Vapnik (1982, 1995) and Chervonenkis (1974). Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting). SVM maps training examples to points in space so as to maximise the width of the gap between the two categories. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.
In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.
When data are unlabelled, supervised learning is not possible, and an unsupervised learning approach is required, which attempts to find natural clustering of the data to groups, and then map new data to these formed groups. The support vector clustering algorithm, created by Hava Siegelmann and Vladimir Vapnik, applies the statistics of support vectors, developed in the support vector machines algorithm, to categorize unlabeled data."
Score (statistics),"In statistics, the score (or informant) is the gradient of the log-likelihood function with respect to the parameter vector. Evaluated at a particular point of the parameter vector, the score indicates the steepness of the log-likelihood function and thereby the sensitivity to infinitesimal changes to the parameter values. If the log-likelihood function is continuous over the parameter space, the score will vanish at a local maximum or minimum; this fact is used in maximum likelihood estimation to find the parameter values that maximize the likelihood function.
Since the score is a function of the observations that are subject to sampling error, it lends itself to a test statistic known as score test in which the parameter is held at a particular value. Further, the ratio of two likelihood functions evaluated at two distinct parameter values can be understood as a definite integral of the score function."
Feature (machine learning),"In machine learning and pattern recognition, a feature is an individual measurable property or characteristic of a phenomenon. Choosing informative, discriminating and independent features is a crucial element of effective algorithms in pattern recognition,  classification and regression. Features are usually numeric, but structural features such as strings and graphs are used in syntactic pattern recognition. The concept of ""feature"" is related to that of explanatory variable used in statistical techniques such as linear regression."
Principal component analysis,"Principal component analysis (PCA) is a popular technique for analyzing large datasets containing a high number of dimensions/features per observation, increasing the interpretability of data while preserving the maximum amount of information, and enabling the visualization of multidimensional data. Formally, PCA is a statistical technique for reducing the dimensionality of a dataset. This is accomplished by linearly transforming the data into a new coordinate system where (most of) the variation in the data can be described with fewer dimensions than the initial data. Many studies use the first two principal components in order to plot the data in two dimensions and to visually identify clusters of closely related data points. Principal component analysis has applications in many fields such as population genetics, microbiome studies, and atmospheric science. 
The principal components of a collection of points in a real coordinate space are a sequence of 
  
    
      
        p
      
    
    {\displaystyle p}
   unit vectors, where the 
  
    
      
        i
      
    
    {\displaystyle i}
  -th vector is the direction of a line that best fits the data while being orthogonal to the first 
  
    
      
        i
        −
        1
      
    
    {\displaystyle i-1}
   vectors. Here, a best-fitting line is defined as one that minimizes the average squared perpendicular distance from the points to the line. These directions constitute an orthonormal basis in which different individual dimensions of the data are linearly uncorrelated. Principal component analysis (PCA) is the process of computing the principal components and using them to perform a change of basis on the data, sometimes using only the first few principal components and ignoring the rest.
In data analysis, the first principal component of a set of  
  
    
      
        p
      
    
    {\displaystyle p}
   variables, presumed to be jointly normally distributed, is the derived variable formed as a linear combination of the original variables that explains the most variance. The second principal component explains the most variance in what is left once the effect of the first component is removed, and we may proceed through  
  
    
      
        p
      
    
    {\displaystyle p}
   iterations until all the variance is explained. PCA is most commonly used when many of the variables are highly correlated with each other and it is desirable to reduce their number to an independent set.
PCA is used in exploratory data analysis and for making predictive models. It is commonly used for dimensionality reduction by projecting each data point onto only the first few principal components to obtain lower-dimensional data while preserving as much of the data's variation as possible. The first principal component can equivalently be defined as a direction that maximizes the variance of the projected data. The 
  
    
      
        i
      
    
    {\displaystyle i}
  -th principal component can be taken as a direction orthogonal to the first 
  
    
      
        i
        −
        1
      
    
    {\displaystyle i-1}
   principal components that maximizes the variance of the projected data.
For either objective, it can be shown that the principal components are eigenvectors of the data's covariance matrix. Thus, the principal components are often computed by eigendecomposition of the data covariance matrix or singular value decomposition of the data matrix. PCA is the simplest of the true eigenvector-based multivariate analyses and is closely related to factor analysis. Factor analysis typically incorporates more domain specific assumptions about the underlying structure and solves eigenvectors of a slightly different matrix. PCA is also related to canonical correlation analysis (CCA). CCA defines coordinate systems that optimally describe the cross-covariance between two datasets while PCA defines a new orthogonal coordinate system that optimally describes variance in a single dataset. Robust and L1-norm-based variants of standard PCA have also been proposed.

"
Word2vec,"Word2vec is a technique for natural language processing (NLP) published in 2013. The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence. As the name implies, word2vec represents each distinct word with a particular list of numbers called a vector. The vectors are chosen carefully such that they capture the semantic and syntactic qualities of words; as such, a simple mathematical function (cosine similarity) can indicate the level of semantic similarity between the words represented by those vectors."
Ranking (information retrieval),"Ranking of query is one of the fundamental problems in information retrieval (IR), the scientific/engineering discipline behind search engines. Given a query q and a collection D of documents that match the query, the problem is to rank, that is, sort, the documents in D according to some criterion so that the ""best"" results appear early in the result list displayed to the user. Ranking in terms of information retrieval is an important concept in computer science and is used in many different applications such as search engine queries and recommender systems. A majority of search engines use ranking algorithms to provide users with accurate and relevant results."
Degrees of freedom (statistics),"In statistics, the number of degrees of freedom is the number of values in the final calculation of a statistic that are free to vary.Estimates of statistical parameters can be based upon different amounts of information or data. The number of independent pieces of information that go into the estimate of a parameter is called the degrees of freedom. In general, the degrees of freedom of an estimate of a parameter are equal to the number of independent scores that go into the estimate minus the number of parameters used as intermediate steps in the estimation of the parameter itself.  For example, if the variance is to be estimated from a random sample of N independent scores, then the degrees of freedom is equal to the number of independent scores (N) minus the number of parameters estimated as intermediate steps (one, namely, the sample mean) and is therefore equal to N − 1.Mathematically, degrees of freedom is the number of dimensions of the domain of a random vector, or essentially the number of ""free"" components (how many components need to be known before the vector is fully determined).
The term is most often used in the context of linear models (linear regression, analysis of variance), where certain random vectors are constrained to lie in linear subspaces, and the number of degrees of freedom is the dimension of the subspace. The degrees of freedom are also commonly associated with the squared lengths (or ""sum of squares"" of the coordinates) of such vectors, and the parameters of chi-squared and other distributions that arise in associated statistical testing problems.
While introductory textbooks may introduce degrees of freedom as distribution parameters or through hypothesis testing, it is the underlying geometry that defines degrees of freedom, and is critical to a proper understanding of the concept."
Multivariate normal distribution,"In probability theory and statistics, the multivariate normal distribution, multivariate Gaussian distribution, or joint normal distribution is a generalization of the one-dimensional (univariate) normal distribution to higher dimensions.  One definition is that a random vector is said to be k-variate normally distributed if every linear combination of its k components has a univariate normal distribution. Its importance derives mainly from the multivariate central limit theorem. The multivariate normal distribution is often used to describe, at least approximately, any set of (possibly) correlated real-valued random variables each of which clusters around a mean value."
Scoring rule,"In decision theory, a scoring rule provides a summary measure for the evaluation of probabilistic predictions or forecasts. It is applicable to tasks in which predictions assign probabilities to events, i.e. one issues a probability distribution 
  
    
      
        F
      
    
    {\displaystyle F}
   as prediction. This includes probabilistic classification of a set of mutually exclusive outcomes or classes.
On the other side, a scoring function provides a summary measure for the evaluation of point predictions, i.e. one predicts a property or functional 
  
    
      
        T
        (
        F
        )
      
    
    {\displaystyle T(F)}
  , like the expectation or the median.
Scoring rules and scoring functions can be thought of as ""cost function"" or ""loss function"". They are evaluated as empirical mean of a given sample, simply called score. Scores of different predictions or models can then be compared to conclude which model is best.
If a cost is levied in proportion to a proper scoring rule, the minimal expected cost corresponds to reporting the true set of probabilities. Proper scoring rules are used in meteorology, finance, and pattern classification where a forecaster or algorithm will attempt to minimize the average score to yield refined, calibrated probabilities (i.e. accurate probabilities)."
documents,"A document is a written, drawn, presented, or memorialized representation of thought, often the manifestation of non-fictional, as well as fictional, content.  The word originates from the Latin Documentum, which denotes a ""teaching"" or ""lesson"": the verb doceō denotes ""to teach"". In the past, the word was usually used to denote written proof useful as evidence of a truth or fact. In the computer age, ""document"" usually denotes a primarily textual computer file, including its structure and format, e.g. fonts, colors, and images. Contemporarily, ""document"" is not defined by its transmission medium, e.g., paper, given the existence of electronic documents. ""Documentation"" is distinct because it has more denotations than ""document"". Documents are also distinguished from ""realia"", which are three-dimensional objects that would otherwise satisfy the definition of ""document"" because they memorialize or represent thought; documents are considered more as 2-dimensional representations. While documents can have large varieties of customization, all documents can be shared freely and have the right to do so, creativity can be represented by documents, also. History, events, examples, opinions, etc. all can be expressed in documents."
Document,"A document is a written, drawn, presented, or memorialized representation of thought, often the manifestation of non-fictional, as well as fictional, content.  The word originates from the Latin Documentum, which denotes a ""teaching"" or ""lesson"": the verb doceō denotes ""to teach"". In the past, the word was usually used to denote written proof useful as evidence of a truth or fact. In the computer age, ""document"" usually denotes a primarily textual computer file, including its structure and format, e.g. fonts, colors, and images. Contemporarily, ""document"" is not defined by its transmission medium, e.g., paper, given the existence of electronic documents. ""Documentation"" is distinct because it has more denotations than ""document"". Documents are also distinguished from ""realia"", which are three-dimensional objects that would otherwise satisfy the definition of ""document"" because they memorialize or represent thought; documents are considered more as 2-dimensional representations. While documents can have large varieties of customization, all documents can be shared freely and have the right to do so, creativity can be represented by documents, also. History, events, examples, opinions, etc. all can be expressed in documents."
False document,A false document is a technique by which an author aims to increase verisimilitude in a work of fiction by inventing and inserting or mentioning documents that appear to be factual. The goal of a false document is to convince an audience that what is being presented is factual.
Book of Documents,"The Book of Documents (Shūjīng, earlier Shu King) or Classic of History, also known as the Shangshu (“Venerated Documents”), is one of the Five Classics of ancient Chinese literature.  It is a collection of rhetorical prose attributed to figures of ancient China, and served as the foundation of Chinese political philosophy for over 2,000 years.
The Book of Documents was the subject of one of China's oldest literary controversies, between proponents of different versions of the text. A version was preserved from Qin Shi Huang's burning of books and burying of scholars by scholar Fu Sheng, in 29 sections (pian 篇). This group of texts were referred to as ""Modern Script"" jinwen 今文, because written with the script in use at the beginning of the Western Han dyansty. According to Western Han dynasty documents, new textual material was  discovered in the wall of Confucius' family estate in Qufu by his descendant Kong Anguo in the late 2nd century BC. This new material was referred to as ""Old Script"" guwen 古文, because written in the script that predated the standardization of Chinese script enforced during the Qing dynasty. Compared to the ""Modern Script"" texts, the ""Old Script"" material had 16 more texts. However, this seems to have been lost at the end of the Eastern Han dynasty, while the ""Modern Script"" text enjoyed circulation, in particular in Ouyang Gao’s 歐陽高 (a. 136 BCE) study of it, the Ouyang Shangshu 歐陽尚書. This was the basis of studies by Ma Rong and Zheng Xuan in the Easter Han Dynasty. By the end of the second century CE, there was knowledge that the Shangshu at some point included more than the ""Modern Script"" text. This likely prompted scholars to recreate the ""Old Script"" texts said to have once belonged to the Shangshu, a process that culminated with the presentation of a 58 section (59 if the preface is included in the count) Shangshu to the Eastern Jin court, in 317 CE, by Mei Ze 梅頤.
This version was accepted, among doubts, and later was canonized as part of Kong Yingda's project. It was only in the 17th century that Qing dynasty scholar Yan Ruoqu demonstrated that the ""Old Script"" were actually fabrications ""reconstructed"" in the 3rd or 4th centuries AD.
In the transmitted edition, texts are grouped into four sections representing different eras: the semi-mythical reign of Yu the Great, and the Xia, Shang and Zhou dynasties.  The Zhou section accounts for over half the text.  Some of its New Text chapters are among the earliest examples of Chinese prose, recording speeches from the early years of the Zhou dynasty in the late 11th century BC.  Although the other three sections purport to record earlier material, most scholars believe that even the New Script chapters in these sections were composed later than those in the Zhou section, with chapters relating to the earliest periods being as recent as the 4th or 3rd centuries BC."
Halloween documents,"The Halloween documents comprise a series of confidential Microsoft memoranda on potential strategies relating to free software, open-source software, and to Linux in particular, and a series of media responses to these memoranda. Both the leaked documents and the responses were published by Eric S. Raymond in 1998.The documents are associated with Halloween because many of them were originally leaked close to October 31 in different years.

"
Electronic document,"An electronic document is any electronic media content (other than computer programs or system files) that is intended to be used in either an electronic form or as printed output. Originally, any computer data were considered as something internal — the final data output was always on paper. However, the development of computer networks has made it so that in most cases it is much more convenient to distribute electronic documents than printed ones. The improvements in electronic visual display technologies made it possible to view documents on screen instead of printing them (thus saving paper and the space required to store the printed copies).However, using electronic documents for final presentation instead of paper has created the problem of multiple incompatible file formats. Even plain text computer files are not free from this problem — e.g. under MS-DOS, most programs could not work correctly with UNIX-style text files (see newline), and for non-English speakers, the different code pages always have been a source of trouble.
Even more problems are connected with complex file formats of various word processors, spreadsheets, and graphics software. To alleviate the problem, many software companies distribute free file viewers for their proprietary file formats (one example is Adobe's Acrobat Reader). The other solution is the development of standardized non-proprietary file formats (such as HTML and OpenDocument), and electronic documents for specialized uses have specialized formats – the specialized electronic articles in physics use TeX or PostScript."
Historical document,"Historical documents are original documents that contain important historical information about a person, place, or event and can thus serve as primary sources as important ingredients of the historical methodology.
Significant historical documents can be deeds, laws, accounts of battles (often given by the victors or persons sharing their viewpoint), or the exploits of the powerful. Though these documents are of historical interest, they do not detail the daily lives of ordinary people, or the way society functioned. Anthropologists, historians and archeologists generally are more interested in documents that describe the day-to-day lives of ordinary people, indicating what they ate, their interaction with other members of their households and social groups, and their states of mind. It is this information that allows them to try to understand and describe the way society was functioning at any particular time in history. Greek ostraka provide good examples of historical documents from ""among the common people"".
Many documents that are produced today, such as personal letters, pictures, contracts, newspapers, and medical records, would be considered valuable historical documents in the future. However most of these will be lost in the future since they are either printed on ordinary paper which has a limited lifespan, or even stored in digital formats, then lost track over time.
Some companies and government entities are attempting to increase the number of documents that will survive the passage of time, by taking into account the preservation issues, and either printing documents in a manner that would increase the likelihood of them surviving indefinitely, or placing selected documents in time capsules or other special storage environments."
Document management system,"A document management system (DMS) is usually a computerized system used to store, share, track and manage files or documents. Some systems include history tracking where a log of the various versions created and modified by different users is recorded. The term has some overlap with the concepts of content management systems. It is often viewed as a component of enterprise content management (ECM) systems and related to digital asset management, document imaging, workflow systems and records management systems."
Constitution,"Institutions are humanly devised structures of rules and norms that shape and constrain individual behavior. All definitions of institutions generally entail that there is a level of persistence and continuity. Laws, rules, social conventions and norms are all examples of institutions. Institutions vary in their level of formality and informality.Institutions are a principal object of study in social sciences such as political science, anthropology, economics, and sociology (the latter described by Émile Durkheim as the ""science of institutions, their genesis and their functioning""). Primary or meta-institutions are institutions such as the family or money that are broad enough to encompass sets of related institutions. Institutions are also a central concern for law, the formal mechanism for political rule-making and enforcement. Historians study and document the founding, growth, decay and development of institutions as part of political, economic and cultural history."
Identity document,
Google Docs,"Google Docs is an online word processor included as part of the free, web-based Google Docs Editors suite offered by Google, which also includes: Google Sheets, Google Slides, Google Drawings, Google Forms, Google Sites and Google Keep. Google Docs is accessible via an internet browser as a web-based application and is also available as a mobile app on Android and iOS and as a desktop application on Google's ChromeOS.
Google Docs allows users to create and edit documents online while collaborating with other users in real time. Edits are tracked by the user making the edit, with a revision history presenting changes. An editor's position is highlighted with an editor-specific color and cursor, and a permissions system regulates what users can do. Updates have introduced features using machine learning, including ""Explore"", offering search results based on the contents of a document, and ""Action items"", allowing users to assign tasks to other users.Google Docs supports opening and saving documents in the standard OpenDocument format as well as in Rich text format, plain Unicode text, zipped HTML, and Microsoft Word. Exporting to PDF and EPUB formats are implemented."
tf-idf,"In information retrieval, tf–idf (also TF*IDF, TFIDF, TF–IDF, or Tf–idf), short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling.
The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. tf–idf is one of the most popular term-weighting schemes today. A survey conducted in 2015 showed that 83% of text-based recommender systems in digital libraries use tf–idf.Variations of the tf–idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query. tf–idf can be successfully used for stop-words filtering in various subject fields, including text summarization and classification.
One of the simplest ranking functions is computed by summing the tf–idf for each query term; many more sophisticated ranking functions are variants of this simple model."
Tf–idf,"In information retrieval, tf–idf (also TF*IDF, TFIDF, TF–IDF, or Tf–idf), short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling.
The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. tf–idf is one of the most popular term-weighting schemes today. A survey conducted in 2015 showed that 83% of text-based recommender systems in digital libraries use tf–idf.Variations of the tf–idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query. tf–idf can be successfully used for stop-words filtering in various subject fields, including text summarization and classification.
One of the simplest ranking functions is computed by summing the tf–idf for each query term; many more sophisticated ranking functions are variants of this simple model."
vector spaces,"In mathematics and physics, a vector space (also called a linear space) is a set whose elements, often called vectors, may be added together and multiplied (""scaled"") by numbers called scalars. Scalars are often real numbers, but can be  complex numbers or, more generally, elements of any field. The operations of vector addition and scalar multiplication must satisfy certain requirements, called vector axioms. The terms real vector space and complex vector space are often used to specify the nature of the scalars: real coordinate space or complex coordinate space.
Vector spaces generalize Euclidean vectors, which allow modeling of physical quantities, such as forces and velocity, that have not only a magnitude, but also a direction. The concept of vector spaces is fundamental for linear algebra, together with the concept of matrix, which allows computing in vector spaces. This provides a concise and synthetic way for manipulating and studying systems of linear equations.
Vector spaces are characterized by their dimension, which, roughly speaking, specifies the number of independent directions in the space. This means that, for two vector spaces with the same dimension, the properties that depend only on the vector-space structure are exactly the same (technically the vector spaces are isomorphic). A vector space is finite-dimensional if its dimension is a natural number. Otherwise, it is infinite-dimensional, and its dimension is an infinite cardinal. Finite-dimensional vector spaces occur naturally in geometry and related areas. Infinite-dimensional vector spaces occur in many areas of mathematics. For example, polynomial rings are countably infinite-dimensional vector spaces, and many function spaces have the cardinality of the continuum as a dimension.
Many vector spaces that are considered in mathematics are also endowed with other structures. This is the case of algebras, which include field extensions, polynomial rings, associative algebras and Lie algebras. This is also the case of topological vector spaces, which include function spaces, inner product spaces, normed spaces, Hilbert spaces and Banach spaces."
Vector space,"In mathematics and physics, a vector space (also called a linear space) is a set whose elements, often called vectors, may be added together and multiplied (""scaled"") by numbers called scalars. Scalars are often real numbers, but can be  complex numbers or, more generally, elements of any field. The operations of vector addition and scalar multiplication must satisfy certain requirements, called vector axioms. The terms real vector space and complex vector space are often used to specify the nature of the scalars: real coordinate space or complex coordinate space.
Vector spaces generalize Euclidean vectors, which allow modeling of physical quantities, such as forces and velocity, that have not only a magnitude, but also a direction. The concept of vector spaces is fundamental for linear algebra, together with the concept of matrix, which allows computing in vector spaces. This provides a concise and synthetic way for manipulating and studying systems of linear equations.
Vector spaces are characterized by their dimension, which, roughly speaking, specifies the number of independent directions in the space. This means that, for two vector spaces with the same dimension, the properties that depend only on the vector-space structure are exactly the same (technically the vector spaces are isomorphic). A vector space is finite-dimensional if its dimension is a natural number. Otherwise, it is infinite-dimensional, and its dimension is an infinite cardinal. Finite-dimensional vector spaces occur naturally in geometry and related areas. Infinite-dimensional vector spaces occur in many areas of mathematics. For example, polynomial rings are countably infinite-dimensional vector spaces, and many function spaces have the cardinality of the continuum as a dimension.
Many vector spaces that are considered in mathematics are also endowed with other structures. This is the case of algebras, which include field extensions, polynomial rings, associative algebras and Lie algebras. This is also the case of topological vector spaces, which include function spaces, inner product spaces, normed spaces, Hilbert spaces and Banach spaces."
Normed vector space,"In mathematics, a normed vector space or normed space is a vector space over the real or complex numbers, on which a norm is defined. A norm is the formalization and the generalization to real vector spaces of the intuitive notion of ""length"" in the real (physical) world. A norm is a real-valued function defined on the vector space that is commonly denoted 
  
    
      
        x
        ↦
        ‖
        x
        ‖
        ,
      
    
    {\displaystyle x\mapsto \|x\|,}
   and has the following properties:
It is nonnegative, meaning that 
  
    
      
        ‖
        x
        ‖
        ≥
        0
      
    
    {\displaystyle \|x\|\geq 0}
   for every vector 
  
    
      
        x
        .
      
    
    {\displaystyle x.}
  
It is positive on nonzero vectors, that is, 
For every vector 
  
    
      
        x
        ,
      
    
    {\displaystyle x,}
   and every scalar 
  
    
      
        α
        ,
      
    
    {\displaystyle \alpha ,}
   
The triangle inequality holds; that is, for every vectors 
  
    
      
        x
      
    
    {\displaystyle x}
   and 
  
    
      
        y
        ,
      
    
    {\displaystyle y,}
   A norm induces a distance, called its (norm) induced metric, by the formula

which makes any normed vector space into a metric space and a topological vector space. If this metric space is complete then the normed space is a Banach space. Every normed vector space can be ""uniquely extended"" to a Banach space, which makes normed spaces intimately related to Banach spaces. Every Banach space is a normed space but converse is not true. For example, the set of the finite sequences of real numbers can be normed with the Euclidean norm, but it is not complete for this norm. 
An inner product space is a normed vector space whose norm is the square root of the inner product of a vector and itself. The Euclidean norm of a Euclidean vector space is a special case that allows defining Euclidean distance by the formula

The study of normed spaces and Banach spaces is a fundamental part of functional analysis, which is a major subfield of mathematics.

"
Topological vector space,"In mathematics, a topological vector space (also called a linear topological space and commonly abbreviated TVS or t.v.s.) is one of the basic structures investigated in functional analysis.
A topological vector space is a vector space that is also a topological space with the property that the vector space operations (vector addition and scalar multiplication) are also continuous functions. Such a topology is called a vector topology and every topological vector space has a uniform topological structure, allowing a notion of uniform convergence and completeness. Some authors also require that the space is a Hausdorff space (although this article does not). One of the most widely studied categories of TVSs are locally convex topological vector spaces. This article focuses on TVSs that are not necessarily locally convex. Banach spaces, Hilbert spaces and Sobolev spaces are other well-known examples of TVSs.
Many topological vector spaces are spaces of functions, or linear operators acting on topological vector spaces, and the topology is often defined so as to capture a particular notion of convergence of sequences of functions.
In this article, the scalar field of a topological vector space will be assumed to be either the complex numbers 
  
    
      
        
          C
        
      
    
    {\displaystyle \mathbb {C} }
   or the real numbers 
  
    
      
        
          R
        
        ,
      
    
    {\displaystyle \mathbb {R} ,}
   unless clearly stated otherwise."
Dimension (vector space),"In mathematics, the dimension of a vector space V is the cardinality (i.e., the number of vectors) of a basis of V over its base field. It is sometimes called Hamel dimension (after Georg Hamel) or algebraic dimension to distinguish it from other types of dimension.
For every vector space there exists a basis, and all bases of a vector space have equal cardinality; as a result, the dimension of a vector space is uniquely defined. We say 
  
    
      
        V
      
    
    {\displaystyle V}
   is finite-dimensional if the dimension of 
  
    
      
        V
      
    
    {\displaystyle V}
   is finite, and infinite-dimensional if its dimension is infinite.
The dimension of the vector space 
  
    
      
        V
      
    
    {\displaystyle V}
   over the field 
  
    
      
        F
      
    
    {\displaystyle F}
   can be written as 
  
    
      
        
          dim
          
            F
          
        
        ⁡
        (
        V
        )
      
    
    {\displaystyle \dim _{F}(V)}
   or as 
  
    
      
        [
        V
        :
        F
        ]
        ,
      
    
    {\displaystyle [V:F],}
   read ""dimension of 
  
    
      
        V
      
    
    {\displaystyle V}
   over 
  
    
      
        F
      
    
    {\displaystyle F}
  "". When 
  
    
      
        F
      
    
    {\displaystyle F}
   can be inferred from context, 
  
    
      
        dim
        ⁡
        (
        V
        )
      
    
    {\displaystyle \dim(V)}
   is typically written."
Inner product space,"In mathematics, an inner product space (or, rarely, a Hausdorff pre-Hilbert space) is a real vector space or a complex vector space with an operation called an inner product.  The inner product of two vectors in the space is a scalar, often denoted with angle brackets such as in 
  
    
      
        ⟨
        a
        ,
        b
        ⟩
      
    
    {\displaystyle \langle a,b\rangle }
  . Inner products allow formal definitions of intuitive geometric notions, such as lengths, angles, and orthogonality (zero inner product) of vectors. Inner product spaces generalize Euclidean vector spaces, in which the inner product is the dot product or scalar product of Cartesian coordinates. Inner product spaces of infinite dimension are widely used in functional analysis. Inner product spaces over the field of complex numbers are sometimes referred to as unitary spaces. The first usage of the concept of a vector space with an inner product is due to Giuseppe Peano, in 1898.An inner product naturally induces an associated norm, (denoted 
  
    
      
        
          |
        
        x
        
          |
        
      
    
    {\displaystyle |x|}
   and 
  
    
      
        
          |
        
        y
        
          |
        
      
    
    {\displaystyle |y|}
   in the picture); so, every inner product space is a normed vector space. If this normed space is also complete (that is, a Banach space) then the inner product space is a Hilbert space. If an inner product space H is not a Hilbert space, it can be extended by completion to a Hilbert space 
  
    
      
        
          
            H
            ¯
          
        
        .
      
    
    {\displaystyle {\overline {H}}.}
   This means that 
  
    
      
        H
      
    
    {\displaystyle H}
   is a linear subspace of 
  
    
      
        
          
            H
            ¯
          
        
        ,
      
    
    {\displaystyle {\overline {H}},}
   the inner product of 
  
    
      
        H
      
    
    {\displaystyle H}
   is the restriction of that of 
  
    
      
        
          
            H
            ¯
          
        
        ,
      
    
    {\displaystyle {\overline {H}},}
   and 
  
    
      
        H
      
    
    {\displaystyle H}
   is dense in 
  
    
      
        
          
            H
            ¯
          
        
      
    
    {\displaystyle {\overline {H}}}
   for the topology defined by the norm.

"
vector space score,"In statistics, the score (or informant) is the gradient of the log-likelihood function with respect to the parameter vector. Evaluated at a particular point of the parameter vector, the score indicates the steepness of the log-likelihood function and thereby the sensitivity to infinitesimal changes to the parameter values. If the log-likelihood function is continuous over the parameter space, the score will vanish at a local maximum or minimum; this fact is used in maximum likelihood estimation to find the parameter values that maximize the likelihood function.
Since the score is a function of the observations that are subject to sampling error, it lends itself to a test statistic known as score test in which the parameter is held at a particular value. Further, the ratio of two likelihood functions evaluated at two distinct parameter values can be understood as a definite integral of the score function."
Score (statistics),"In statistics, the score (or informant) is the gradient of the log-likelihood function with respect to the parameter vector. Evaluated at a particular point of the parameter vector, the score indicates the steepness of the log-likelihood function and thereby the sensitivity to infinitesimal changes to the parameter values. If the log-likelihood function is continuous over the parameter space, the score will vanish at a local maximum or minimum; this fact is used in maximum likelihood estimation to find the parameter values that maximize the likelihood function.
Since the score is a function of the observations that are subject to sampling error, it lends itself to a test statistic known as score test in which the parameter is held at a particular value. Further, the ratio of two likelihood functions evaluated at two distinct parameter values can be understood as a definite integral of the score function."
Support vector machine,"In machine learning, support vector machines (SVMs, also support vector networks) are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis. Developed at AT&T Bell Laboratories by Vladimir Vapnik with colleagues (Boser et al., 1992, Guyon et al., 1993, Cortes and Vapnik, 1995, Vapnik et al., 1997) SVMs are one of the most robust prediction methods, being based on statistical learning frameworks or VC theory proposed by Vapnik (1982, 1995) and Chervonenkis (1974). Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting). SVM maps training examples to points in space so as to maximise the width of the gap between the two categories. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.
In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.
When data are unlabelled, supervised learning is not possible, and an unsupervised learning approach is required, which attempts to find natural clustering of the data to groups, and then map new data to these formed groups. The support vector clustering algorithm, created by Hava Siegelmann and Vladimir Vapnik, applies the statistics of support vectors, developed in the support vector machines algorithm, to categorize unlabeled data."
Feature (machine learning),"In machine learning and pattern recognition, a feature is an individual measurable property or characteristic of a phenomenon. Choosing informative, discriminating and independent features is a crucial element of effective algorithms in pattern recognition,  classification and regression. Features are usually numeric, but structural features such as strings and graphs are used in syntactic pattern recognition. The concept of ""feature"" is related to that of explanatory variable used in statistical techniques such as linear regression."
Word embedding,"In natural language processing (NLP), word embedding is a term used for the representation of words for text analysis, typically in the form of a real-valued vector that encodes the meaning of the word such that the words that are closer in the vector space are expected to be similar in meaning. Word embeddings can be obtained using a set of language modeling and feature learning techniques where words or phrases from the vocabulary are mapped to vectors of real numbers.
Methods to generate this mapping include neural networks, dimensionality reduction on the word co-occurrence matrix, probabilistic models, explainable knowledge base method, and explicit representation in terms of the context in which words appear.Word and phrase embeddings, when used as the underlying input representation, have been shown to boost the performance in NLP tasks such as syntactic parsing and sentiment analysis."
Ranking (information retrieval),"Ranking of query is one of the fundamental problems in information retrieval (IR), the scientific/engineering discipline behind search engines. Given a query q and a collection D of documents that match the query, the problem is to rank, that is, sort, the documents in D according to some criterion so that the ""best"" results appear early in the result list displayed to the user. Ranking in terms of information retrieval is an important concept in computer science and is used in many different applications such as search engine queries and recommender systems. A majority of search engines use ranking algorithms to provide users with accurate and relevant results."
Principal component analysis,"Principal component analysis (PCA) is a popular technique for analyzing large datasets containing a high number of dimensions/features per observation, increasing the interpretability of data while preserving the maximum amount of information, and enabling the visualization of multidimensional data. Formally, PCA is a statistical technique for reducing the dimensionality of a dataset. This is accomplished by linearly transforming the data into a new coordinate system where (most of) the variation in the data can be described with fewer dimensions than the initial data. Many studies use the first two principal components in order to plot the data in two dimensions and to visually identify clusters of closely related data points. Principal component analysis has applications in many fields such as population genetics, microbiome studies, and atmospheric science. 
The principal components of a collection of points in a real coordinate space are a sequence of 
  
    
      
        p
      
    
    {\displaystyle p}
   unit vectors, where the 
  
    
      
        i
      
    
    {\displaystyle i}
  -th vector is the direction of a line that best fits the data while being orthogonal to the first 
  
    
      
        i
        −
        1
      
    
    {\displaystyle i-1}
   vectors. Here, a best-fitting line is defined as one that minimizes the average squared perpendicular distance from the points to the line. These directions constitute an orthonormal basis in which different individual dimensions of the data are linearly uncorrelated. Principal component analysis (PCA) is the process of computing the principal components and using them to perform a change of basis on the data, sometimes using only the first few principal components and ignoring the rest.
In data analysis, the first principal component of a set of  
  
    
      
        p
      
    
    {\displaystyle p}
   variables, presumed to be jointly normally distributed, is the derived variable formed as a linear combination of the original variables that explains the most variance. The second principal component explains the most variance in what is left once the effect of the first component is removed, and we may proceed through  
  
    
      
        p
      
    
    {\displaystyle p}
   iterations until all the variance is explained. PCA is most commonly used when many of the variables are highly correlated with each other and it is desirable to reduce their number to an independent set.
PCA is used in exploratory data analysis and for making predictive models. It is commonly used for dimensionality reduction by projecting each data point onto only the first few principal components to obtain lower-dimensional data while preserving as much of the data's variation as possible. The first principal component can equivalently be defined as a direction that maximizes the variance of the projected data. The 
  
    
      
        i
      
    
    {\displaystyle i}
  -th principal component can be taken as a direction orthogonal to the first 
  
    
      
        i
        −
        1
      
    
    {\displaystyle i-1}
   principal components that maximizes the variance of the projected data.
For either objective, it can be shown that the principal components are eigenvectors of the data's covariance matrix. Thus, the principal components are often computed by eigendecomposition of the data covariance matrix or singular value decomposition of the data matrix. PCA is the simplest of the true eigenvector-based multivariate analyses and is closely related to factor analysis. Factor analysis typically incorporates more domain specific assumptions about the underlying structure and solves eigenvectors of a slightly different matrix. PCA is also related to canonical correlation analysis (CCA). CCA defines coordinate systems that optimally describe the cross-covariance between two datasets while PCA defines a new orthogonal coordinate system that optimally describes variance in a single dataset. Robust and L1-norm-based variants of standard PCA have also been proposed.

"
Explicit semantic analysis,"In natural language processing and information retrieval, explicit semantic analysis (ESA) is a vectoral representation of text (individual words or entire documents) that uses a document corpus as a knowledge base. Specifically, in ESA, a word is represented as a column vector in the tf–idf matrix of the text corpus and a document (string of words) is represented as the centroid of the vectors representing its words. Typically, the text corpus is English Wikipedia, though other corpora including the Open Directory Project have been used.ESA was designed by Evgeniy Gabrilovich and Shaul Markovitch as a means of improving text categorization
and has been used by this pair of researchers to compute what they refer to as ""semantic relatedness"" by means of cosine similarity between the aforementioned vectors, collectively interpreted as a space of ""concepts explicitly defined and described by humans"", where Wikipedia articles (or ODP entries, or otherwise titles of documents in the knowledge base corpus) are equated with concepts. The name ""explicit semantic analysis"" contrasts with latent semantic analysis (LSA), because the use of a knowledge base makes it possible to assign human-readable labels to the concepts that make up the vector space."
Word2vec,"Word2vec is a technique for natural language processing (NLP) published in 2013. The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence. As the name implies, word2vec represents each distinct word with a particular list of numbers called a vector. The vectors are chosen carefully such that they capture the semantic and syntactic qualities of words; as such, a simple mathematical function (cosine similarity) can indicate the level of semantic similarity between the words represented by those vectors."
Feature scaling,"Feature scaling is a method used to normalize the range of independent variables or features of data. In data processing, it is also known as data normalization and is generally performed during the data preprocessing step."
Propensity score matching,"In the statistical analysis of observational data, propensity score matching (PSM) is a statistical matching technique that attempts to estimate the effect of a treatment, policy, or other intervention by accounting for the covariates that predict receiving the treatment. PSM attempts to reduce the bias due to confounding variables that could be found in an estimate of the treatment effect obtained from simply comparing outcomes among units that received the treatment versus those that did not. Paul R. Rosenbaum and Donald Rubin introduced the technique in 1983.The possibility of bias arises because a difference in the treatment outcome (such as the average treatment effect) between treated and untreated groups may be caused by a factor that predicts treatment rather than the treatment itself. In randomized experiments, the randomization enables unbiased estimation of treatment effects; for each covariate, randomization implies that treatment-groups will be balanced on average, by the law of large numbers. Unfortunately, for observational studies, the assignment of treatments to research subjects is typically not random. Matching attempts to reduce the treatment assignment bias, and mimic randomization, by creating a sample of units that received the treatment that is comparable on all observed covariates to a sample of units that did not receive the treatment.
For example, one may be interested to know the consequences of smoking. An observational study is required since it is unethical to randomly assign people to the treatment 'smoking.' The treatment effect estimated by simply comparing those who smoked to those who did not smoke would be biased by any factors that predict smoking (e.g.: gender and age). PSM attempts to control for these biases by making the groups receiving treatment and not-treatment comparable with respect to the control variables."
information needs,"The term information need is often understood as an individual or group's desire to locate and obtain information to satisfy a conscious or unconscious need. Rarely mentioned in general literature about needs, it is a common term in information science. According to Hjørland (1997) it is closely related to the concept of relevance: If something is relevant for a person in relation to a given task, we might say that the person needs the information for that task. 
Information needs are related to, but distinct from information requirements. They are studied for:

The explanation of observed phenomena of information use or expressed need;
The prediction of instances of information uses;
The control and thereby improvement of the utilization of information manipulation of essential conditions."
Information needs,"The term information need is often understood as an individual or group's desire to locate and obtain information to satisfy a conscious or unconscious need. Rarely mentioned in general literature about needs, it is a common term in information science. According to Hjørland (1997) it is closely related to the concept of relevance: If something is relevant for a person in relation to a given task, we might say that the person needs the information for that task. 
Information needs are related to, but distinct from information requirements. They are studied for:

The explanation of observed phenomena of information use or expressed need;
The prediction of instances of information uses;
The control and thereby improvement of the utilization of information manipulation of essential conditions."
Maslow's hierarchy of needs,"Maslow's hierarchy of needs is an idea in psychology proposed by American psychologist Abraham Maslow in his 1943 paper ""A Theory of Human Motivation"" in the journal Psychological Review. Maslow subsequently extended the idea to include his observations of humans' innate curiosity. His theories parallel many other theories of human developmental psychology, some of which focus on describing the stages of growth in humans. The theory is a classification system intended to reflect the universal needs of society as its base, then proceeding to more acquired emotions. The hierarchy of needs is split between deficiency needs and growth needs, with two key themes involved within the theory being individualism and the prioritization of needs. While the theory is usually shown as a pyramid in illustrations, Maslow himself never created a pyramid to represent the hierarchy of needs. The hierarchy of needs is a psychological idea and also an assessment tool, particularly in education, healthcare and social work. The hierarchy remains a popular framework in sociology research, including management training and higher education.Moreover, the hierarchy of needs is used to study how humans intrinsically partake in behavioral motivation. Maslow used the terms ""physiological"", ""safety"", ""belonging and love"", ""social needs"" or ""esteem"", ""self-actualization"" and ""transcendence"" to describe the pattern through which human needs and motivations generally move. This means that, according to the theory, for motivation to arise at the next stage, each prior stage must be satisfied by an individual. The hierarchy has been used to explain how effort and motivation are correlated in the context of human behavior. Each of these individual levels contains a certain amount of internal sensation that must be met in order for an individual to complete their hierarchy. The goal in Maslow's hierarchy is to attain the level or stage of self-actualization.Although widely used and researched, Maslow's hierarchy of needs lacks conclusive supporting evidence and the validity of the theory remains contested in academia. One criticism of the original theory which has been revised into newer versions of the theory, was that the original hierarchy states that a lower level must be completely satisfied and fulfilled before moving onto a higher pursuit; there is evidence to suggest that levels continuously overlap each other. Other criticisms include the placement location of sex in the hierarchy, the assumption of individualism in the theory, and lack of accounting for regional variances in culture and availability of resources."
Library,"A library is a collection of materials, books or media that are accessible for use and not just for display purposes. A library provides physical (hard copies) or digital access (soft copies) materials, and may be a physical location or a virtual space, or both. A library's collection can include printed materials and other physical resources in many formats such as DVD, CD and cassette as well as access to information, music or other content held on bibliographic databases.
A library, which may vary widely in size, may be organized for use and maintained by a public body such as a government; an institution such as a school or museum; a corporation; or a private individual. In addition to providing materials, libraries also provide the services of librarians who are trained and experts at finding, selecting, circulating and organizing information and at interpreting information needs, navigating and analyzing very large amounts of information with a variety of resources.
Library buildings often provide quiet areas for studying, as well as common areas for group study and collaboration, and may provide public facilities for access to their electronic resources; for instance: computers and access to the Internet. The library's clientele and services offered vary depending on its type: users of a public library have different needs from those of a special library or academic library, for example. Libraries may also be community hubs, where programs are delivered and people engage in lifelong learning. Modern libraries extend their services beyond the physical walls of a building by providing material accessible by electronic means, including from home via the Internet.
The services the library offers are variously described as library services, information services, or the combination ""library and information services"", although different institutions and sources define such terminology differently."
test collections,"In software development, a test suite, less commonly known as a validation suite, is a collection of test cases that are intended to be used to test a software program to show that it has some specified set of behaviors. A test suite often contains detailed instructions or goals for each collection of test cases and information on the system configuration to be used during testing. A group of test cases may also contain prerequisite states or steps, and descriptions of the following tests.
Collections of test cases are sometimes termed a test plan, a test script, or even a test scenario."
ranked retrieval results,"Ranking of query is one of the fundamental problems in information retrieval (IR), the scientific/engineering discipline behind search engines. Given a query q and a collection D of documents that match the query, the problem is to rank, that is, sort, the documents in D according to some criterion so that the ""best"" results appear early in the result list displayed to the user. Ranking in terms of information retrieval is an important concept in computer science and is used in many different applications such as search engine queries and recommender systems. A majority of search engines use ranking algorithms to provide users with accurate and relevant results."
Ranking (information retrieval),"Ranking of query is one of the fundamental problems in information retrieval (IR), the scientific/engineering discipline behind search engines. Given a query q and a collection D of documents that match the query, the problem is to rank, that is, sort, the documents in D according to some criterion so that the ""best"" results appear early in the result list displayed to the user. Ranking in terms of information retrieval is an important concept in computer science and is used in many different applications such as search engine queries and recommender systems. A majority of search engines use ranking algorithms to provide users with accurate and relevant results."
Evaluation measures (information retrieval),"Evaluation measures for an information retrieval (IR) system assess how well an index, search engine or database returns results from a collection of resources that satisfy a user's query. They are therefore fundamental to the success of information systems and digital platforms. The success of an IR system may be judged by a range of criteria including relevance, speed, user satisfaction, usability, efficiency and reliability. However the most important factor in determining a systems effectiveness for users is the overall relevance of results retrieved in response to a query. Evaluation measures may be categorised in various ways including offline or online, user-based or system-based and include methods such as observed user behaviour, test collections, precision and recall, and scores from prepared benchmark test sets. 
Evaluation for an information retrieval system should also include a validation of the measures used, i.e. an assessment of how well they measure what they are intended to measure and how well the system fits its intended use case. Measures are generally used in two settings: online experimentation, which assesses users' interactions with the search system, and offline evaluation, which measures the effectiveness of an information retrieval system on a static offline collection."
documents,"A document is a written, drawn, presented, or memorialized representation of thought, often the manifestation of non-fictional, as well as fictional, content.  The word originates from the Latin Documentum, which denotes a ""teaching"" or ""lesson"": the verb doceō denotes ""to teach"". In the past, the word was usually used to denote written proof useful as evidence of a truth or fact. In the computer age, ""document"" usually denotes a primarily textual computer file, including its structure and format, e.g. fonts, colors, and images. Contemporarily, ""document"" is not defined by its transmission medium, e.g., paper, given the existence of electronic documents. ""Documentation"" is distinct because it has more denotations than ""document"". Documents are also distinguished from ""realia"", which are three-dimensional objects that would otherwise satisfy the definition of ""document"" because they memorialize or represent thought; documents are considered more as 2-dimensional representations. While documents can have large varieties of customization, all documents can be shared freely and have the right to do so, creativity can be represented by documents, also. History, events, examples, opinions, etc. all can be expressed in documents."
Document,"A document is a written, drawn, presented, or memorialized representation of thought, often the manifestation of non-fictional, as well as fictional, content.  The word originates from the Latin Documentum, which denotes a ""teaching"" or ""lesson"": the verb doceō denotes ""to teach"". In the past, the word was usually used to denote written proof useful as evidence of a truth or fact. In the computer age, ""document"" usually denotes a primarily textual computer file, including its structure and format, e.g. fonts, colors, and images. Contemporarily, ""document"" is not defined by its transmission medium, e.g., paper, given the existence of electronic documents. ""Documentation"" is distinct because it has more denotations than ""document"". Documents are also distinguished from ""realia"", which are three-dimensional objects that would otherwise satisfy the definition of ""document"" because they memorialize or represent thought; documents are considered more as 2-dimensional representations. While documents can have large varieties of customization, all documents can be shared freely and have the right to do so, creativity can be represented by documents, also. History, events, examples, opinions, etc. all can be expressed in documents."
False document,A false document is a technique by which an author aims to increase verisimilitude in a work of fiction by inventing and inserting or mentioning documents that appear to be factual. The goal of a false document is to convince an audience that what is being presented is factual.
Book of Documents,"The Book of Documents (Shūjīng, earlier Shu King) or Classic of History, also known as the Shangshu (“Venerated Documents”), is one of the Five Classics of ancient Chinese literature.  It is a collection of rhetorical prose attributed to figures of ancient China, and served as the foundation of Chinese political philosophy for over 2,000 years.
The Book of Documents was the subject of one of China's oldest literary controversies, between proponents of different versions of the text. A version was preserved from Qin Shi Huang's burning of books and burying of scholars by scholar Fu Sheng, in 29 sections (pian 篇). This group of texts were referred to as ""Modern Script"" jinwen 今文, because written with the script in use at the beginning of the Western Han dyansty. According to Western Han dynasty documents, new textual material was  discovered in the wall of Confucius' family estate in Qufu by his descendant Kong Anguo in the late 2nd century BC. This new material was referred to as ""Old Script"" guwen 古文, because written in the script that predated the standardization of Chinese script enforced during the Qing dynasty. Compared to the ""Modern Script"" texts, the ""Old Script"" material had 16 more texts. However, this seems to have been lost at the end of the Eastern Han dynasty, while the ""Modern Script"" text enjoyed circulation, in particular in Ouyang Gao’s 歐陽高 (a. 136 BCE) study of it, the Ouyang Shangshu 歐陽尚書. This was the basis of studies by Ma Rong and Zheng Xuan in the Easter Han Dynasty. By the end of the second century CE, there was knowledge that the Shangshu at some point included more than the ""Modern Script"" text. This likely prompted scholars to recreate the ""Old Script"" texts said to have once belonged to the Shangshu, a process that culminated with the presentation of a 58 section (59 if the preface is included in the count) Shangshu to the Eastern Jin court, in 317 CE, by Mei Ze 梅頤.
This version was accepted, among doubts, and later was canonized as part of Kong Yingda's project. It was only in the 17th century that Qing dynasty scholar Yan Ruoqu demonstrated that the ""Old Script"" were actually fabrications ""reconstructed"" in the 3rd or 4th centuries AD.
In the transmitted edition, texts are grouped into four sections representing different eras: the semi-mythical reign of Yu the Great, and the Xia, Shang and Zhou dynasties.  The Zhou section accounts for over half the text.  Some of its New Text chapters are among the earliest examples of Chinese prose, recording speeches from the early years of the Zhou dynasty in the late 11th century BC.  Although the other three sections purport to record earlier material, most scholars believe that even the New Script chapters in these sections were composed later than those in the Zhou section, with chapters relating to the earliest periods being as recent as the 4th or 3rd centuries BC."
Halloween documents,"The Halloween documents comprise a series of confidential Microsoft memoranda on potential strategies relating to free software, open-source software, and to Linux in particular, and a series of media responses to these memoranda. Both the leaked documents and the responses were published by Eric S. Raymond in 1998.The documents are associated with Halloween because many of them were originally leaked close to October 31 in different years.

"
Electronic document,"An electronic document is any electronic media content (other than computer programs or system files) that is intended to be used in either an electronic form or as printed output. Originally, any computer data were considered as something internal — the final data output was always on paper. However, the development of computer networks has made it so that in most cases it is much more convenient to distribute electronic documents than printed ones. The improvements in electronic visual display technologies made it possible to view documents on screen instead of printing them (thus saving paper and the space required to store the printed copies).However, using electronic documents for final presentation instead of paper has created the problem of multiple incompatible file formats. Even plain text computer files are not free from this problem — e.g. under MS-DOS, most programs could not work correctly with UNIX-style text files (see newline), and for non-English speakers, the different code pages always have been a source of trouble.
Even more problems are connected with complex file formats of various word processors, spreadsheets, and graphics software. To alleviate the problem, many software companies distribute free file viewers for their proprietary file formats (one example is Adobe's Acrobat Reader). The other solution is the development of standardized non-proprietary file formats (such as HTML and OpenDocument), and electronic documents for specialized uses have specialized formats – the specialized electronic articles in physics use TeX or PostScript."
Historical document,"Historical documents are original documents that contain important historical information about a person, place, or event and can thus serve as primary sources as important ingredients of the historical methodology.
Significant historical documents can be deeds, laws, accounts of battles (often given by the victors or persons sharing their viewpoint), or the exploits of the powerful. Though these documents are of historical interest, they do not detail the daily lives of ordinary people, or the way society functioned. Anthropologists, historians and archeologists generally are more interested in documents that describe the day-to-day lives of ordinary people, indicating what they ate, their interaction with other members of their households and social groups, and their states of mind. It is this information that allows them to try to understand and describe the way society was functioning at any particular time in history. Greek ostraka provide good examples of historical documents from ""among the common people"".
Many documents that are produced today, such as personal letters, pictures, contracts, newspapers, and medical records, would be considered valuable historical documents in the future. However most of these will be lost in the future since they are either printed on ordinary paper which has a limited lifespan, or even stored in digital formats, then lost track over time.
Some companies and government entities are attempting to increase the number of documents that will survive the passage of time, by taking into account the preservation issues, and either printing documents in a manner that would increase the likelihood of them surviving indefinitely, or placing selected documents in time capsules or other special storage environments."
Document management system,"A document management system (DMS) is usually a computerized system used to store, share, track and manage files or documents. Some systems include history tracking where a log of the various versions created and modified by different users is recorded. The term has some overlap with the concepts of content management systems. It is often viewed as a component of enterprise content management (ECM) systems and related to digital asset management, document imaging, workflow systems and records management systems."
Constitution,"Institutions are humanly devised structures of rules and norms that shape and constrain individual behavior. All definitions of institutions generally entail that there is a level of persistence and continuity. Laws, rules, social conventions and norms are all examples of institutions. Institutions vary in their level of formality and informality.Institutions are a principal object of study in social sciences such as political science, anthropology, economics, and sociology (the latter described by Émile Durkheim as the ""science of institutions, their genesis and their functioning""). Primary or meta-institutions are institutions such as the family or money that are broad enough to encompass sets of related institutions. Institutions are also a central concern for law, the formal mechanism for political rule-making and enforcement. Historians study and document the founding, growth, decay and development of institutions as part of political, economic and cultural history."
Identity document,
Google Docs,"Google Docs is an online word processor included as part of the free, web-based Google Docs Editors suite offered by Google, which also includes: Google Sheets, Google Slides, Google Drawings, Google Forms, Google Sites and Google Keep. Google Docs is accessible via an internet browser as a web-based application and is also available as a mobile app on Android and iOS and as a desktop application on Google's ChromeOS.
Google Docs allows users to create and edit documents online while collaborating with other users in real time. Edits are tracked by the user making the edit, with a revision history presenting changes. An editor's position is highlighted with an editor-specific color and cursor, and a permissions system regulates what users can do. Updates have introduced features using machine learning, including ""Explore"", offering search results based on the contents of a document, and ""Action items"", allowing users to assign tasks to other users.Google Docs supports opening and saving documents in the standard OpenDocument format as well as in Rich text format, plain Unicode text, zipped HTML, and Microsoft Word. Exporting to PDF and EPUB formats are implemented."
system evaluation,"The Kawabata evaluation system (KES) is used to measure the mechanical properties of fabrics. The system was developed by a team led by Professor Kawabata in the department of polymer chemistry, Kyoto University Japan.
KES is composed of four different machines on which a total of six tests can be performed:
Tensile & shear tester – tensile, shear
Pure bending tester – pure bending
Compression tester – compression
Surface tester – surface friction and roughnessThe evaluation can include measurement of the transient heat transfer properties associated with the sensation of coolness generated when fabrics contact the skin during wear. The KES not only predicts human response but understands the perception of softness."
Kawabata evaluation system,"The Kawabata evaluation system (KES) is used to measure the mechanical properties of fabrics. The system was developed by a team led by Professor Kawabata in the department of polymer chemistry, Kyoto University Japan.
KES is composed of four different machines on which a total of six tests can be performed:
Tensile & shear tester – tensile, shear
Pure bending tester – pure bending
Compression tester – compression
Surface tester – surface friction and roughnessThe evaluation can include measurement of the transient heat transfer properties associated with the sensation of coolness generated when fabrics contact the skin during wear. The KES not only predicts human response but understands the perception of softness."
Trusted Computer System Evaluation Criteria,"Trusted Computer System Evaluation Criteria (TCSEC) is a United States Government Department of Defense (DoD) standard that sets basic requirements for assessing the effectiveness of computer security controls built into a computer system. The TCSEC was used to evaluate, classify, and select computer systems being considered for the processing, storage, and retrieval of sensitive or classified information.The TCSEC, frequently referred to as the Orange Book, is the centerpiece of the DoD Rainbow Series publications. Initially issued in 1983 by the National Computer Security Center (NCSC), an arm of the National Security Agency, and then updated in 1985, TCSEC was eventually replaced by the Common Criteria international standard, originally published in 2005."
Evaluation,"Evolution is change in the heritable characteristics of biological populations over successive generations. These characteristics are the expressions of genes, which are passed on from parent to offspring during reproduction. Variation tends to exist within any given population as a result of genetic mutation and recombination. Evolution occurs when evolutionary processes such as natural selection (including sexual selection) and genetic drift act on this variation, resulting in certain characteristics becoming more common or more rare within a population. The evolutionary pressures that determine whether a characteristic is common or rare within a population constantly change, resulting in a change in heritable characteristics arising over successive generations. It is this process of evolution that has given rise to biodiversity at every level of biological organisation, including the levels of species, individual organisms, and molecules.The theory of evolution by natural selection was conceived independently by Charles Darwin and Alfred Russel Wallace in the mid-19th century and was set out in detail in Darwin's book On the Origin of Species. Evolution by natural selection is established by observable facts about living organisms: (1) more offspring are often produced than can possibly survive (2) traits vary among individuals with respect to their morphology, physiology, and behaviour (phenotypic variation); (3) different traits confer different rates of survival and reproduction (differential fitness); and (4) traits can be passed from generation to generation (heritability of fitness). In successive generations, members of a population are therefore more likely to be replaced by the offspring of parents with favourable characteristics. In the early 20th century, other competing ideas of evolution such as mutationism and orthogenesis were refuted as the modern synthesis concluded Darwinian evolution acts on Mendelian genetic variation.All life on Earth shares a last universal common ancestor (LUCA), which lived approximately 3.5–3.8 billion years ago. The fossil record includes a progression from early biogenic graphite to microbial mat fossils to fossilised multicellular organisms. Existing patterns of biodiversity have been shaped by repeated formations of new species (speciation), changes within species (anagenesis), and loss of species (extinction) throughout the evolutionary history of life on Earth. Morphological and biochemical traits are more similar among species that share a more recent common ancestor, and these traits can be used to reconstruct phylogenetic trees.Evolutionary biologists have continued to study various aspects of evolution by forming and testing hypotheses as well as constructing theories based on evidence from the field or laboratory and on data generated by the methods of mathematical and theoretical biology. Their discoveries have influenced not just the development of biology but numerous other scientific and industrial fields, including agriculture, medicine, and computer science."
Recommender system,"A recommender system, or a recommendation system (sometimes replacing 'system' with a synonym such as platform or engine), is a subclass of information filtering system that provide suggestions for items that are most pertinent to a particular user. Typically, the suggestions refer to various decision-making processes, such as what product to purchase, what music to listen to, or what online news to read. Recommender systems are particularly useful when an individual needs to choose an item from a potentially overwhelming number of items that a service may offer.Recommender systems are used in a variety of areas, with commonly recognised examples taking the form of playlist generators for video and music services, product recommenders for online stores, or content recommenders for social media platforms and open web content recommenders. These systems can operate using a single input, like music, or multiple inputs within and across platforms like news, books and search queries. There are also popular recommender systems for specific topics like restaurants and online dating. Recommender systems have also been developed to explore research articles and experts, collaborators, and financial services."
user utility,"Utility software is software designed to help analyze, configure, optimize or maintain a computer. It is used to support the computer infrastructure - in contrast to application software, which is aimed at directly performing tasks that benefit ordinary users. However, utilities often form part of the application systems. For example, a batch job may run user-written code to update a database and may then include a step that runs a utility to back up the database, or a job may run a utility to compress a disk before copying files.
Although a basic set of utility programs is usually distributed with an operating system (OS), and this first party utility software is often considered part of the operating system, users often install replacements or additional utilities. Those utilities may provide additional facilities to carry out tasks that are beyond the capabilities of the operating system.
Many utilities that might affect the entire computer system require the user to have elevated privileges, while others that operate only on the user's data do not.

"
Utility software,"Utility software is software designed to help analyze, configure, optimize or maintain a computer. It is used to support the computer infrastructure - in contrast to application software, which is aimed at directly performing tasks that benefit ordinary users. However, utilities often form part of the application systems. For example, a batch job may run user-written code to update a database and may then include a step that runs a utility to back up the database, or a job may run a utility to compress a disk before copying files.
Although a basic set of utility programs is usually distributed with an operating system (OS), and this first party utility software is often considered part of the operating system, users often install replacements or additional utilities. Those utilities may provide additional facilities to carry out tasks that are beyond the capabilities of the operating system.
Many utilities that might affect the entire computer system require the user to have elevated privileges, while others that operate only on the user's data do not.

"
User experience,"The user experience (UX) is how a user interacts with and experiences a product, system or service. It includes a person's perceptions of  utility, ease of use, and efficiency. Improving user experience is important to most companies, designers, and creators when creating and refining products because negative user experience can diminish the use of the product and, therefore, any desired positive impacts; conversely, designing toward profitability often conflicts with ethical user experience objectives and even causes harm. User experience is subjective. However, the attributes that make up the user experience are objective."
Disk Utility,Disk Utility is a system utility for performing disk and disk volume-related tasks on the macOS operating system by Apple Inc.
List of macOS built-in apps,This is a list of macOS built-in apps and system components.
documents,"A document is a written, drawn, presented, or memorialized representation of thought, often the manifestation of non-fictional, as well as fictional, content.  The word originates from the Latin Documentum, which denotes a ""teaching"" or ""lesson"": the verb doceō denotes ""to teach"". In the past, the word was usually used to denote written proof useful as evidence of a truth or fact. In the computer age, ""document"" usually denotes a primarily textual computer file, including its structure and format, e.g. fonts, colors, and images. Contemporarily, ""document"" is not defined by its transmission medium, e.g., paper, given the existence of electronic documents. ""Documentation"" is distinct because it has more denotations than ""document"". Documents are also distinguished from ""realia"", which are three-dimensional objects that would otherwise satisfy the definition of ""document"" because they memorialize or represent thought; documents are considered more as 2-dimensional representations. While documents can have large varieties of customization, all documents can be shared freely and have the right to do so, creativity can be represented by documents, also. History, events, examples, opinions, etc. all can be expressed in documents."
Document,"A document is a written, drawn, presented, or memorialized representation of thought, often the manifestation of non-fictional, as well as fictional, content.  The word originates from the Latin Documentum, which denotes a ""teaching"" or ""lesson"": the verb doceō denotes ""to teach"". In the past, the word was usually used to denote written proof useful as evidence of a truth or fact. In the computer age, ""document"" usually denotes a primarily textual computer file, including its structure and format, e.g. fonts, colors, and images. Contemporarily, ""document"" is not defined by its transmission medium, e.g., paper, given the existence of electronic documents. ""Documentation"" is distinct because it has more denotations than ""document"". Documents are also distinguished from ""realia"", which are three-dimensional objects that would otherwise satisfy the definition of ""document"" because they memorialize or represent thought; documents are considered more as 2-dimensional representations. While documents can have large varieties of customization, all documents can be shared freely and have the right to do so, creativity can be represented by documents, also. History, events, examples, opinions, etc. all can be expressed in documents."
False document,A false document is a technique by which an author aims to increase verisimilitude in a work of fiction by inventing and inserting or mentioning documents that appear to be factual. The goal of a false document is to convince an audience that what is being presented is factual.
Book of Documents,"The Book of Documents (Shūjīng, earlier Shu King) or Classic of History, also known as the Shangshu (“Venerated Documents”), is one of the Five Classics of ancient Chinese literature.  It is a collection of rhetorical prose attributed to figures of ancient China, and served as the foundation of Chinese political philosophy for over 2,000 years.
The Book of Documents was the subject of one of China's oldest literary controversies, between proponents of different versions of the text. A version was preserved from Qin Shi Huang's burning of books and burying of scholars by scholar Fu Sheng, in 29 sections (pian 篇). This group of texts were referred to as ""Modern Script"" jinwen 今文, because written with the script in use at the beginning of the Western Han dyansty. According to Western Han dynasty documents, new textual material was  discovered in the wall of Confucius' family estate in Qufu by his descendant Kong Anguo in the late 2nd century BC. This new material was referred to as ""Old Script"" guwen 古文, because written in the script that predated the standardization of Chinese script enforced during the Qing dynasty. Compared to the ""Modern Script"" texts, the ""Old Script"" material had 16 more texts. However, this seems to have been lost at the end of the Eastern Han dynasty, while the ""Modern Script"" text enjoyed circulation, in particular in Ouyang Gao’s 歐陽高 (a. 136 BCE) study of it, the Ouyang Shangshu 歐陽尚書. This was the basis of studies by Ma Rong and Zheng Xuan in the Easter Han Dynasty. By the end of the second century CE, there was knowledge that the Shangshu at some point included more than the ""Modern Script"" text. This likely prompted scholars to recreate the ""Old Script"" texts said to have once belonged to the Shangshu, a process that culminated with the presentation of a 58 section (59 if the preface is included in the count) Shangshu to the Eastern Jin court, in 317 CE, by Mei Ze 梅頤.
This version was accepted, among doubts, and later was canonized as part of Kong Yingda's project. It was only in the 17th century that Qing dynasty scholar Yan Ruoqu demonstrated that the ""Old Script"" were actually fabrications ""reconstructed"" in the 3rd or 4th centuries AD.
In the transmitted edition, texts are grouped into four sections representing different eras: the semi-mythical reign of Yu the Great, and the Xia, Shang and Zhou dynasties.  The Zhou section accounts for over half the text.  Some of its New Text chapters are among the earliest examples of Chinese prose, recording speeches from the early years of the Zhou dynasty in the late 11th century BC.  Although the other three sections purport to record earlier material, most scholars believe that even the New Script chapters in these sections were composed later than those in the Zhou section, with chapters relating to the earliest periods being as recent as the 4th or 3rd centuries BC."
Halloween documents,"The Halloween documents comprise a series of confidential Microsoft memoranda on potential strategies relating to free software, open-source software, and to Linux in particular, and a series of media responses to these memoranda. Both the leaked documents and the responses were published by Eric S. Raymond in 1998.The documents are associated with Halloween because many of them were originally leaked close to October 31 in different years.

"
Electronic document,"An electronic document is any electronic media content (other than computer programs or system files) that is intended to be used in either an electronic form or as printed output. Originally, any computer data were considered as something internal — the final data output was always on paper. However, the development of computer networks has made it so that in most cases it is much more convenient to distribute electronic documents than printed ones. The improvements in electronic visual display technologies made it possible to view documents on screen instead of printing them (thus saving paper and the space required to store the printed copies).However, using electronic documents for final presentation instead of paper has created the problem of multiple incompatible file formats. Even plain text computer files are not free from this problem — e.g. under MS-DOS, most programs could not work correctly with UNIX-style text files (see newline), and for non-English speakers, the different code pages always have been a source of trouble.
Even more problems are connected with complex file formats of various word processors, spreadsheets, and graphics software. To alleviate the problem, many software companies distribute free file viewers for their proprietary file formats (one example is Adobe's Acrobat Reader). The other solution is the development of standardized non-proprietary file formats (such as HTML and OpenDocument), and electronic documents for specialized uses have specialized formats – the specialized electronic articles in physics use TeX or PostScript."
Historical document,"Historical documents are original documents that contain important historical information about a person, place, or event and can thus serve as primary sources as important ingredients of the historical methodology.
Significant historical documents can be deeds, laws, accounts of battles (often given by the victors or persons sharing their viewpoint), or the exploits of the powerful. Though these documents are of historical interest, they do not detail the daily lives of ordinary people, or the way society functioned. Anthropologists, historians and archeologists generally are more interested in documents that describe the day-to-day lives of ordinary people, indicating what they ate, their interaction with other members of their households and social groups, and their states of mind. It is this information that allows them to try to understand and describe the way society was functioning at any particular time in history. Greek ostraka provide good examples of historical documents from ""among the common people"".
Many documents that are produced today, such as personal letters, pictures, contracts, newspapers, and medical records, would be considered valuable historical documents in the future. However most of these will be lost in the future since they are either printed on ordinary paper which has a limited lifespan, or even stored in digital formats, then lost track over time.
Some companies and government entities are attempting to increase the number of documents that will survive the passage of time, by taking into account the preservation issues, and either printing documents in a manner that would increase the likelihood of them surviving indefinitely, or placing selected documents in time capsules or other special storage environments."
Document management system,"A document management system (DMS) is usually a computerized system used to store, share, track and manage files or documents. Some systems include history tracking where a log of the various versions created and modified by different users is recorded. The term has some overlap with the concepts of content management systems. It is often viewed as a component of enterprise content management (ECM) systems and related to digital asset management, document imaging, workflow systems and records management systems."
Constitution,"Institutions are humanly devised structures of rules and norms that shape and constrain individual behavior. All definitions of institutions generally entail that there is a level of persistence and continuity. Laws, rules, social conventions and norms are all examples of institutions. Institutions vary in their level of formality and informality.Institutions are a principal object of study in social sciences such as political science, anthropology, economics, and sociology (the latter described by Émile Durkheim as the ""science of institutions, their genesis and their functioning""). Primary or meta-institutions are institutions such as the family or money that are broad enough to encompass sets of related institutions. Institutions are also a central concern for law, the formal mechanism for political rule-making and enforcement. Historians study and document the founding, growth, decay and development of institutions as part of political, economic and cultural history."
Identity document,
Google Docs,"Google Docs is an online word processor included as part of the free, web-based Google Docs Editors suite offered by Google, which also includes: Google Sheets, Google Slides, Google Drawings, Google Forms, Google Sites and Google Keep. Google Docs is accessible via an internet browser as a web-based application and is also available as a mobile app on Android and iOS and as a desktop application on Google's ChromeOS.
Google Docs allows users to create and edit documents online while collaborating with other users in real time. Edits are tracked by the user making the edit, with a revision history presenting changes. An editor's position is highlighted with an editor-specific color and cursor, and a permissions system regulates what users can do. Updates have introduced features using machine learning, including ""Explore"", offering search results based on the contents of a document, and ""Action items"", allowing users to assign tasks to other users.Google Docs supports opening and saving documents in the standard OpenDocument format as well as in Rich text format, plain Unicode text, zipped HTML, and Microsoft Word. Exporting to PDF and EPUB formats are implemented."
synonymy,"A synonym is a word, morpheme, or phrase that means exactly or nearly the same as another word, morpheme, or phrase in a given language. For example, in the English language, the words begin, start, commence, and initiate are all synonyms of one another: they are synonymous. The standard test for synonymy is substitution: one form can be replaced by another in a sentence without changing its meaning. Words are considered synonymous in only one particular sense: for example, long and extended in the context long time or extended time are synonymous, but long cannot be used in the phrase extended family. Synonyms with exactly the same meaning share a seme or denotational sememe, whereas those with inexactly similar meanings share a broader denotational or connotational sememe and thus overlap within a semantic field. The former are sometimes called cognitive synonyms and the latter, near-synonyms, plesionyms or poecilonyms."
Synonym,"A synonym is a word, morpheme, or phrase that means exactly or nearly the same as another word, morpheme, or phrase in a given language. For example, in the English language, the words begin, start, commence, and initiate are all synonyms of one another: they are synonymous. The standard test for synonymy is substitution: one form can be replaced by another in a sentence without changing its meaning. Words are considered synonymous in only one particular sense: for example, long and extended in the context long time or extended time are synonymous, but long cannot be used in the phrase extended family. Synonyms with exactly the same meaning share a seme or denotational sememe, whereas those with inexactly similar meanings share a broader denotational or connotational sememe and thus overlap within a semantic field. The former are sometimes called cognitive synonyms and the latter, near-synonyms, plesionyms or poecilonyms."
Synonym (taxonomy),"The Botanical and Zoological Codes of nomenclature treat the concept of synonymy differently. 

In botanical nomenclature, a synonym is a scientific name that applies to a taxon that (now) goes by a different scientific name. For example, Linnaeus was the first to give a scientific name (under the currently used system of scientific nomenclature) to the Norway spruce, which he called Pinus abies. This name is no longer in use, so it is now a synonym of the current scientific name, Picea abies.
In zoology, moving a species from one genus to another results in a different binomen, but the name is considered an alternative combination rather than a synonym. The concept of synonymy in zoology is reserved for two names at the same rank that refers to a taxon at that rank - for example, the name Papilio prorsa Linnaeus, 1758 is a junior synonym of Papilio levana Linnaeus, 1758, being names for different seasonal forms of the species now referred to as Araschnia levana (Linnaeus, 1758), the map butterfly. However, Araschnia levana is not a synonym of Papilio levana in the taxonomic sense employed by the Zoological code.Unlike synonyms in other contexts, in taxonomy a synonym is not interchangeable with the name of which it is a synonym. In taxonomy, synonyms are not equals, but have a different status. For any taxon with a particular circumscription, position, and rank, only one scientific name is considered to be the correct one at any given time (this correct name is to be determined by applying the relevant code of nomenclature). A synonym cannot exist in isolation: it is always an alternative to a different scientific name. Given that the correct name of a taxon depends on the taxonomic viewpoint used (resulting in a particular circumscription, position and rank) a name that is one taxonomist's synonym may be another taxonomist's correct name (and vice versa).
Synonyms may arise whenever the same taxon is described and named more than once, independently. They may also arise when existing taxa are changed, as when two taxa are joined to become one, a species is moved to a different genus, a variety is moved to a different species, etc. Synonyms also come about when the codes of nomenclature change, so that older names are no longer acceptable; for example, Erica herbacea L. has been rejected in favour of Erica carnea L. and is thus its synonym."
Cognitive synonymy,"Cognitive synonymy is a type of synonymy in which synonyms are so similar in meaning that they cannot be differentiated either denotatively or connotatively, that is, not even by mental associations, connotations, emotive responses, and poetic value. It is a stricter (more precise) technical definition of synonymy, specifically for theoretical (e.g., linguistic and philosophical) purposes. In usage employing this definition, synonyms with greater differences are often called near-synonyms rather than synonyms (compare also plesionyms)."
Synonymy in Japanese,"There are many synonyms in Japanese because the Japanese language draws from several different languages for loanwords, notably Chinese and English, as well as its own native words.  In Japanese, synonyms are called dōgigo (kanji: 同義語) or ruigigo (kanji: 類義語).Full synonymy, however, is rare. In general, native Japanese words may have broader meanings than those that are borrowed, Sino-Japanese words tend to suggest a more formal tone, while Western borrowed words more modern."
pseudo relevance feedback,"Relevance feedback is a feature of some information retrieval systems.  The idea behind relevance feedback is to take the results that are initially returned from a given query, to gather user feedback, and to use information about whether or not those results are relevant to perform a new query.  We can usefully distinguish between three types of feedback: explicit feedback, implicit feedback, and blind or ""pseudo"" feedback."
Relevance feedback,"Relevance feedback is a feature of some information retrieval systems.  The idea behind relevance feedback is to take the results that are initially returned from a given query, to gather user feedback, and to use information about whether or not those results are relevant to perform a new query.  We can usefully distinguish between three types of feedback: explicit feedback, implicit feedback, and blind or ""pseudo"" feedback."
Query expansion,"Query expansion (QE) is the process of reformulating a given query to improve retrieval performance in information retrieval operations, particularly in the context of query understanding.
In the context of search engines, query expansion involves evaluating a user's input (what words were typed into the search query area, and sometimes other types of data) and expanding the search query to match additional documents.  Query expansion involves techniques such as:

Finding synonyms of words, and searching for the synonyms as well
Finding semantically related words (e.g. antonyms, meronyms, hyponyms, hypernyms)
Finding all the various morphological forms of words by stemming each word in the search query
Fixing spelling errors and automatically searching for the corrected form or suggesting it in the results
Re-weighting the terms in the original queryQuery expansion is a methodology studied in the field of computer science, particularly within the realm of natural language processing and information retrieval."
Additive smoothing,"In statistics, additive smoothing, also called Laplace smoothing or Lidstone smoothing, is a technique used to smooth categorical data. Given a set of observation counts 
  
    
      
        
          
            
              x
            
             
            =
             
            
              ⟨
              
                
                  x
                  
                    1
                  
                
                ,
                
                
                  x
                  
                    2
                  
                
                ,
                
                …
                ,
                
                
                  x
                  
                    d
                  
                
              
              ⟩
            
          
        
      
    
    {\textstyle \textstyle {\mathbf {x} \ =\ \left\langle x_{1},\,x_{2},\,\ldots ,\,x_{d}\right\rangle }}
   from a 
  
    
      
        
          
            d
          
        
      
    
    {\textstyle \textstyle {d}}
  -dimensional multinomial distribution with 
  
    
      
        
          
            N
          
        
      
    
    {\textstyle \textstyle {N}}
   trials, a ""smoothed"" version of the counts gives the estimator:

  
    
      
        
          
            
              
                θ
                ^
              
            
          
          
            i
          
        
        =
        
          
            
              
                x
                
                  i
                
              
              +
              α
            
            
              N
              +
              α
              d
            
          
        
        
        (
        i
        =
        1
        ,
        …
        ,
        d
        )
        ,
      
    
    {\displaystyle {\hat {\theta }}_{i}={\frac {x_{i}+\alpha }{N+\alpha d}}\qquad (i=1,\ldots ,d),}
  where the smoothed count 
  
    
      
        
          
            
              
                
                  
                    x
                    ^
                  
                
              
              
                i
              
            
            =
            N
            
              
                
                  
                    θ
                    ^
                  
                
              
              
                i
              
            
          
        
      
    
    {\textstyle \textstyle {{\hat {x}}_{i}=N{\hat {\theta }}_{i}}}
   and the ""pseudocount""  α > 0 is a smoothing parameter. α = 0 corresponds to no smoothing. (This parameter is explained in § Pseudocount below.) Additive smoothing is a type of shrinkage estimator, as the resulting estimate will be between the empirical probability (relative frequency) 
  
    
      
        
          
            
              x
              
                i
              
            
            
              /
            
            N
          
        
      
    
    {\textstyle \textstyle {x_{i}/N}}
  , and the uniform probability 
  
    
      
        
          
            1
            
              /
            
            d
          
        
      
    
    {\textstyle \textstyle {1/d}}
  . Invoking Laplace's rule of succession, some authors have argued that α should be 1 (in which case the term add-one smoothing is also used), though in practice a smaller value is typically chosen.
From a Bayesian point of view, this corresponds to the expected value of the posterior distribution, using a symmetric Dirichlet distribution with parameter α as a prior distribution. In the special case where the number of categories is 2, this is equivalent to using a Beta distribution as the conjugate prior for the parameters of Binomial distribution."
Implicit stereotype,"In social identity theory, an implicit bias or implicit stereotype, is the pre-reflective attribution of particular qualities by an individual to a member of some social out group.Implicit stereotypes are thought to be shaped by experience and based on learned associations between particular qualities and social categories, including race and/or gender. Individuals' perceptions and behaviors can be influenced by the implicit stereotypes they hold, even if they are sometimes unaware they hold such stereotypes. Implicit bias is an aspect of implicit social cognition: the phenomenon that perceptions, attitudes, and stereotypes can operate prior to conscious intention or endorsement. The existence of implicit bias is supported by a variety of scientific articles in psychological literature. Implicit stereotype was first defined by psychologists Mahzarin Banaji and Anthony Greenwald in 1995.
Explicit stereotypes, by contrast, are consciously endorsed, intentional, and sometimes controllable thoughts and beliefs.Implicit biases, however, are thought to be the product of associations learned through past experiences. Implicit biases can be activated by the environment and operate prior to a person's intentional, conscious endorsement. Implicit bias can persist even when an individual rejects the bias explicitly.

"
Hyperreality,"Described by Jean Baudrillard, the concept of hyperreality captures the inability to distinguish ""The Real"" (a term borrowed from Jacques Lacan) from the signifier of it. This is more prominent in technologically advanced societies. Hyperreality is seen as a condition in which what is real and what is fiction are seamlessly blended together so that there is no clear distinction between where one ends and the other begins. It allows the merging of physical reality with virtual reality (VR) or augmented reality (AR), and human intelligence with artificial intelligence (AI).Jean Baudrillard is a French cultural theorist, sociologist and philosopher. His most notable work consists of establishing the concept of hyperreality and the simulacra. Some of Baudrillard's most influential theorists consist of Karl Marx, Freud, Levi Strauss, Nietzsche, etc. Baudrillard's work stems around his interest in the theories of post-structuralism and post-modernism. Some famous theorists who contributed to the field of hyperreality/hyperrealism include Jean Baudrillard, Albert Borgmann, Daniel J. Boorstin, Neil Postman and Umberto Eco.
The study of hyperreality and the effects it has on the consumer falls under the study of  semiotics and postmodernism studies.  As the study of semiotics advances, codes are used to categorize a map of meanings. These codes are systems of ideas that people use to interpret behaviours and messages they receive. Cultural codes are specific sets of knowledge that provides reference points in the process of interpretation of signs. Thus codes connect semiotics systems of meaning with social values and structure.
Postmodernism is a scholarly tradition in the field of communication studies that speaks directly to larger social concerns. Postmodernism was established through the social turmoil of the 1960s, spurred by social movements that questioned pre-existing conventions and social institutions. Through the postmodern lens reality is viewed as fragmented, locally produced and polysemic. Social realities are constantly produced and reproduced, ever changing through the use of language and symbolic forms. Systems, signs, objects and symbols are viewed to have multiple meanings."
List of Latin phrases (full),"This is a list of Wikipedia articles of Latin phrases and their translation into English.

To view all phrases on a single, lengthy document, see: List of Latin phrases (full)
The list also is divided alphabetically into twenty pages:

"
TRACE (psycholinguistics),"TRACE is a connectionist model of speech perception, proposed by James McClelland and Jeffrey Elman in 1986. It is based on a structure called ""the Trace,"" a dynamic processing structure made up of a network of units, which performs as the system's working memory as well as the perceptual processing mechanism. TRACE was made into a working computer program for running perceptual simulations.  These simulations are predictions about how a human mind/brain processes speech sounds and words as they are heard in real time."
Microelectrode array,"Microelectrode arrays (MEAs) (also referred to as multielectrode arrays) are devices that contain multiple (tens to thousands) microelectrodes through which neural signals are obtained or delivered, essentially serving as neural interfaces that connect neurons to electronic circuitry.  There are two general classes of MEAs: implantable MEAs, used in vivo, and non-implantable MEAs, used in vitro."
System analysis,"Systems analysis is ""the process of studying a procedure or business to identify its goal and purposes and create systems and procedures that will efficiently achieve them"". Another view sees system analysis as a problem-solving technique that breaks down a system into its component pieces, and how well those parts work and interact to accomplish their purpose.The field of system analysis relates closely to requirements analysis or to operations research. It is also ""an explicit formal inquiry carried out to help a decision maker identify a better course of action and make a better decision than they might otherwise have made.""The terms analysis and synthesis stems from Greek, meaning ""to take apart"" and ""to put together,"" respectively. These terms are used in many scientific disciplines, from mathematics and logic to economics and psychology, to denote similar investigative procedures. The analysis is defined as ""the procedure by which we break down an intellectual or substantial whole into parts,"" while synthesis means ""the procedure by which we combine separate elements or components to form a coherent whole."" System analysis researchers apply methodology to the systems involved, forming an overall picture.
System analysis is used in every field where something is developed. Analysis can also be a series of components that perform organic functions together, such as system engineering. System engineering is an interdisciplinary field of engineering that focuses on how complex engineering projects should be designed and managed.

"
Scrying,"Scrying, also known by various names such as ""seeing"" or ""peeping"", is the practice of looking into a suitable medium in the hope of detecting significant messages or visions. The objective might be personal guidance, prophecy, revelation, or inspiration, but down the ages, scrying in various forms also has been a means of divination or fortune-telling.  It remains popular in occult circles, discussed in many media, both modern and centuries old."
relational databases,"A relational database is a (most commonly digital) database  based on the relational model of data, as proposed by E. F. Codd in 1970. A system used to maintain relational databases is a relational database management system (RDBMS). Many relational database systems are equipped with the option of using the SQL (Structured Query Language) for querying and maintaining the database."
Relational database,"A relational database is a (most commonly digital) database  based on the relational model of data, as proposed by E. F. Codd in 1970. A system used to maintain relational databases is a relational database management system (RDBMS). Many relational database systems are equipped with the option of using the SQL (Structured Query Language) for querying and maintaining the database."
NoSQL,"A NoSQL (originally referring to ""non-SQL"" or ""non-relational"") database provides a mechanism for storage and retrieval of data that is modeled in means other than the tabular relations used in relational databases. Such databases have existed since the late 1960s, but the name ""NoSQL"" was only coined in the early 21st century, triggered by the needs of Web 2.0 companies. NoSQL databases are increasingly used in big data and real-time web applications. NoSQL systems are also sometimes called Not only SQL to emphasize that they may support SQL-like query languages or sit alongside SQL databases in polyglot-persistent architectures.Motivations for this approach include simplicity of design, simpler ""horizontal"" scaling to clusters of machines (which is a problem for relational databases), finer control over availability and limiting the object-relational impedance mismatch. The data structures used by NoSQL databases (e.g. key–value pair, wide column, graph, or document) are different from those used by default in relational databases, making some operations faster in NoSQL. The particular suitability of a given NoSQL database depends on the problem it must solve.  Sometimes the data structures used by NoSQL databases are also viewed as ""more flexible"" than relational database tables.Many NoSQL stores compromise consistency (in the sense of the CAP theorem) in favor of availability, partition tolerance, and speed. Barriers to the greater adoption of NoSQL stores include the use of low-level query languages (instead of SQL, for instance), lack of ability to perform ad hoc joins across tables, lack of standardized interfaces, and huge previous investments in existing relational databases. Most NoSQL stores lack true ACID transactions, although a few databases have made them central to their designs.
Instead, most NoSQL databases offer a concept of ""eventual consistency"", in which database changes are propagated to all nodes ""eventually"" (typically within milliseconds), so queries for data might not return updated data immediately or might result in reading data that is not accurate, a problem known as stale reads. Additionally, some NoSQL systems may exhibit lost writes and other forms of data loss. Some NoSQL systems provide concepts such as write-ahead logging to avoid data loss. For distributed transaction processing across multiple databases, data consistency is an even bigger challenge that is difficult for both NoSQL and relational databases. Relational databases ""do not allow referential integrity constraints to span databases"". Few systems maintain both ACID transactions and X/Open XA standards for distributed transaction processing. Interactive relational databases share conformational relay analysis techniques as a common feature. Limitations within the interface environment are overcome using semantic virtualization protocols, such that NoSQL services are accessible to most operating systems."
Relational model,"The relational model (RM) is an approach to managing data using a structure and language consistent with first-order predicate logic, first described in 1969 by English computer scientist Edgar F. Codd, where all data is represented in terms of tuples, grouped into relations. A database organized in terms of the relational model is a relational database.
The purpose of the relational model is to provide a declarative method for specifying data and queries: users directly state what information the database contains and what information they want from it, and let the database management system software take care of describing data structures for storing the data and retrieval procedures for answering queries.
Most relational databases use the SQL data definition and query language; these systems implement what can be regarded as an engineering approximation to the relational model. A table in a SQL database schema corresponds to a predicate variable; the contents of a table to a relation; key constraints, other constraints, and SQL queries correspond to predicates. However, SQL databases deviate from the relational model in many details, and Codd fiercely argued against deviations that compromise the original principles."
Object–relational database,"An object–relational database (ORD), or object–relational database management system (ORDBMS), is a database management system (DBMS) similar to a relational database, but with an object-oriented database model: objects, classes and inheritance are directly supported in database schemas and in the query language.  In addition, just as with pure relational systems, it supports extension of the data model with custom data types and methods.

An object–relational database can be said to provide a middle ground between relational databases and object-oriented databases. In object–relational databases, the approach is essentially that of relational databases: the data resides in the database and is manipulated collectively with queries in a query language; at the other extreme are OODBMSes in which the database is essentially a persistent object store for software written in an object-oriented programming language, with a programming API for storing and retrieving objects, and little or no specific support for querying."
Database normalization,"Database normalization or database normalisation (see spelling differences) is the process of structuring a relational database in accordance with a series of so-called normal forms in order to reduce data redundancy and improve data integrity. It was first proposed by British computer scientist Edgar F. Codd as part of his relational model.
Normalization entails organizing the columns (attributes) and tables (relations) of a database to ensure that their dependencies are properly enforced by database integrity constraints. It is accomplished by applying some formal rules either by a process of synthesis (creating a new database design) or decomposition (improving an existing database design).

"
Amazon Relational Database Service,"Amazon Relational Database Service (or Amazon RDS) is a distributed relational database service by Amazon Web Services (AWS). It is a web service running ""in the cloud"" designed to simplify the setup, operation, and scaling of a relational database for use in applications. Administration processes like patching the database software, backing up databases and enabling point-in-time recovery are managed automatically. Scaling storage and compute resources can be performed by a single API call to the AWS control plane on-demand. AWS does not offer an SSH connection to the underlying virtual machine as part of the managed service."
Database,"In computing, a database is an organized collection of data stored and accessed electronically. Small databases can be stored on a file system, while large databases are hosted on computer clusters or cloud storage. The design of databases spans formal techniques and practical considerations, including data modeling, efficient data representation and storage, query languages, security and privacy of sensitive data, and distributed computing issues, including supporting concurrent access and fault tolerance.
A database management system (DBMS) is the software that interacts with end users, applications, and the database itself to capture and analyze the data. The DBMS software additionally encompasses the core facilities provided to administer the database. The sum total of the database, the DBMS and the associated applications can be referred to as a database system. Often the term ""database"" is also used loosely to refer to any of the DBMS, the database system or an application associated with the database.
Computer scientists may classify database management systems according to the database models that they support. Relational databases became dominant in the 1980s. These model data as rows and columns in a series of tables, and the vast majority use SQL for writing and querying data. In the 2000s, non-relational databases became popular, collectively referred to as NoSQL, because they use different query languages.

"
Object–relational mapping,"Object–relational mapping (ORM, O/RM, and O/R mapping tool) in computer science is a programming technique for converting data between type systems using object-oriented programming languages. This creates, in effect, a ""virtual object database"" that can be used from within the programming language. There are both free and commercial packages available that perform object–relational mapping, although some programmers opt to construct their own ORM tools.
In object-oriented programming, data-management tasks act on objects that are almost always non-scalar values. For example, consider an address book entry that represents a single person along with zero or more phone numbers and zero or more addresses. This could be modeled in an object-oriented implementation by a ""Person object"" with an attribute/field to hold each data item that the entry comprises: the person's name, a list of phone numbers, and a list of addresses. The list of phone numbers would itself contain ""PhoneNumber objects"" and so on. Each such address-book entry is treated as a single object by the programming language (it can be referenced by a single variable containing a pointer to the object, for instance). Various methods can be associated with the object, such as methods to return the preferred phone number, the home address, and so on.
By contrast, many popular database products, such as SQL, are not object-oriented and can only store and manipulate scalar values such as integers and strings organized within tables. The programmer must either convert the object values into groups of simpler values for storage in the database (and convert them back upon retrieval), or only use simple scalar values within the program. Object–relational mapping implements the first approach.The heart of the problem involves translating the logical representation of the objects into an atomized form that is capable of being stored in the database while preserving the properties of the objects and their relationships so that they can be reloaded as objects when needed. If this storage and retrieval functionality is implemented, the objects are said to be persistent."
Object–relational impedance mismatch,"The object–relational impedance mismatch is a set of conceptual and technical difficulties that are often encountered when a relational database management system (RDBMS) is being served by an application program (or multiple application programs) written in an object-oriented programming language or style, particularly because objects or class definitions must be mapped to database tables defined by a relational schema.
The term object–relational impedance mismatch is derived from the electrical engineering term impedance matching."
List of relational database management systems,This is a list of relational database management systems.
unstructured text,"Unstructured data (or unstructured information) is information that either does not have a pre-defined data model or is not organized in a pre-defined manner. Unstructured information is typically text-heavy, but may contain data such as dates, numbers, and facts as well. This results in irregularities and ambiguities that make it difficult to understand using traditional programs as compared to data stored in fielded form in databases or annotated (semantically tagged) in documents.
In 1998, Merrill Lynch said ""unstructured data comprises the vast majority of data found in an organization, some estimates run as high as 80%."" It's unclear what the source of this number is, but nonetheless it is accepted by some. Other sources have reported similar or higher percentages of unstructured data.As of 2012, IDC and Dell EMC project that data will grow to 40 zettabytes by 2020, resulting in a 50-fold growth from the beginning of 2010. More recently, IDC and Seagate predict that the global datasphere will grow to 163 zettabytes by 2025  and majority of that will be unstructured. The Computer World magazine states that unstructured information might account for more than 70–80% of all data in organizations.[1]"
Unstructured data,"A data model  is an abstract model that organizes elements of data and standardizes how they relate to one another and to the properties of real-world entities. For instance, a data model may specify that the data element representing a car be composed of a number of other elements which, in turn, represent the color and size of the car and define its owner.
The term data model can refer to two distinct but closely related concepts. Sometimes it refers to an abstract formalization of the objects and relationships found in a particular application domain: for example the customers, products, and orders found in a manufacturing organization. At other times it refers to the set of concepts used in defining such formalizations: for example concepts such as entities, attributes, relations, or tables. So the ""data model"" of a banking application may be defined using the entity-relationship ""data model"". This article uses the term in both senses.

A data model explicitly determines the structure of data. Data models are typically specified by a data specialist, data librarian, or a digital humanities scholar in a data modeling notation. These notations are often represented in graphical form.A data model can sometimes be referred to as a data structure, especially in the context of programming languages. Data models are often complemented by function models, especially in the context of enterprise models."
Homeschooling,"Homeschooling or home schooling, also known as home education or elective home education (EHE), is the education of school-aged children at home or a variety of places other than a school. Usually conducted by a parent, tutor, or an online teacher, many homeschool families use less formal, more personalized and individualized methods of learning that are not always found in schools. The actual practice of homeschooling can vary. The spectrum ranges from highly structured forms based on traditional school lessons to more open, free forms such as unschooling, which is a lesson- and curriculum-free implementation of homeschooling. Some families who initially attended a school go through a deschool phase to break away from school habits and prepare for homeschooling. While ""homeschooling"" is the term commonly used in North America, ""home education"" is primarily used in Europe and many Commonwealth countries. Homeschooling should not be confused with distance education, which generally refers to the arrangement where the student is educated by and conforms to the requirements of an online school, rather than being educated independently and unrestrictedly by their parents or by themselves.
Before the introduction of compulsory school attendance laws, most childhood education was done by families and local communities. By the early 19th century, attending a school became the most common means of education in the developed world. In the mid to late 20th century, more people began questioning the efficiency and sustainability of school learning, which again led to an increase in the number of homeschoolers, especially in the Americas and some European countries. Today, homeschooling is a relatively widespread form of education and a legal alternative to public and private schools in many countries, which many people believe is due to the rise of the Internet, which enables people to obtain information very quickly. There are also nations in which homeschooling is regulated or illegal, as recorded in the article Homeschooling international status and statistics. During the COVID-19 pandemic, many students from all over the world had to study from home due to the danger posed by the virus. However, this was mostly implemented in the form of distance education rather than traditional homeschooling.
There are many different reasons for homeschooling, ranging from personal interests to dissatisfaction with the public school system. Some parents see better educational opportunities for their child in homeschooling, for example because they know their child more accurately than a teacher and can concentrate fully on educating usually one to a few persons and therefore can respond more precisely to their individual strengths and weaknesses, or because they think that they can better prepare their children for the life outside of school. Some children can also learn better at home, for example, because they are not held back, disturbed or distracted from school matters, do not feel underchallenged or overwhelmed with certain topics, find that certain temperaments are encouraged in school, while others are inhibited, do not cope well with the often predetermined structure or are bullied there. Homeschooling is also an option for families living in remote rural areas, those temporarily abroad and those who travel frequently and therefore face the physical impossibility or difficulty of getting their children into school and families who want to spend more and better time with their children. Health reasons and special needs can also play a role in why children cannot attend a school regularly and are at least partially homeschooled.
Critics of homeschooling argue that children may lack adequate socialization and therefore have poorer social skills. Some are also concerned that parents may be unqualified to guide and advise their children in life skills. Critics also say that a child might not encounter people of other cultures, worldviews, and socioeconomic groups if they are not enrolled in a school. Therefore, these critics believe that homeschooling cannot guarantee a comprehensive and neutral education if educational standards are not prescribed. Homeschooled children sometimes score higher on standardized tests and their parents reported via survey that their children have equally or better developed social skills and participate more in cultural and family activities on average than public school students. In addition, studies suggest that homeschoolers are generally more likely to have higher self-esteem, deeper friendships, and better relationships with adults, and are less susceptible to peer pressure."
OpenText,"In semiotic analysis (the studies of signs or symbols), an open text is a text that allows multiple or mediated interpretation by the readers.  In contrast, a closed text leads the reader to one intended interpretation.
The concept of the open text comes from Umberto Eco's collection of essays The Role of the Reader, but it is also derivative of Roland Barthes's distinction between 'readerly' (lisible) and 'writerly' (scriptible) texts as set out in his 1968 essay, ""The Death of the Author""."
List of text mining software,Text mining computer programs are available from many commercial and open source companies and sources.
Named-entity recognition,"Named-entity recognition (NER) (also known as (named) entity identification, entity chunking, and entity extraction) is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc.
Most research on NER/NEE systems has been structured as taking an unannotated block of text, such as this one:

Jim bought 300 shares of Acme Corp. in 2006.
And producing an annotated block of text that highlights the names of entities:

[Jim]Person bought 300 shares of [Acme Corp.]Organization in [2006]Time.
In this example, a person name consisting of one token, a two-token company name and a temporal expression have been detected and classified.
State-of-the-art NER systems for English produce near-human performance. For example, the best system entering MUC-7 scored 93.39% of F-measure while human annotators scored 97.60% and 96.95%."
Text mining,"Text mining, also referred to as text data mining, similar to text analytics, is the process of deriving high-quality information from text. It involves ""the discovery by computer of new, previously unknown information, by automatically extracting information from different written resources."" Written resources may include websites, books, emails, reviews, and articles. High-quality information is typically obtained by devising patterns and trends by means such as statistical pattern learning. According to Hotho et al. (2005) we can distinguish between three different perspectives of text mining: information extraction, data mining, and a KDD (Knowledge Discovery in Databases) process. Text mining usually involves the process of structuring the input text (usually parsing, along with the addition of some derived linguistic features and the removal of others, and subsequent insertion into a database), deriving patterns within the structured data, and finally evaluation and interpretation of the output. 'High quality' in text mining usually refers to some combination of relevance, novelty, and interest. Typical text mining tasks include text categorization, text clustering, concept/entity extraction, production of granular taxonomies, sentiment analysis, document summarization, and entity relation modeling (i.e., learning relations between named entities).
Text analysis involves information retrieval, lexical analysis to study word frequency distributions, pattern recognition, tagging/annotation, information extraction, data mining techniques including link and association analysis, visualization, and predictive analytics. The overarching goal is, essentially, to turn text into data for analysis, via application of natural language processing (NLP), different types of algorithms and analytical methods. An important phase of this process is the interpretation of the gathered information.
A typical application is to scan a set of documents written in a natural language and either model the document set for predictive classification purposes or populate a database or search index with the information extracted.
The document is the basic element while starting with text mining. Here, we define a document as a unit of textual data, which normally exists in many types of collections."
Document retrieval,"Document retrieval is defined as the matching of some stated user query against a set of free-text records. These records could be any type of mainly unstructured text, such as newspaper articles, real estate records or paragraphs in a manual. User queries can range from multi-sentence full descriptions of an information need to a few words.
Document retrieval is sometimes referred to as, or as a branch of, text retrieval. Text retrieval is a branch of information retrieval where the information is stored primarily in the form of text. Text databases became decentralized thanks to the personal computer. Text retrieval is a critical area of study today, since it is the fundamental basis of all internet search engines."
Business intelligence,"Business intelligence (BI) comprises the strategies and technologies used by enterprises for the data analysis and management of business information. Common functions of business intelligence technologies include reporting, online analytical processing, analytics, dashboard development, data mining, process mining, complex event processing, business performance management, benchmarking, text mining, predictive analytics, and prescriptive analytics. 
BI tools can handle large amounts of structured and sometimes unstructured data to help identify, develop, and otherwise create new strategic business opportunities. They aim to allow for the easy interpretation of these big data. Identifying new opportunities and implementing an effective strategy based on insights can provide businesses with a competitive market advantage and long-term stability, and help them take strategic decisions.Business intelligence can be used by enterprises to support a wide range of business decisions ranging from operational to strategic. Basic operating decisions include product positioning or pricing. Strategic business decisions involve priorities, goals, and directions at the broadest level. In all cases, BI is most effective when it combines data derived from the market in which a company operates (external data) with data from company sources internal to the business such as financial and operations data (internal data). When combined, external and internal data can provide a complete picture which, in effect, creates an ""intelligence"" that cannot be derived from any singular set of data.Among myriad uses, business intelligence tools empower organizations to gain insight into new markets, to assess demand and suitability of products and services for different market segments, and to gauge the impact of marketing efforts.BI applications use data gathered from a data warehouse (DW) or from a data mart, and the concepts of BI and DW combine as ""BI/DW""
or as ""BIDW"". A data warehouse contains a copy of analytical data that facilitates decision support."
Concept search,"A concept search (or conceptual search) is an automated information retrieval method that is used to search electronically stored unstructured text (for example, digital archives, email, scientific literature, etc.) for information that is conceptually similar to the information provided in a search query. In other words, the ideas expressed in the information retrieved in response to a concept search query are relevant to the ideas contained in the text of the query."
Topic model,"In statistics and natural language processing, a topic model is a type of statistical model for discovering the abstract ""topics"" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body. Intuitively, given that a document is about a particular topic, one would expect particular words to appear in the document more or less frequently: ""dog"" and ""bone"" will appear more often in documents about dogs, ""cat"" and ""meow"" will appear in documents about cats, and ""the"" and ""is"" will appear approximately equally in both. A document typically concerns multiple topics in different proportions; thus, in a document that is 10% about cats and 90% about dogs, there would probably be about 9 times more dog words than cat words. The ""topics"" produced by topic modeling techniques are clusters of similar words. A topic model captures this intuition in a mathematical framework, which allows examining a set of documents and discovering, based on the statistics of the words in each, what the topics might be and what each document's balance of topics is.
Topic models are also referred to as probabilistic topic models, which refers to statistical algorithms for discovering the latent semantic structures of an extensive text body. In the age of information, the amount of the written material we encounter each day is simply beyond our processing capacity. Topic models can help to organize and offer insights for us to understand large collections of unstructured text bodies. Originally developed as a text-mining tool, topic models have been used to detect instructive structures in data such as genetic information, images, and networks. They also have applications in other fields such as bioinformatics and computer vision."
database systems,"In computing, a database is an organized collection of data stored and accessed electronically. Small databases can be stored on a file system, while large databases are hosted on computer clusters or cloud storage. The design of databases spans formal techniques and practical considerations, including data modeling, efficient data representation and storage, query languages, security and privacy of sensitive data, and distributed computing issues, including supporting concurrent access and fault tolerance.
A database management system (DBMS) is the software that interacts with end users, applications, and the database itself to capture and analyze the data. The DBMS software additionally encompasses the core facilities provided to administer the database. The sum total of the database, the DBMS and the associated applications can be referred to as a database system. Often the term ""database"" is also used loosely to refer to any of the DBMS, the database system or an application associated with the database.
Computer scientists may classify database management systems according to the database models that they support. Relational databases became dominant in the 1980s. These model data as rows and columns in a series of tables, and the vast majority use SQL for writing and querying data. In the 2000s, non-relational databases became popular, collectively referred to as NoSQL, because they use different query languages.

"
Database,"In computing, a database is an organized collection of data stored and accessed electronically. Small databases can be stored on a file system, while large databases are hosted on computer clusters or cloud storage. The design of databases spans formal techniques and practical considerations, including data modeling, efficient data representation and storage, query languages, security and privacy of sensitive data, and distributed computing issues, including supporting concurrent access and fault tolerance.
A database management system (DBMS) is the software that interacts with end users, applications, and the database itself to capture and analyze the data. The DBMS software additionally encompasses the core facilities provided to administer the database. The sum total of the database, the DBMS and the associated applications can be referred to as a database system. Often the term ""database"" is also used loosely to refer to any of the DBMS, the database system or an application associated with the database.
Computer scientists may classify database management systems according to the database models that they support. Relational databases became dominant in the 1980s. These model data as rows and columns in a series of tables, and the vast majority use SQL for writing and querying data. In the 2000s, non-relational databases became popular, collectively referred to as NoSQL, because they use different query languages.

"
Relational database,"A relational database is a (most commonly digital) database  based on the relational model of data, as proposed by E. F. Codd in 1970. A system used to maintain relational databases is a relational database management system (RDBMS). Many relational database systems are equipped with the option of using the SQL (Structured Query Language) for querying and maintaining the database."
Atomicity (database systems),"In database systems, atomicity (; from Ancient Greek: ἄτομος, romanized: átomos, lit. 'undividable') is one of the ACID (Atomicity, Consistency, Isolation, Durability) transaction properties. An atomic transaction is an indivisible and irreducible series of database operations such that either all occurs, or nothing occurs. A guarantee of atomicity prevents updates to the database occurring only partially, which can cause greater problems than rejecting the whole series outright. As a consequence, the transaction cannot be observed to be in progress by another database client. At one moment in time, it has not yet happened, and at the next it has already occurred in whole (or nothing happened if the transaction was cancelled in progress).
An example of an atomic transaction is a monetary transfer from bank account A to account B. It consists of two operations, withdrawing the money from account A and saving it to account B. Performing these operations in an atomic transaction ensures that the database remains in a consistent state, that is, money is neither lost nor created if either of those two operations fails.The same term is also used in the definition of First normal form in database systems, where it instead refers to the concept that the values for fields may not consist of multiple smaller value to be decomposed, such as a string into which multiple names, numbers, dates, or other types may be packed."
Isolation (database systems),"In database systems, isolation determines how transaction integrity is visible to other users and systems.
A lower isolation level increases the ability of many users to access the same data at the same time, but increases the number of concurrency effects (such as  dirty reads or lost updates) users might encounter. Conversely, a higher isolation level reduces the types of concurrency effects that users may encounter, but requires more system resources and increases the chances that one transaction will block another.Isolation is typically defined at database level as a property that defines how or when the changes made by one operation become visible to others. On older systems, it may be implemented systemically, for example through the use of temporary tables. In two-tier systems, a transaction processing (TP) manager is required to maintain isolation. In n-tier systems (such as multiple websites attempting to book the last seat on a flight), a combination of stored procedures and transaction management is required to commit the booking and send confirmation to the customer.Isolation is one of the four ACID properties, along with atomicity, consistency and durability."
Consistency (database systems),"In database systems, consistency (or correctness) refers to the requirement that any given database transaction must change affected data only in allowed ways. Any data written to the database must be valid according to all defined rules, including constraints, cascades, triggers, and any combination thereof.  This does not guarantee correctness of the transaction in all ways the application programmer might have wanted (that is the responsibility of application-level code) but merely that any programming errors cannot result in the violation of any defined database constraints.Consistency can also be understood as after a successful write, update or delete of a Record, any read request immediately receives the latest value of the Record."
Durability (database systems),"In database systems, durability is the ACID property which guarantees that transactions that have committed will survive permanently. 
For example, if a flight booking reports that a seat has successfully been booked, then the seat will remain booked even if the system crashes.Durability can be achieved by flushing the transaction's log records to non-volatile storage before acknowledging commitment.
In distributed transactions, all participating servers must coordinate before commit can be acknowledged. This is usually done by a two-phase commit protocol.
Many DBMSs implement durability by writing transactions into a transaction log that can be reprocessed to recreate the system state right before any later failure. A transaction is deemed committed only after it is entered in the log."
Federated database system,"A federated database system (FDBS) is a type of meta-database management system (DBMS), which transparently maps multiple autonomous database systems into a single federated database. The constituent databases are interconnected via a computer network and may be geographically decentralized. Since the constituent database systems remain autonomous, a federated database system is a contrastable alternative to the (sometimes daunting) task of merging several disparate databases. A federated database, or virtual database, is a composite of all constituent databases in a federated database system.  There is no actual data integration in the constituent disparate databases as a result of data federation.
Through data abstraction, federated database systems can provide a uniform user interface, enabling users and clients to store and retrieve data from multiple noncontiguous databases with a single query—even if the constituent databases are heterogeneous. To this end, a federated database system must be able to decompose the query into subqueries for submission to the relevant constituent DBMSs, after which the system must composite the result sets of the subqueries. Because various database management systems employ different query languages, federated database systems can apply wrappers to the subqueries to translate them into the appropriate query languages."
Object database,"An object database or object-oriented database  is a database management system in which information is represented in the form of objects as used in object-oriented programming. Object databases are different from relational databases which are table-oriented. A third type, object–relational databases, is a hybrid of both approaches.
Object databases have been considered since the early 1980s."
Heterogeneous database system,"A heterogeneous database system is an automated (or semi-automated) system for the integration of heterogeneous, disparate database management systems to present a user with a single, unified query interface.
Heterogeneous database systems (HDBs) are computational models and software implementations that provide heterogeneous database integration.

"
Oracle Database,"Oracle Database (commonly referred to as Oracle DBMS, Oracle Autonomous Database, or simply as Oracle) is a multi-model database management system produced and marketed by Oracle Corporation.
It is a database commonly used for running online transaction processing (OLTP), data warehousing (DW) and mixed (OLTP & DW) database workloads. Oracle Database is available by several service providers on-prem, on-cloud, or as a hybrid cloud installation.  It may be run on third party servers as well as on Oracle hardware (Exadata on-prem, on Oracle Cloud or at Cloud at Customer)."
data structures,"In computer science, a data structure is a data organization, management, and storage format that is usually chosen for efficient access to data. More precisely, a data structure is a collection of data values, the relationships among them, and the functions or operations that can be applied to the data, i.e., it is an algebraic structure about data.

"
Data structure,"In computer science, a data structure is a data organization, management, and storage format that is usually chosen for efficient access to data. More precisely, a data structure is a collection of data values, the relationships among them, and the functions or operations that can be applied to the data, i.e., it is an algebraic structure about data.

"
List of data structures,"This is a list of well-known data structures. For a wider list of terms, see list of terms relating to algorithms and data structures. For a comparison of running times for a subset of this list see comparison of data structures."
Array (data structure),"In computer science, an array is a data structure consisting of a collection of elements (values or variables), each identified by at least one array index or key. An array is stored such that the position of each element can be computed from its index tuple by a mathematical formula. The simplest type of data structure is a linear array, also called one-dimensional array.
For example, an array of ten 32-bit (4-byte) integer variables, with indices 0 through 9, may be stored as ten words at memory addresses 2000, 2004, 2008, ..., 2036, (in hexadecimal: 0x7D0, 0x7D4, 0x7D8, ..., 0x7F4) so that the element with index i has the address 2000 + (i × 4).
The memory address of the first element of an array is called first address, foundation address, or base address.
Because the mathematical concept of a matrix can be represented as a two-dimensional grid, two-dimensional arrays are also sometimes called ""matrices"". In some cases the term ""vector"" is used in computing to refer to an array, although tuples rather than vectors are the more mathematically correct equivalent. Tables are often implemented in the form of arrays, especially lookup tables; the word ""table"" is sometimes used as a synonym of array.
Arrays are among the oldest and most important data structures, and are used by almost every program. They are also used to implement many other data structures, such as lists and strings. They effectively exploit the addressing logic of computers. In most modern computers and many external storage devices, the memory is a one-dimensional array of words, whose indices are their addresses. Processors, especially vector processors, are often optimized for array operations.
Arrays are useful mostly because the element indices can be computed at run time. Among other things, this feature allows a single iterative statement to process arbitrarily many elements of an array. For that reason, the elements of an array data structure are required to have the same size and should use the same data representation. The set of valid index tuples and the addresses of the elements (and hence the element addressing formula) are usually, but not always, fixed while the array is in use.
The term ""array"" may also refer to an array data type, a kind of data type provided by most high-level programming languages that consists of a collection of values or variables that can be selected by one or more indices computed at run-time. Array types are often implemented by array structures; however, in some languages they may be implemented by hash tables, linked lists, search trees, or other data structures.
The term is also used, especially in the description of algorithms, to mean associative array or ""abstract array"", a theoretical computer science model (an abstract data type or ADT) intended to capture the essential properties of arrays."
Tree (data structure),"In computer science, a tree is a widely used abstract data type that represents a hierarchical tree structure with a set of connected nodes. Each node in the tree can be connected to many children (depending on the type of tree), but must be connected to exactly one parent, except for the root node, which has no parent. These constraints mean there are no cycles or ""loops"" (no node can be its own ancestor), and also that each child can be treated like the root node of its own subtree, making recursion a useful technique for tree traversal. In contrast to linear data structures, many trees cannot be represented by relationships between neighboring nodes in a single straight line.
Binary trees are a commonly used type, which constrain the number of children for each parent to exactly two. When the order of the children is specified, this data structure corresponds to an  ordered tree in graph theory. A value or pointer to other data may be associated with every node in the tree, or sometimes only with the leaf nodes, which have no children.
The abstract data type can be represented in a number of ways, including a list of parents with pointers to children, a list of children with pointers to parents, or a list of nodes and a separate list of parent-child relations (a specific type of adjacency list). Representations might also be more complicated, for example using indexes or ancestor lists for performance.
Trees as used in computing are similar to but can be different from mathematical constructs of trees in graph theory, trees in set theory, and trees in descriptive set theory.

"
Heap (data structure),"In computer science, a heap is a specialized tree-based data structure which is essentially an almost complete tree that satisfies the heap property: in a max heap, for any given node C, if P is a parent node of C, then the key (the value) of P is greater than or equal to the key of C. In a min heap, the key of P is less than or equal to the key of C. The node at the ""top"" of the heap (with no parents) is called the root node.
The heap is one maximally efficient implementation of an abstract data type called a priority queue, and in fact, priority queues are often referred to as ""heaps"", regardless of how they may be implemented. In a heap, the highest (or lowest) priority element is always stored at the root. However, a heap is not a sorted structure; it can be regarded as being partially ordered. A heap is a useful data structure when it is necessary to repeatedly remove the object with the highest (or lowest) priority, or when insertions need to be interspersed with removals of the root node.
A common implementation of a heap is the binary heap, in which the tree is a binary tree (see figure). The heap data structure, specifically the binary heap, was introduced by J. W. J. Williams in 1964, as a data structure for the heapsort sorting algorithm. Heaps are also crucial in several efficient graph algorithms such as Dijkstra's algorithm. When a heap is a complete binary tree, it has a smallest possible height—a heap with N nodes and a branches for each node always has loga N height.
Note that, as shown in the graphic, there is no implied ordering between siblings or cousins and no implied sequence for an in-order traversal (as there would be in, e.g., a binary search tree). The heap relation mentioned above applies only between nodes and their parents, grandparents, etc. The maximum number of children each node can have depends on the type of heap."
Data structure alignment,"Data structure alignment is the way data is arranged and accessed in computer memory. It consists of three separate but related issues: data alignment, data structure padding, and packing.
The CPU in modern computer hardware performs reads and writes to memory most efficiently when the data is naturally aligned, which generally means that the data's memory address is a multiple of the data size. For instance, in a 32-bit architecture, the data may be aligned if the data is stored in four consecutive bytes and the first byte lies on a 4-byte boundary.
Data alignment is the aligning of elements according to their natural alignment. To ensure natural alignment, it may be necessary to insert some padding between structure elements or after the last element of a structure. For example, on a 32-bit machine, a data structure containing a 16-bit value followed by a 32-bit value could have 16 bits of padding between the 16-bit value and the 32-bit value to align the 32-bit value on a 32-bit boundary. Alternatively, one can pack the structure, omitting the padding, which may lead to slower access, but uses three quarters as much memory.
Although data structure alignment is a fundamental issue for all modern computers, many computer languages and computer language implementations handle data alignment automatically. Fortran, Ada, PL/I, Pascal, certain C and C++ implementations, D, Rust, C#, and assembly language allow at least partial control of data structure padding, which may be useful in certain special circumstances."
Passive data structure,"In computer science and object-oriented programming, a passive data structure (PDS, also termed a plain old data structure, or plain old data, POD) is a term for a record, to contrast with objects. It is a data structure that is represented only as passive collections of field values (instance variables), without using object-oriented features."
Persistent data structure,"In computing, a persistent data structure or not ephemeral data structure is a data structure that always preserves the previous version of itself when it is modified. Such data structures are effectively immutable, as their operations do not (visibly) update the structure in-place, but instead always yield a new updated structure. The term was introduced in Driscoll, Sarnak, Sleator, and Tarjans' 1986 article.A data structure is partially persistent if all versions can be accessed but only the newest version can be modified. The data structure is fully persistent if every version can be both accessed and modified. If there is also a meld or merge operation that can create a new version from two previous versions, the data structure is called confluently persistent. Structures that are not persistent are called ephemeral.These types of data structures are particularly common in logical and functional programming, as languages in those paradigms discourage (or fully forbid) the use of mutable data.

"
Disjoint-set data structure,"In computer science, a disjoint-set data structure, also called a union–find data structure or merge–find set, is a data structure that stores a collection of disjoint (non-overlapping) sets.  Equivalently, it stores a partition of a set into disjoint subsets.  It provides operations for adding new sets, merging sets (replacing them by their union), and finding a representative member of a set. The last operation makes it possible to find out efficiently if any two elements are in the same or different sets.
While there are several ways of implementing disjoint-set data structures, in practice they are often identified with a particular implementation called a disjoint-set forest.  This is a specialized type of forest which performs unions and finds in near-constant amortized time.  To perform a sequence of m addition, union, or find operations on a disjoint-set forest with n nodes requires total time O(mα(n)), where α(n) is the extremely slow-growing inverse Ackermann function.  Disjoint-set forests do not guarantee this performance on a per-operation basis.  Individual union and find operations can take longer than a constant times α(n) time, but each operation causes the disjoint-set forest to adjust itself so that successive operations are faster.  Disjoint-set forests are both asymptotically optimal and practically efficient.
Disjoint-set data structures play a key role in Kruskal's algorithm for finding the minimum spanning tree of a graph.  The importance of minimum spanning trees means that disjoint-set data structures underlie a wide variety of algorithms.  In addition, disjoint-set data structures also have applications to symbolic computation, as well in compilers, especially for register allocation problems."
Abstract data type,"In computer science, an abstract data type (ADT) is a mathematical model for data types. An abstract data type is defined by its behavior (semantics) from the point of view of a user, of the data, specifically in terms of possible values, possible operations on data of this type, and the behavior of these operations. This mathematical model contrasts with data structures, which are concrete representations of data, and are the point of view of an implementer, not a user.
Formally, an ADT may be defined as a ""class of objects whose logical behavior is defined by a set of values and a set of operations""; this is analogous to an algebraic structure in mathematics. What is meant by ""behaviour"" varies by author, with the two main types of formal specifications for behavior being axiomatic (algebraic) specification and an abstract model; these correspond to axiomatic semantics and operational semantics of an abstract machine, respectively. Some authors also include the computational complexity (""cost""), both in terms of time (for computing operations) and space (for representing values). In practice, many common data types are not ADTs, as the abstraction is not perfect, and users must be aware of issues like arithmetic overflow that are due to the representation. For example, integers are often stored as fixed-width values (32-bit or 64-bit binary numbers), and thus experience integer overflow if the maximum value is exceeded.
ADTs are a theoretical concept, in computer science, used in the design and analysis of algorithms, data structures, and software systems, and do not correspond to specific features of computer languages—mainstream computer languages do not directly support formally specified ADTs. However, various language features correspond to certain aspects of ADTs, and are easily confused with ADTs proper; these include abstract types, opaque data types, protocols, and design by contract. ADTs were first proposed by Barbara Liskov and Stephen N. Zilles in 1974, as part of the development of the CLU language."
query language,"Query languages, data query languages or database query languages (DQL) are computer languages used to make queries in databases and information systems. A well known example is the Structured Query Language (SQL)."
Query language,"Query languages, data query languages or database query languages (DQL) are computer languages used to make queries in databases and information systems. A well known example is the Structured Query Language (SQL)."
Graph Query Language,"GQL (Graph Query Language) is a proposed standard graph query language. In September 2019 a proposal for a project to create a new standard graph query language (ISO/IEC 39075 Information Technology — Database Languages — GQL) was approved by a vote of national standards bodies which are members of ISO/IEC Joint Technical Committee 1(ISO/IEC JTC 1). JTC 1 is responsible for international Information Technology standards. GQL is intended to be a declarative database query language, like SQL."
Object Query Language,"Object Query Language (OQL) is a query language standard for object-oriented databases modeled after SQL and developed by the Object Data Management Group (ODMG). Because of its overall complexity the complete OQL standard has not yet been fully implemented in any software. The OQL standard influenced the design of later query languages such as JDOQL and EJB QL, though none are considered to be any version of OQL."
Cypher (query language),"Cypher is a declarative graph query language that allows for expressive and efficient data querying in a property graph.Cypher was largely an invention of Andrés Taylor while working for Neo4j, Inc. (formerly Neo Technology) in 2011. Cypher was originally intended to be used with the graph database Neo4j, but was opened up through the openCypher project in October 2015.The language was designed with the power and capability of SQL (standard query language for the relational database model) in mind, but Cypher was based on the components and needs of a database built upon the concepts of graph theory. In a graph model, data is structured as nodes (vertices in math and network science) and relationships (edges in math and network science) to focus on how entities in the data are connected and related to one another."
Data query language,"Data Query Language (DQL) is part of the base grouping of SQL sub-languages. These sub-languages are mainly categorized into four categories: a data query language (DQL), a data definition language (DDL), a data control language (DCL), and a data manipulation language (DML). Sometimes a transaction control language (TCL) is argued to be part of the sub-language set as well.
DQL statements are used for performing queries on the data within schema objects. The purpose of DQL commands is to get the schema relation based on the query passed to it.
Although often considered part of DML, the SQL SELECT statement is strictly speaking an example of DQL. When adding FROM or WHERE data manipulators to the SELECT statement the statement is then considered part of the DML."
Language Integrated Query,"Language Integrated Query (LINQ, pronounced ""link"") is a Microsoft .NET Framework component that adds native data querying capabilities to .NET languages, originally released as a major part of .NET Framework 3.5 in 2007.
LINQ extends the language by the addition of query expressions, which are akin to SQL statements, and can be used to conveniently extract and process data from arrays, enumerable classes, XML documents, relational databases, and third-party data sources. Other uses, which utilize query expressions as a general framework for readably composing arbitrary computations, include the construction of event handlers or monadic parsers. It also defines a set of method names (called standard query operators, or standard sequence operators), along with translation rules used by the compiler to translate query syntax expressions into expressions using fluent-style (called method syntax by Microsoft) with these method names, lambda expressions and anonymous types. Many of the concepts that LINQ introduced were originally tested in Microsoft's Cω research project. 
Ports of LINQ exist for PHP (PHPLinq), JavaScript (linq.js), TypeScript (linq.ts), and ActionScript (ActionLinq), although none are strictly equivalent to LINQ in the .NET inspired languages C#, F# and VB.NET (where it is a part of the language, not an external library, and where it often addresses a wider range of needs)."
Gremlin (query language),"Gremlin is a graph traversal language and virtual machine developed by Apache TinkerPop of the Apache Software Foundation. Gremlin works for both OLTP-based graph databases as well as OLAP-based graph processors. Gremlin's automata and functional language foundation enable Gremlin to naturally support: imperative and declarative querying; host language agnosticism; user-defined domain specific languages; an extensible compiler/optimizer, single- and multi-machine execution models; hybrid depth- and breadth-first evaluation with Turing Completeness.As an explanatory analogy, Apache TinkerPop and Gremlin are to graph databases what the JDBC and SQL are to relational databases. Likewise, the Gremlin traversal machine is to graph computing as what the Java virtual machine is to general purpose computing."
RDF query language,"An RDF query language is a computer language, specifically a query language for databases, able to retrieve and manipulate data stored in Resource Description Framework (RDF) format.
SPARQL has emerged as the standard RDF query language, and in 2008 became a W3C recommendation."
Yahoo! Query Language,"Yahoo! Query Language (YQL) is an SQL-like query language created by Yahoo! as part of their Developer Network. YQL is designed to retrieve and manipulate data from APIs through a single Web interface, thus allowing mashups that enable developers to create their own applications using Yahoo! Pipes online tool.
Initially launched in October 2008 with access to Yahoo APIs, February 2009 saw the addition of open data tables from third parties such as Google Reader, the Guardian, and The New York Times. Some of these APIs still require an API key to access them. On April 29 of 2009, Yahoo introduced the capability to execute the tables of data built through YQL using JavaScript run on the company's servers for free. On January 3, 2019 Yahoo retired the YQL API service."
Facebook Query Language,"Facebook Query Language (FQL) is a query language that allows querying Facebook user data by using a SQL-style interface, avoiding the need to use the Facebook Platform Graph API.  Data returned from an FQL query is in JSON format by default."
structured text search problems,"In computational complexity theory and computability theory, a search problem is a type of computational problem represented by a binary relation.  If R is a binary relation such that field(R) ⊆ Γ+ and T is a Turing machine, then T calculates R if:

If x is such that there is some y such that R(x, y) then T accepts x with output z such that R(x, z) (there may be multiple y, and T need only find one of them)
If x is such that there is no y such that R(x, y) then T rejects xIntuitively, the problem consists in finding structure ""y"" in object ""x"". An algorithm is said to solve the problem if at least one corresponding structure exists, and then one occurrence of this structure is made output; otherwise, the algorithm stops with an appropriate output (""Item not found"" or any message of the like).
Such problems occur very frequently in graph theory, for example, where searching graphs for structures such as particular matching, cliques, independent set, etc. are subjects of interest.
Note that the graph of a partial function is a binary relation, and if T calculates a partial function then there is at most one possible output.
A relation R can be viewed as a search problem, and a Turing machine which calculates R is also said to solve it. Every search problem has a corresponding decision problem, namely

  
    
      
        L
        (
        R
        )
        =
        {
        x
        ∣
        ∃
        y
        R
        (
        x
        ,
        y
        )
        }
        .
        
      
    
    {\displaystyle L(R)=\{x\mid \exists yR(x,y)\}.\,}
  This definition may be generalized to n-ary relations using any suitable encoding which allows multiple strings to be compressed into one string (for instance by listing them consecutively with a delimiter)."
Search problem,"In computational complexity theory and computability theory, a search problem is a type of computational problem represented by a binary relation.  If R is a binary relation such that field(R) ⊆ Γ+ and T is a Turing machine, then T calculates R if:

If x is such that there is some y such that R(x, y) then T accepts x with output z such that R(x, z) (there may be multiple y, and T need only find one of them)
If x is such that there is no y such that R(x, y) then T rejects xIntuitively, the problem consists in finding structure ""y"" in object ""x"". An algorithm is said to solve the problem if at least one corresponding structure exists, and then one occurrence of this structure is made output; otherwise, the algorithm stops with an appropriate output (""Item not found"" or any message of the like).
Such problems occur very frequently in graph theory, for example, where searching graphs for structures such as particular matching, cliques, independent set, etc. are subjects of interest.
Note that the graph of a partial function is a binary relation, and if T calculates a partial function then there is at most one possible output.
A relation R can be viewed as a search problem, and a Turing machine which calculates R is also said to solve it. Every search problem has a corresponding decision problem, namely

  
    
      
        L
        (
        R
        )
        =
        {
        x
        ∣
        ∃
        y
        R
        (
        x
        ,
        y
        )
        }
        .
        
      
    
    {\displaystyle L(R)=\{x\mid \exists yR(x,y)\}.\,}
  This definition may be generalized to n-ary relations using any suitable encoding which allows multiple strings to be compressed into one string (for instance by listing them consecutively with a delimiter)."
Binary search algorithm,"In computer science, binary search, also known as half-interval search, logarithmic search, or binary chop, is a search algorithm that finds the position of a target value within a sorted array. Binary search compares the target value to the middle element of the array. If they are not equal, the half in which the target cannot lie is eliminated and the search continues on the remaining half, again taking the middle element to compare to the target value, and repeating this until the target value is found. If the search ends with the remaining half being empty, the target is not in the array.
Binary search runs in logarithmic time in the worst case, making 
  
    
      
        O
        (
        log
        ⁡
        n
        )
      
    
    {\displaystyle O(\log n)}
   comparisons, where 
  
    
      
        n
      
    
    {\displaystyle n}
   is the number of elements in the array. Binary search is faster than linear search except for small arrays. However, the array must be sorted first to be able to apply binary search. There are specialized data structures designed for fast searching, such as hash tables, that can be searched more efficiently than binary search. However, binary search can be used to solve a wider range of problems, such as finding the next-smallest or next-largest element in the array relative to the target even if it is absent from the array.
There are numerous variations of binary search. In particular, fractional cascading speeds up binary searches for the same value in multiple arrays. Fractional cascading efficiently solves a number of search problems in computational geometry and in numerous other fields. Exponential search extends binary search to unbounded lists. The binary search tree and B-tree data structures are based on binary search.

"
Search engine indexing,"Search engine indexing is the collecting, parsing, and storing of data to facilitate fast and accurate information retrieval. Index design incorporates interdisciplinary concepts from linguistics, cognitive psychology, mathematics, informatics, and computer science. An alternate name for the process, in the context of search engines designed to find web pages on the Internet, is web indexing.
Popular engines focus on the full-text indexing of online, natural language documents. Media types such as pictures, video, audio, and graphics are also searchable.
Meta search engines reuse the indices of other services and do not store a local index whereas cache-based search engines permanently store the index along with the corpus. Unlike full-text indices, partial-text services restrict the depth indexed to reduce index size. Larger services typically perform indexing at a predetermined time interval due to the required time and processing costs, while agent-based search engines index in real time.

"
String-searching algorithm,"In computer science, string-searching algorithms, sometimes called string-matching algorithms, are an important class of string algorithms that try to find a place where one or several strings (also called patterns) are found within a larger string or text.
A basic example of string searching is when the pattern and the searched text are arrays of elements of an alphabet (finite set) Σ.  Σ may be a human language alphabet, for example, the letters A through Z and other applications may use a binary alphabet (Σ = {0,1}) or a DNA alphabet (Σ = {A,C,G,T}) in bioinformatics.
In practice, the method of feasible string-search algorithm may be affected by the string encoding. In particular, if a variable-width encoding is in use, then it may be slower to find the Nth character, perhaps requiring time proportional to N. This may significantly slow some search algorithms. One of many possible solutions is to search for the sequence of code units instead, but doing so may produce false matches unless the encoding is specifically designed to avoid it."
Structured prediction,"Structured prediction or structured (output) learning is an umbrella term for supervised machine learning techniques that involves predicting structured objects, rather than scalar discrete or real values.Similar to commonly used supervised learning techniques, structured prediction models are typically trained by means of observed data in which the true prediction value is used to adjust model parameters. Due to the complexity of the model and the interrelations of predicted variables the process of prediction using a trained model and of training itself is often computationally infeasible and approximate inference and learning methods are used."
Binary search tree,"In computer science, a binary search tree (BST), also called an ordered or sorted binary tree, is a rooted binary tree data structure with the key of each internal node being greater than all the keys in the respective node's left subtree and less than the ones in its right subtree. The time complexity of operations on the binary search tree is directly proportional to the height of the tree.
Binary search trees allow binary search for fast lookup, addition, and removal of data items. Since the nodes in a BST are laid out so that each comparison skips about half of the remaining tree, the lookup performance is proportional to that of binary logarithm. BSTs were devised in the 1960s for the problem of efficient storage of labeled data and are attributed to Conway Berners-Lee and David Wheeler.
The performance of a binary search tree is dependent on the order of insertion of the nodes into the tree since arbitrary insertions may lead to degeneracy; several variations of the binary search tree can be built with guaranteed worst-case performance. The basic operations include: search, traversal, insert and delete. BSTs with guaranteed worst-case complexities perform better than an unsorted array, which would require linear search time.
The complexity analysis of BST shows that, on average, the insert, delete and search takes 
  
    
      
        O
        (
        log
        ⁡
        n
        )
      
    
    {\displaystyle O(\log n)}
   for 
  
    
      
        n
      
    
    {\displaystyle n}
   nodes. In the worst case, they degrade to that of a singly linked list: 
  
    
      
        O
        (
        n
        )
      
    
    {\displaystyle O(n)}
  . To address the boundless increase of the tree height with arbitrary insertions and deletions, self-balancing variants of BSTs are introduced to bound the worst lookup complexity to that of the binary logarithm. AVL trees were the first self-balancing binary search trees, invented in 1962 by Georgy Adelson-Velsky and Evgenii Landis.
Binary search trees can be used to implement abstract data types such as dynamic sets, lookup tables and priority queues, and used in sorting algorithms such as tree sort."
Text mining,"Text mining, also referred to as text data mining, similar to text analytics, is the process of deriving high-quality information from text. It involves ""the discovery by computer of new, previously unknown information, by automatically extracting information from different written resources."" Written resources may include websites, books, emails, reviews, and articles. High-quality information is typically obtained by devising patterns and trends by means such as statistical pattern learning. According to Hotho et al. (2005) we can distinguish between three different perspectives of text mining: information extraction, data mining, and a KDD (Knowledge Discovery in Databases) process. Text mining usually involves the process of structuring the input text (usually parsing, along with the addition of some derived linguistic features and the removal of others, and subsequent insertion into a database), deriving patterns within the structured data, and finally evaluation and interpretation of the output. 'High quality' in text mining usually refers to some combination of relevance, novelty, and interest. Typical text mining tasks include text categorization, text clustering, concept/entity extraction, production of granular taxonomies, sentiment analysis, document summarization, and entity relation modeling (i.e., learning relations between named entities).
Text analysis involves information retrieval, lexical analysis to study word frequency distributions, pattern recognition, tagging/annotation, information extraction, data mining techniques including link and association analysis, visualization, and predictive analytics. The overarching goal is, essentially, to turn text into data for analysis, via application of natural language processing (NLP), different types of algorithms and analytical methods. An important phase of this process is the interpretation of the gathered information.
A typical application is to scan a set of documents written in a natural language and either model the document set for predictive classification purposes or populate a database or search index with the information extracted.
The document is the basic element while starting with text mining. Here, we define a document as a unit of textual data, which normally exists in many types of collections."
Scunthorpe problem,"The Scunthorpe problem is the unintentional blocking of websites, e-mails, forum posts or search results by a spam filter or search engine because their text contains a string (or substring) of letters that appear to have an obscene or otherwise unacceptable meaning. Names, abbreviations, and technical terms are most often cited as being affected by the issue.
The problem arises since computers can easily identify strings of text within a document, but interpreting words of this kind requires considerable ability to interpret a wide range of contexts, possibly across many cultures, which is an extremely difficult task. As a result, broad blocking rules may result in false positives affecting innocent phrases."
Regular expression,"A regular expression (shortened as regex or regexp; sometimes referred to as rational expression) is a sequence of characters that specifies a search pattern in text. Usually such patterns are used by string-searching algorithms for ""find"" or ""find and replace"" operations on strings, or for input validation. Regular expression techniques are developed in theoretical computer science and formal language theory.
The concept of regular expressions began in the 1950s, when the American mathematician Stephen Cole Kleene formalized the concept of a regular language. They came into common use with Unix text-processing utilities. Different syntaxes for writing regular expressions have existed since the 1980s, one being the POSIX standard and another, widely used, being the Perl syntax.
Regular expressions are used in search engines, in search and replace dialogs of word processors and text editors, in text processing utilities such as sed and AWK, and in lexical analysis. Most general-purpose programming languages support regex capabilities either natively or via libraries, including Python, C, C++,Java, Rust, OCaml, and JavaScript."
Problem structuring methods,"Problem structuring methods (PSMs) are a group of techniques used to model or to map the nature or structure of a situation or state of affairs that some people want to change. PSMs are usually used by a group of people in collaboration (rather than by a solitary individual) to create a consensus about, or at least to facilitate negotiations about, what needs to change. Some widely adopted PSMs include soft systems methodology, the strategic choice approach, and strategic options development and analysis (SODA).Unlike some problem solving methods that assume that all the relevant issues and constraints and goals that constitute the problem are defined in advance or are uncontroversial, PSMs assume that there is no single uncontested representation of what constitutes the problem.PSMs are mostly used with groups of people, but PSMs have also influenced the coaching and counseling of individuals."
unstructured retrieval,"An unstructured interview or non-directive interview is an interview in which questions are not prearranged. These non-directive interviews are considered to be the opposite of a structured interview which offers a set amount of standardized questions. The form of the unstructured interview varies widely, with some questions being prepared in advance in relation to a topic that the researcher or interviewer wishes to cover. They tend to be more informal and free flowing than a structured interview, much like an everyday conversation. Probing is seen to be the part of the research process that differentiates the in-depth, unstructured interview from an everyday conversation. This nature of conversation allows for spontaneity and for questions to develop during the course of the interview, which are based on the interviewees' responses. The chief feature of the unstructured interview is the idea of probe questions that are designed to be as open as possible. It is a qualitative research method and accordingly prioritizes validity and the depth of the interviewees' answers. One of the potential drawbacks  is the loss of reliability, thereby making it more difficult to draw patterns among interviewees' responses in comparison to structured interviews. Unstructured interviews are used in a variety of fields and circumstances, ranging from research in social sciences, such as sociology, to college and job interviews. Fontana and Frey have identified three types of in depth, ethnographic, unstructured interviews - oral history, creative interviews (an unconventional interview in that it does not follow the rules of traditional interviewing), and post-modern interviews."
Unstructured interview,A structured interview (also known as a standardized interview or a researcher-administered survey) is a quantitative research method commonly employed in survey research. The aim of this approach is to ensure that each interview is presented with exactly the same questions in the same order. This ensures that answers can be reliably aggregated and that comparisons can be made with confidence between sample sub groups or between different survey periods.
Unstructured data,"A data model  is an abstract model that organizes elements of data and standardizes how they relate to one another and to the properties of real-world entities. For instance, a data model may specify that the data element representing a car be composed of a number of other elements which, in turn, represent the color and size of the car and define its owner.
The term data model can refer to two distinct but closely related concepts. Sometimes it refers to an abstract formalization of the objects and relationships found in a particular application domain: for example the customers, products, and orders found in a manufacturing organization. At other times it refers to the set of concepts used in defining such formalizations: for example concepts such as entities, attributes, relations, or tables. So the ""data model"" of a banking application may be defined using the entity-relationship ""data model"". This article uses the term in both senses.

A data model explicitly determines the structure of data. Data models are typically specified by a data specialist, data librarian, or a digital humanities scholar in a data modeling notation. These notations are often represented in graphical form.A data model can sometimes be referred to as a data structure, especially in the context of programming languages. Data models are often complemented by function models, especially in the context of enterprise models."
vector space model,"Vector space model or term vector model is an algebraic model for representing text documents (and any objects, in general) as vectors of identifiers (such as index terms). It is used in information filtering, information retrieval, indexing and relevancy rankings.  Its first use was in the SMART Information Retrieval System.

"
Vector space model,"Vector space model or term vector model is an algebraic model for representing text documents (and any objects, in general) as vectors of identifiers (such as index terms). It is used in information filtering, information retrieval, indexing and relevancy rankings.  Its first use was in the SMART Information Retrieval System.

"
Vector space,"In mathematics and physics, a vector space (also called a linear space) is a set whose elements, often called vectors, may be added together and multiplied (""scaled"") by numbers called scalars. Scalars are often real numbers, but can be  complex numbers or, more generally, elements of any field. The operations of vector addition and scalar multiplication must satisfy certain requirements, called vector axioms. The terms real vector space and complex vector space are often used to specify the nature of the scalars: real coordinate space or complex coordinate space.
Vector spaces generalize Euclidean vectors, which allow modeling of physical quantities, such as forces and velocity, that have not only a magnitude, but also a direction. The concept of vector spaces is fundamental for linear algebra, together with the concept of matrix, which allows computing in vector spaces. This provides a concise and synthetic way for manipulating and studying systems of linear equations.
Vector spaces are characterized by their dimension, which, roughly speaking, specifies the number of independent directions in the space. This means that, for two vector spaces with the same dimension, the properties that depend only on the vector-space structure are exactly the same (technically the vector spaces are isomorphic). A vector space is finite-dimensional if its dimension is a natural number. Otherwise, it is infinite-dimensional, and its dimension is an infinite cardinal. Finite-dimensional vector spaces occur naturally in geometry and related areas. Infinite-dimensional vector spaces occur in many areas of mathematics. For example, polynomial rings are countably infinite-dimensional vector spaces, and many function spaces have the cardinality of the continuum as a dimension.
Many vector spaces that are considered in mathematics are also endowed with other structures. This is the case of algebras, which include field extensions, polynomial rings, associative algebras and Lie algebras. This is also the case of topological vector spaces, which include function spaces, inner product spaces, normed spaces, Hilbert spaces and Banach spaces."
Ranking (information retrieval),"Ranking of query is one of the fundamental problems in information retrieval (IR), the scientific/engineering discipline behind search engines. Given a query q and a collection D of documents that match the query, the problem is to rank, that is, sort, the documents in D according to some criterion so that the ""best"" results appear early in the result list displayed to the user. Ranking in terms of information retrieval is an important concept in computer science and is used in many different applications such as search engine queries and recommender systems. A majority of search engines use ranking algorithms to provide users with accurate and relevant results."
Word embedding,"In natural language processing (NLP), word embedding is a term used for the representation of words for text analysis, typically in the form of a real-valued vector that encodes the meaning of the word such that the words that are closer in the vector space are expected to be similar in meaning. Word embeddings can be obtained using a set of language modeling and feature learning techniques where words or phrases from the vocabulary are mapped to vectors of real numbers.
Methods to generate this mapping include neural networks, dimensionality reduction on the word co-occurrence matrix, probabilistic models, explainable knowledge base method, and explicit representation in terms of the context in which words appear.Word and phrase embeddings, when used as the underlying input representation, have been shown to boost the performance in NLP tasks such as syntactic parsing and sentiment analysis."
Bag-of-words model,"The bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity. The bag-of-words model has also been used for computer vision.The bag-of-words model is commonly used in methods of document classification where the (frequency of) occurrence of each word is used as a feature for training a classifier.An early reference to ""bag of words"" in a linguistic context can be found in Zellig Harris's 1954 article on Distributional Structure.The Bag-of-words model is one example of a Vector space model."
relevance feedback,"Relevance feedback is a feature of some information retrieval systems.  The idea behind relevance feedback is to take the results that are initially returned from a given query, to gather user feedback, and to use information about whether or not those results are relevant to perform a new query.  We can usefully distinguish between three types of feedback: explicit feedback, implicit feedback, and blind or ""pseudo"" feedback."
Relevance feedback,"Relevance feedback is a feature of some information retrieval systems.  The idea behind relevance feedback is to take the results that are initially returned from a given query, to gather user feedback, and to use information about whether or not those results are relevant to perform a new query.  We can usefully distinguish between three types of feedback: explicit feedback, implicit feedback, and blind or ""pseudo"" feedback."
Content-based image retrieval,"Content-based image retrieval, also known as query by image content (QBIC) and content-based visual information retrieval (CBVIR), is the application of computer vision techniques to the image retrieval problem, that is, the problem of searching for digital images in large databases (see this survey for a scientific overview of the CBIR field). Content-based image retrieval is opposed to traditional concept-based approaches (see Concept-based image indexing).
""Content-based"" means that the search analyzes the contents of the image rather than the metadata such as keywords, tags, or descriptions associated with the image. The term ""content"" in this context might refer to colors, shapes, textures, or any other information that can be derived from the image itself.  CBIR is desirable because searches that rely purely on metadata are dependent on annotation quality and completeness.
Having humans manually annotate images by entering keywords or metadata in a large database can be time-consuming and may not capture the keywords desired to describe the image.  The evaluation of the effectiveness of keyword image search is subjective and has not been well-defined.  In the same regard, CBIR systems have similar challenges in defining success. ""Keywords also limit the scope of queries to the set of predetermined criteria."" and, ""having been set up"" are less reliable than using the content itself."
collection of documents,"US Government Documents is a digital collection of documents at the Internet Archive. This collection contains digital versions of over 50,000 United States Government documents. The contributors of this collection are Kahle/Austin Foundation, MSN and Omidyar Network."
Khalili Collection of Aramaic Documents,"The Khalili Collection of Aramaic Documents is a private collection of letters and documents from the Bactria region in present-day Afghanistan, assembled by the British-Iranian collector and philanthropist Nasser D. Khalili. It is one of the Khalili Collections: eight collections of artifacts assembled, conserved, published and exhibited by Khalili. 
The documents, written in Imperial Aramaic, likely originated from the historical city of Balkh and all are dated between 353 BC to 324 BC, mostly during the reign of Artaxerxes III. The most recent of the documents was written during the early part of Alexander the Great's reign in the region. These letters use in Aramaic the original Greek form Alexandros (spelled Lksndrs) instead of the Eastern variant Iskandar (spelled Lksndr). The collection also includes eighteen tally sticks recording transfers of goods during the reign of Darius III. The collection's letters, administrative records, and military documents are significant for the linguistic study of the Official Aramaic language and of daily life in the Achaemenid empire."
US Government Documents,"US Government Documents is a digital collection of documents at the Internet Archive. This collection contains digital versions of over 50,000 United States Government documents. The contributors of this collection are Kahle/Austin Foundation, MSN and Omidyar Network."
Tf–idf,"In information retrieval, tf–idf (also TF*IDF, TFIDF, TF–IDF, or Tf–idf), short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling.
The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. tf–idf is one of the most popular term-weighting schemes today. A survey conducted in 2015 showed that 83% of text-based recommender systems in digital libraries use tf–idf.Variations of the tf–idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query. tf–idf can be successfully used for stop-words filtering in various subject fields, including text summarization and classification.
One of the simplest ranking functions is computed by summing the tf–idf for each query term; many more sophisticated ranking functions are variants of this simple model."
File,"A film – also called a movie, motion picture, moving picture, picture, photoplay  or (slang) flick – is a work of visual art that simulates experiences and otherwise communicates ideas, stories, perceptions, feelings, beauty, or atmosphere through the use of moving images. These images are generally accompanied by sound and, more rarely, other sensory stimulations. The word ""cinema"", short for cinematography, is often used to refer to filmmaking and the film industry, and to the art form that is the result of it."
Document management system,"A document management system (DMS) is usually a computerized system used to store, share, track and manage files or documents. Some systems include history tracking where a log of the various versions created and modified by different users is recorded. The term has some overlap with the concepts of content management systems. It is often viewed as a component of enterprise content management (ECM) systems and related to digital asset management, document imaging, workflow systems and records management systems."
Document-term matrix,"A document-term matrix is a mathematical matrix that describes the frequency of terms that occur in a collection of documents. In a document-term matrix, rows correspond to documents in the collection and columns correspond to terms. This matrix is a specific instance of a document-feature matrix where ""features"" may refer to other properties of a document besides terms. It is also common to encounter the transpose, or term-document matrix where documents are the columns and terms are the rows. They are useful in the field of natural language processing and computational text analysis.While the value of the cells is commonly the raw count of a given term, there are various schemes for weighting the raw counts such as, row normalizing (i.e. relative frequency/proportions) and tf-idf.
Terms are commonly single words separated by whitespace or punctuation on either side (a.k.a. unigrams). In such a case, this is also referred to as ""bag of words"" representation because the counts of individual words is retained, but not the order of the words in the document."
Data set,"A data set (or dataset) is a collection of data. In the case of tabular data, a data set corresponds to one or more database tables, where every column of a table represents a particular variable, and each row corresponds to a given record of the data set in question. The data set lists values for each of the variables, such as for example height and weight of an object, for each member of the data set.  Data sets can also consist of a collection of documents or files.In the open data discipline, data set is the unit to measure the information released in a public open data repository.  The European data.europa.eu portal aggregates more than a million data sets.  Some other issues (real-time data sources, non-relational data sets, etc.) increases the difficulty to reach a consensus about it.

"
Book of Documents,"The Book of Documents (Shūjīng, earlier Shu King) or Classic of History, also known as the Shangshu (“Venerated Documents”), is one of the Five Classics of ancient Chinese literature.  It is a collection of rhetorical prose attributed to figures of ancient China, and served as the foundation of Chinese political philosophy for over 2,000 years.
The Book of Documents was the subject of one of China's oldest literary controversies, between proponents of different versions of the text. A version was preserved from Qin Shi Huang's burning of books and burying of scholars by scholar Fu Sheng, in 29 sections (pian 篇). This group of texts were referred to as ""Modern Script"" jinwen 今文, because written with the script in use at the beginning of the Western Han dyansty. According to Western Han dynasty documents, new textual material was  discovered in the wall of Confucius' family estate in Qufu by his descendant Kong Anguo in the late 2nd century BC. This new material was referred to as ""Old Script"" guwen 古文, because written in the script that predated the standardization of Chinese script enforced during the Qing dynasty. Compared to the ""Modern Script"" texts, the ""Old Script"" material had 16 more texts. However, this seems to have been lost at the end of the Eastern Han dynasty, while the ""Modern Script"" text enjoyed circulation, in particular in Ouyang Gao’s 歐陽高 (a. 136 BCE) study of it, the Ouyang Shangshu 歐陽尚書. This was the basis of studies by Ma Rong and Zheng Xuan in the Easter Han Dynasty. By the end of the second century CE, there was knowledge that the Shangshu at some point included more than the ""Modern Script"" text. This likely prompted scholars to recreate the ""Old Script"" texts said to have once belonged to the Shangshu, a process that culminated with the presentation of a 58 section (59 if the preface is included in the count) Shangshu to the Eastern Jin court, in 317 CE, by Mei Ze 梅頤.
This version was accepted, among doubts, and later was canonized as part of Kong Yingda's project. It was only in the 17th century that Qing dynasty scholar Yan Ruoqu demonstrated that the ""Old Script"" were actually fabrications ""reconstructed"" in the 3rd or 4th centuries AD.
In the transmitted edition, texts are grouped into four sections representing different eras: the semi-mythical reign of Yu the Great, and the Xia, Shang and Zhou dynasties.  The Zhou section accounts for over half the text.  Some of its New Text chapters are among the earliest examples of Chinese prose, recording speeches from the early years of the Zhou dynasty in the late 11th century BC.  Although the other three sections purport to record earlier material, most scholars believe that even the New Script chapters in these sections were composed later than those in the Zhou section, with chapters relating to the earliest periods being as recent as the 4th or 3rd centuries BC."
Erowid,"Erowid, also called Erowid Center, is a non-profit educational organization that provides information about psychoactive plants and chemicals.Erowid documents legal and illegal substances, including their intended and adverse effects. Information on Erowid's website is gathered from diverse sources including published literature, experts in related fields, and the experiences of the general public. Erowid acts as a publisher of new information as well as a library for the collection of documents and images published elsewhere."
Transmittal document,"A transmittal document is a ""packing slip"" for a document or collection of documents that are transferred from one company to another. The transmittal might be just the front page in an extensive document. But more often it is a separate document file that contains details of the documents that are sent. The transmittal also contains specific (company or project-related) details to help further processing of the documents for the recipient.
The content of the transmittal document depends on the situation. Some typical content in a transmittal can be:

Date of the sending.
Name details of sender/company and recipient/company.
Project name, number(s), and other references to the project.
Reason(s) for sending.
Deadline(s) and/or descriptions of actions to be taken by recipient.
Other status details.
List of files sent: file name, size, type, revision number and other relevant metadata.
Limitations, security measures or other dependencies of the document transmittal.Transmittals are used in engineering and construction companies as a necessary tool in projects where a large number of documents are involved. Several document handling systems have functions for generating transmittal document along with packages of document for transfer."
query,"A quest is a journey toward a specific mission or a goal. The word serves as a plot device in mythology and fiction: a difficult journey towards a goal, often symbolic or allegorical.  Tales of quests figure prominently in the folklore of every nation and ethnic culture.  In literature, the object of a quest requires great exertion on the part of the hero, who must overcome many obstacles, typically including much travel.  The aspect of travel allows the storyteller to showcase exotic locations and cultures (an objective of the narrative, not of the character).  The object of a quest may also have supernatural properties, often leading the protagonist into other worlds and dimensions. The moral of a quest tale often centers on the changed character of the hero."
Query,"A quest is a journey toward a specific mission or a goal. The word serves as a plot device in mythology and fiction: a difficult journey towards a goal, often symbolic or allegorical.  Tales of quests figure prominently in the folklore of every nation and ethnic culture.  In literature, the object of a quest requires great exertion on the part of the hero, who must overcome many obstacles, typically including much travel.  The aspect of travel allows the storyteller to showcase exotic locations and cultures (an objective of the narrative, not of the character).  The object of a quest may also have supernatural properties, often leading the protagonist into other worlds and dimensions. The moral of a quest tale often centers on the changed character of the hero."
Query string,"A query string is a part of a uniform resource locator (URL) that assigns values to specified parameters. A query string commonly includes fields added to a base URL by a Web browser or other client application, for example as part of an HTML, choosing the appearance of a page, or jumping to positions in multimedia content.

A web server can handle a Hypertext Transfer Protocol (HTTP) request either by reading a file from its file system based on the URL path or by handling the request using logic that is specific to the type of resource. In cases where special logic is invoked, the query string will be available to that logic for use in its processing, along with the path component of the URL."
Web query,"A web query or web search query is a query that a user enters into a web search engine to satisfy their information needs. Web search queries are distinctive in that they are often plain text and boolean search directives are rarely used. They vary greatly from standard query languages, which are governed by strict syntax rules as command languages with keyword or positional parameters."
probabilistic methods,"The probabilistic method is a nonconstructive method, primarily used in combinatorics and pioneered by Paul Erdős, for proving the existence of a prescribed kind of mathematical object. It works by showing that if one randomly chooses objects from a specified class, the probability that the result is of the prescribed kind is strictly greater than zero. Although the proof uses probability, the final conclusion is determined for certain, without any possible error.
This method has now been applied to other areas of mathematics such as number theory, linear algebra, and real analysis, as well as in computer science (e.g. randomized rounding), and information theory."
Probabilistic method,"The probabilistic method is a nonconstructive method, primarily used in combinatorics and pioneered by Paul Erdős, for proving the existence of a prescribed kind of mathematical object. It works by showing that if one randomly chooses objects from a specified class, the probability that the result is of the prescribed kind is strictly greater than zero. Although the proof uses probability, the final conclusion is determined for certain, without any possible error.
This method has now been applied to other areas of mathematics such as number theory, linear algebra, and real analysis, as well as in computer science (e.g. randomized rounding), and information theory."
Artificial intelligence,"Artificial intelligence (AI) is intelligence—perceiving, synthesizing, and inferring information—demonstrated by machines, as opposed to intelligence displayed by animals and humans. Example tasks in which this is done include speech recognition, computer vision, translation between (natural) languages, as well as other mappings of inputs. The Oxford English Dictionary of Oxford University Press defines artificial intelligence as:

the theory and development of computer systems able to perform tasks that normally require human intelligence, such as visual perception, speech recognition, decision-making, and translation between languages.

AI applications include advanced web search engines (e.g., Google), recommendation systems (used by YouTube, Amazon and Netflix), understanding human speech (such as Siri and Alexa), self-driving cars (e.g., Tesla), automated decision-making and competing at the highest level in strategic game systems (such as chess and Go).
As machines become increasingly capable, tasks considered to require ""intelligence"" are often removed from the definition of AI, a phenomenon known as the AI effect. For instance, optical character recognition is frequently excluded from things considered to be AI, having become a routine technology.Artificial intelligence was founded as an academic discipline in 1956, and in the years since has experienced several waves of optimism, followed by disappointment and the loss of funding (known as an ""AI winter""), followed by new approaches, success and renewed funding. AI research has tried and discarded many different approaches since its founding, including simulating the brain, modeling human problem solving, formal logic, large databases of knowledge and imitating animal behavior. In the first decades of the 21st century, highly mathematical-statistical machine learning has dominated the field, and this technique has proved highly successful, helping to solve many challenging problems throughout industry and academia.The various sub-fields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and the ability to move and manipulate objects. General intelligence (the ability to solve an arbitrary problem) is among the field's long-term goals. To solve these problems, AI researchers have adapted and integrated a wide range of problem-solving techniques – including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, probability and economics. AI also draws upon computer science, psychology, linguistics, philosophy, and many other fields.
The field was founded on the assumption that human intelligence ""can be so precisely described that a machine can be made to simulate it"".
This raised philosophical arguments about the mind and the ethical consequences of creating artificial beings endowed with human-like intelligence; these issues have previously been explored by myth, fiction and philosophy since antiquity. Computer scientists and philosophers have since suggested that AI may become an existential risk to humanity if its rational capacities are not steered towards beneficial goals."
Unsupervised learning,"Supervised learning (SL) is a machine learning paradigm for problems where the available data consists of labelled examples, meaning that each data point contains features (covariates) and an associated label. The goal of supervised learning algorithms is learning a function that maps feature vectors (inputs) to labels (output), based on example input-output pairs. It infers a function from labeled training data consisting of a set of training examples. In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a ""reasonable"" way (see inductive bias). This statistical quality of an algorithm is measured through the so-called generalization error."
Probability,"Probability is the branch of mathematics concerning numerical descriptions of how likely an event is to occur, or how likely it is that a proposition is true.  The probability of an event is a number between 0 and 1, where, roughly speaking, 0 indicates impossibility of the event and 1 indicates certainty. The higher the probability of an event, the more likely it is that the event will occur. A simple example is the tossing of a fair (unbiased) coin. Since the coin is fair, the two outcomes (""heads"" and ""tails"") are both equally probable; the probability of ""heads"" equals the probability of ""tails""; and since no other outcomes are possible, the probability of either ""heads"" or ""tails"" is 1/2 (which could also be written as 0.5 or 50%).
These concepts have been given an axiomatic mathematical formalization in probability theory, which is used widely in areas of study such as statistics, mathematics, science, finance, gambling, artificial intelligence, machine learning, computer science, game theory, and philosophy to, for example, draw inferences about the expected frequency of events. Probability theory is also used to describe the underlying mechanics and regularities of complex systems."
Computational intelligence,"The expression computational intelligence (CI) usually refers to the ability of a computer to learn a specific task from data or experimental observation. Even though it is commonly considered a synonym of soft computing, there is still no commonly accepted definition of computational intelligence.
Generally, computational intelligence is a set of nature-inspired computational methodologies and approaches to address complex real-world problems to which mathematical or traditional modelling can be useless for a few reasons: the processes might be too complex for mathematical reasoning, it might contain some uncertainties during the process, or the process might simply be stochastic in nature. Indeed, many real-life problems cannot be translated into binary language (unique values of 0 and 1) for computers to process it. Computational Intelligence therefore provides solutions for such problems.
The methods used are close to the human's way of reasoning, i.e. it uses inexact and incomplete knowledge, and it is able to produce control actions in an adaptive way. CI therefore uses a combination of five main complementary techniques. The fuzzy logic which enables the computer to understand natural language, artificial neural networks which permits the system to learn experiential data by operating like the biological one, evolutionary computing, which is based on the process of natural selection, learning theory, and probabilistic methods which helps dealing with uncertainty imprecision.Except those main principles, currently popular approaches include biologically inspired algorithms such as swarm intelligence and artificial immune systems, which can be seen as a part of evolutionary computation, image processing, data mining, natural language processing, and artificial intelligence, which tends to be confused with Computational Intelligence. But although both Computational Intelligence (CI) and Artificial Intelligence (AI) seek similar goals, there's a clear distinction between them.
Computational Intelligence is thus a way of performing like human beings. Indeed, the characteristic of ""intelligence"" is usually attributed to humans. More recently, many products and items also claim to be ""intelligent"", an attribute which is directly linked to the reasoning and decision making."
Probabilistic classification,"In machine learning, a probabilistic classifier is a classifier that is able to predict, given an observation of an input, a probability distribution over a set of classes, rather than only outputting the most likely class that the observation should belong to. Probabilistic classifiers provide classification that can be useful in its own right or when combining classifiers into ensembles."
Oil and gas reserves and resource quantification,"Oil and gas reserves denote discovered quantities of crude oil and natural gas (oil or gas fields) that can be profitably produced/recovered from an approved development. Oil and gas reserves tied to approved operational plans filed on the day of reserves reporting are also sensitive to fluctuating global market pricing.  The remaining resource estimates (after the reserves have been accounted) are likely sub-commercial and may still be under appraisal with the potential to be technically recoverable once commercially established. Natural gas is frequently associated with oil directly and gas reserves are commonly quoted in barrels of oil equivalent (BoE).  Consequently both oil and gas reserves, as well as resource estimates, follow the same reporting guidelines, and are referred to collectively hereinafter as oil & gas."
Probabilistic design,"Probabilistic design is a discipline within engineering design. It deals primarily with the consideration of the effects of random variability upon the performance of an engineering system during the design phase. Typically, these effects are related to quality and reliability. Thus, probabilistic design is a tool that is mostly used in areas that are  concerned with quality and reliability. For example, product design, quality control, systems engineering, machine design, civil engineering (particularly useful in limit state design) and manufacturing. It differs from the classical approach to design by assuming a small probability of failure instead of using the safety factor."
language models,"A language model is a probability distribution over sequences of words. Given any sequence of words of length m, a language model assigns a probability 
  
    
      
        P
        (
        
          w
          
            1
          
        
        ,
        …
        ,
        
          w
          
            m
          
        
        )
      
    
    {\displaystyle P(w_{1},\ldots ,w_{m})}
   to the whole sequence. Language models generate probabilities by training on text corpora in one or many languages. Given that languages can be used to express an infinite variety of valid sentences (the property of digital infinity), language modeling faces the problem of assigning non-zero probabilities to linguistically valid sequences that may never be encountered in the training data. Several modelling approaches have been designed to surmount this problem, such as applying the Markov assumption or using neural architectures such as recurrent neural networks or transformers.
Language models are useful for a variety of problems in computational linguistics; from initial applications in speech recognition to ensure nonsensical (i.e. low-probability) word sequences are not predicted, to wider use in machine translation (e.g. scoring candidate translations), natural language generation (generating more human-like text), part-of-speech tagging, parsing, Optical Character Recognition, handwriting recognition, grammar induction, information retrieval, and other applications.
Language models are used in information retrieval in the query likelihood model. There, a separate language model is associated with each document in a collection. Documents are ranked based on the probability of the query Q in the document's language model 
  
    
      
        
          M
          
            d
          
        
      
    
    {\displaystyle M_{d}}
  : 
  
    
      
        P
        (
        Q
        ∣
        
          M
          
            d
          
        
        )
      
    
    {\displaystyle P(Q\mid M_{d})}
  . Commonly, the unigram language model is used for this purpose."
Language model,"A language model is a probability distribution over sequences of words. Given any sequence of words of length m, a language model assigns a probability 
  
    
      
        P
        (
        
          w
          
            1
          
        
        ,
        …
        ,
        
          w
          
            m
          
        
        )
      
    
    {\displaystyle P(w_{1},\ldots ,w_{m})}
   to the whole sequence. Language models generate probabilities by training on text corpora in one or many languages. Given that languages can be used to express an infinite variety of valid sentences (the property of digital infinity), language modeling faces the problem of assigning non-zero probabilities to linguistically valid sequences that may never be encountered in the training data. Several modelling approaches have been designed to surmount this problem, such as applying the Markov assumption or using neural architectures such as recurrent neural networks or transformers.
Language models are useful for a variety of problems in computational linguistics; from initial applications in speech recognition to ensure nonsensical (i.e. low-probability) word sequences are not predicted, to wider use in machine translation (e.g. scoring candidate translations), natural language generation (generating more human-like text), part-of-speech tagging, parsing, Optical Character Recognition, handwriting recognition, grammar induction, information retrieval, and other applications.
Language models are used in information retrieval in the query likelihood model. There, a separate language model is associated with each document in a collection. Documents are ranked based on the probability of the query Q in the document's language model 
  
    
      
        
          M
          
            d
          
        
      
    
    {\displaystyle M_{d}}
  : 
  
    
      
        P
        (
        Q
        ∣
        
          M
          
            d
          
        
        )
      
    
    {\displaystyle P(Q\mid M_{d})}
  . Commonly, the unigram language model is used for this purpose."
Unified Modeling Language,"The Unified Modeling Language (UML) is a general-purpose, developmental modeling language in the field of software engineering that is intended to provide a standard way to visualize the design of a system.The creation of UML was originally motivated by the desire to standardize the disparate notational systems and approaches to software design. It was developed at Rational Software in 1994–1995, with further development led by them through 1996.In 1997, UML was adopted as a standard by the Object Management Group (OMG), and has been managed by this organization ever since. In 2005, UML was also published by the International Organization for Standardization (ISO) as an approved ISO standard. Since then the standard has been periodically revised to cover the latest revision of UML. In software engineering, most practitioners do not use UML, but instead produce informal hand drawn diagrams; these diagrams, however, often include elements from UML.: 536 "
BERT (language model),"Bidirectional Encoder Representations from Transformers (BERT) is a transformer-based machine learning technique for natural language processing (NLP) pre-training developed by Google. BERT was created and published in 2018 by Jacob Devlin and his colleagues from Google. In 2019, Google announced that it had begun leveraging BERT in its search engine, and by late 2020 it was using BERT in almost every English-language query.  A 2020 literature survey concluded that ""in a little over a year, BERT has become a ubiquitous baseline in NLP experiments"", counting over 150 research publications analyzing and improving the model.The original English-language BERT has two models: (1) the BERTBASE: 12 encoders with 12 bidirectional self-attention heads, and (2) the BERTLARGE: 24 encoders with 16 bidirectional self-attention heads. Both models are pre-trained from unlabeled data extracted from the BooksCorpus with 800M words and English Wikipedia with 2,500M words."
Modeling language,A modeling language is any artificial language that can be used to express information or knowledge or systems in a structure that is defined by a consistent set of rules. The rules are used for interpretation of the meaning of components in the structure.
query likelihood model,The query likelihood model is a language model used in information retrieval. A language model is constructed for each document in the collection.  It is then possible to rank each document by the probability of specific documents given a query. This is interpreted as being the likelihood of a document being relevant given a query.
Query likelihood model,The query likelihood model is a language model used in information retrieval. A language model is constructed for each document in the collection.  It is then possible to rank each document by the probability of specific documents given a query. This is interpreted as being the likelihood of a document being relevant given a query.
Language model,"A language model is a probability distribution over sequences of words. Given any sequence of words of length m, a language model assigns a probability 
  
    
      
        P
        (
        
          w
          
            1
          
        
        ,
        …
        ,
        
          w
          
            m
          
        
        )
      
    
    {\displaystyle P(w_{1},\ldots ,w_{m})}
   to the whole sequence. Language models generate probabilities by training on text corpora in one or many languages. Given that languages can be used to express an infinite variety of valid sentences (the property of digital infinity), language modeling faces the problem of assigning non-zero probabilities to linguistically valid sequences that may never be encountered in the training data. Several modelling approaches have been designed to surmount this problem, such as applying the Markov assumption or using neural architectures such as recurrent neural networks or transformers.
Language models are useful for a variety of problems in computational linguistics; from initial applications in speech recognition to ensure nonsensical (i.e. low-probability) word sequences are not predicted, to wider use in machine translation (e.g. scoring candidate translations), natural language generation (generating more human-like text), part-of-speech tagging, parsing, Optical Character Recognition, handwriting recognition, grammar induction, information retrieval, and other applications.
Language models are used in information retrieval in the query likelihood model. There, a separate language model is associated with each document in a collection. Documents are ranked based on the probability of the query Q in the document's language model 
  
    
      
        
          M
          
            d
          
        
      
    
    {\displaystyle M_{d}}
  : 
  
    
      
        P
        (
        Q
        ∣
        
          M
          
            d
          
        
        )
      
    
    {\displaystyle P(Q\mid M_{d})}
  . Commonly, the unigram language model is used for this purpose."
Command–query separation,"Command-query separation (CQS) is a principle of imperative computer programming. It was devised by Bertrand Meyer as part of his pioneering work on the Eiffel programming language.
It states that every method should either be a command that performs an action, or a query that returns data to the caller, but not both. In other words, asking a question should not change the answer. More formally, methods should return a value only if they are referentially transparent and hence possess no side effects."
Bayesian network,"A Bayesian network (also known as a Bayes network, Bayes net, belief network, or decision network) is a probabilistic graphical model that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG). Bayesian networks are ideal for taking an event that occurred and predicting the likelihood that any one of several possible known causes was the contributing factor.  For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases.
Efficient algorithms can perform inference and learning in Bayesian networks. Bayesian networks that model sequences of variables (e.g. speech signals or protein sequences) are called dynamic Bayesian networks. Generalizations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams."
Bradley–Terry model,"The Bradley–Terry model is a probability model that can predict the outcome of a paired comparison. Given a pair of individuals i and j drawn from some population, it estimates the probability that the pairwise comparison i > j turns out true, as

  
    
      
        P
        (
        i
        >
        j
        )
        =
        
          
            
              p
              
                i
              
            
            
              
                p
                
                  i
                
              
              +
              
                p
                
                  j
                
              
            
          
        
      
    
    {\displaystyle P(i>j)={\frac {p_{i}}{p_{i}+p_{j}}}}
  where pi is a positive real-valued score assigned to individual i. The comparison i > j can be read as ""i is preferred to j"", ""i ranks higher than j"", or ""i beats j"", depending on the application.
For example, pi may represent the skill of a team in a sports tournament, estimated from the number of times i has won a match. 
  
    
      
        P
        (
        i
        >
        j
        )
      
    
    {\displaystyle P(i>j)}
   then represents the probability that i will win a match against j. Another example used to explain the model's purpose is that of scoring products in a certain category by quality. While it's hard for a person to draft a direct ranking of (many) brands of wine, it may be feasible to compare a sample of pairs of wines and say, for each pair, which one is better. The Bradley–Terry model can then be used to derive a full ranking."
Ranking (information retrieval),"Ranking of query is one of the fundamental problems in information retrieval (IR), the scientific/engineering discipline behind search engines. Given a query q and a collection D of documents that match the query, the problem is to rank, that is, sort, the documents in D according to some criterion so that the ""best"" results appear early in the result list displayed to the user. Ranking in terms of information retrieval is an important concept in computer science and is used in many different applications such as search engine queries and recommender systems. A majority of search engines use ranking algorithms to provide users with accurate and relevant results."
N-gram,"In the fields of computational linguistics and probability, an n-gram (sometimes also called Q-gram) is a contiguous sequence of n items from a given sample of text or speech. The items can be phonemes, syllables, letters, words or base pairs according to the application.  The n-grams typically are collected from a text or speech corpus. When the items are words, n-grams may also be called shingles.Using Latin numerical prefixes, an n-gram of size 1 is referred to as a ""unigram""; size 2 is a ""bigram"" (or, less commonly, a ""digram""); size 3 is a ""trigram"". English cardinal numbers are sometimes used, e.g., ""four-gram"", ""five-gram"", and so on.    In computational biology, a polymer or oligomer of a known size is called a k-mer instead of an n-gram, with specific names using Greek numerical prefixes such as ""monomer"", ""dimer"", ""trimer"", ""tetramer"", ""pentamer"", etc., or English cardinal numbers, ""one-mer"", ""two-mer"", ""three-mer"", etc."
Homology modeling,"Homology modeling, also known as comparative modeling of protein, refers to constructing an atomic-resolution model of the ""target"" protein from its amino acid sequence and an experimental three-dimensional structure of a related homologous protein (the ""template""). Homology modeling relies on the identification of one or more known protein structures likely to resemble the structure of the query sequence, and on the production of an alignment that maps residues in the query sequence to residues in the template sequence. It has been seen that protein structures are more conserved than protein sequences amongst homologues, but sequences falling below a 20% sequence identity can have very different structure.Evolutionarily related proteins have similar sequences and naturally occurring homologous proteins have similar protein structure.
It has been shown that three-dimensional protein structure is evolutionarily more conserved than would be expected on the basis of sequence conservation alone.The sequence alignment and template structure are then used to produce a structural model of the target. Because protein structures are more conserved than DNA sequences, and detectable levels of sequence similarity usually imply significant structural similarity.The quality of the homology model is dependent on the quality of the sequence alignment and template structure. The approach can be complicated by the presence of alignment gaps (commonly called indels) that indicate a structural region present in the target but not in the template, and by structure gaps in the template that arise from poor resolution in the experimental procedure (usually X-ray crystallography) used to solve the structure. Model quality declines with decreasing sequence identity; a typical model has ~1–2 Å root mean square deviation between the matched Cα atoms at 70% sequence identity but only 2–4 Å agreement at 25% sequence identity. However, the errors are significantly higher in the loop regions, where the amino acid sequences of the target and template proteins may be completely different.
Regions of the model that were constructed without a template, usually by loop modeling, are generally much less accurate than the rest of the model. Errors in side chain packing and position also increase with decreasing identity, and variations in these packing configurations have been suggested as a major reason for poor model quality at low identity. Taken together, these various atomic-position errors are significant and impede the use of homology models for purposes that require atomic-resolution data, such as drug design and protein–protein interaction predictions; even the quaternary structure of a protein may be difficult to predict from homology models of its subunit(s). Nevertheless, homology models can be useful in reaching qualitative conclusions about the biochemistry of the query sequence, especially in formulating hypotheses about why certain residues are conserved, which may in turn lead to experiments to test those hypotheses.  For example, the spatial arrangement of conserved residues may suggest whether a particular residue is conserved to stabilize the folding, to participate in binding some small molecule, or to foster association with another protein or nucleic acid.
Homology modeling can produce high-quality structural models when the target and template are closely related, which has inspired the formation of a structural genomics consortium dedicated to the production of representative experimental structures for all classes of protein folds.  The chief inaccuracies in homology modeling, which worsen with lower sequence identity, derive from errors in the initial sequence alignment and from improper template selection. Like other methods of structure prediction, current practice in homology modeling is assessed in a biennial large-scale experiment known as the Critical Assessment of Techniques for Protein Structure Prediction, or CASP."
One-shot learning,"One-shot learning is an object categorization problem, found mostly in computer vision. Whereas most machine learning-based object categorization algorithms require training on hundreds or thousands of examples, one-shot learning aims to classify objects from one, or only a few, examples. The term few-shot learning is also used for these problems, especially when more than one example is needed."
G-test,"In statistics, G-tests are likelihood-ratio or maximum likelihood statistical significance tests that are increasingly being used in situations where chi-squared tests were previously recommended.The general formula for G is

  
    
      
        G
        =
        2
        
          ∑
          
            i
          
        
        
          
            O
            
              i
            
          
          ⋅
          ln
          ⁡
          
            (
            
              
                
                  O
                  
                    i
                  
                
                
                  E
                  
                    i
                  
                
              
            
            )
          
        
        ,
      
    
    {\displaystyle G=2\sum _{i}{O_{i}\cdot \ln \left({\frac {O_{i}}{E_{i}}}\right)},}
  where 
  
    
      
        
          O
          
            i
          
        
        ≥
        0
      
    
    {\textstyle O_{i}\geq 0}
   is the observed count in a cell, 
  
    
      
        
          E
          
            i
          
        
        >
        0
      
    
    {\textstyle E_{i}>0}
   is the expected count under the null hypothesis, 
  
    
      
        ln
      
    
    {\textstyle \ln }
   denotes the natural logarithm, and the sum is taken over all non-empty cells. Furthermore, the total observed count should be equal to the total expected count:where 
  
    
      
        N
      
    
    {\textstyle N}
   is the total number of observations.
G-tests have been recommended at least since the 1981 edition of Biometry, a statistics textbook by Robert R. Sokal and F. James Rohlf."
query likelihood language models,The query likelihood model is a language model used in information retrieval. A language model is constructed for each document in the collection.  It is then possible to rank each document by the probability of specific documents given a query. This is interpreted as being the likelihood of a document being relevant given a query.
Query likelihood model,The query likelihood model is a language model used in information retrieval. A language model is constructed for each document in the collection.  It is then possible to rank each document by the probability of specific documents given a query. This is interpreted as being the likelihood of a document being relevant given a query.
Language model,"A language model is a probability distribution over sequences of words. Given any sequence of words of length m, a language model assigns a probability 
  
    
      
        P
        (
        
          w
          
            1
          
        
        ,
        …
        ,
        
          w
          
            m
          
        
        )
      
    
    {\displaystyle P(w_{1},\ldots ,w_{m})}
   to the whole sequence. Language models generate probabilities by training on text corpora in one or many languages. Given that languages can be used to express an infinite variety of valid sentences (the property of digital infinity), language modeling faces the problem of assigning non-zero probabilities to linguistically valid sequences that may never be encountered in the training data. Several modelling approaches have been designed to surmount this problem, such as applying the Markov assumption or using neural architectures such as recurrent neural networks or transformers.
Language models are useful for a variety of problems in computational linguistics; from initial applications in speech recognition to ensure nonsensical (i.e. low-probability) word sequences are not predicted, to wider use in machine translation (e.g. scoring candidate translations), natural language generation (generating more human-like text), part-of-speech tagging, parsing, Optical Character Recognition, handwriting recognition, grammar induction, information retrieval, and other applications.
Language models are used in information retrieval in the query likelihood model. There, a separate language model is associated with each document in a collection. Documents are ranked based on the probability of the query Q in the document's language model 
  
    
      
        
          M
          
            d
          
        
      
    
    {\displaystyle M_{d}}
  : 
  
    
      
        P
        (
        Q
        ∣
        
          M
          
            d
          
        
        )
      
    
    {\displaystyle P(Q\mid M_{d})}
  . Commonly, the unigram language model is used for this purpose."
Command–query separation,"Command-query separation (CQS) is a principle of imperative computer programming. It was devised by Bertrand Meyer as part of his pioneering work on the Eiffel programming language.
It states that every method should either be a command that performs an action, or a query that returns data to the caller, but not both. In other words, asking a question should not change the answer. More formally, methods should return a value only if they are referentially transparent and hence possess no side effects."
N-gram,"In the fields of computational linguistics and probability, an n-gram (sometimes also called Q-gram) is a contiguous sequence of n items from a given sample of text or speech. The items can be phonemes, syllables, letters, words or base pairs according to the application.  The n-grams typically are collected from a text or speech corpus. When the items are words, n-grams may also be called shingles.Using Latin numerical prefixes, an n-gram of size 1 is referred to as a ""unigram""; size 2 is a ""bigram"" (or, less commonly, a ""digram""); size 3 is a ""trigram"". English cardinal numbers are sometimes used, e.g., ""four-gram"", ""five-gram"", and so on.    In computational biology, a polymer or oligomer of a known size is called a k-mer instead of an n-gram, with specific names using Greek numerical prefixes such as ""monomer"", ""dimer"", ""trimer"", ""tetramer"", ""pentamer"", etc., or English cardinal numbers, ""one-mer"", ""two-mer"", ""three-mer"", etc."
One-shot learning,"One-shot learning is an object categorization problem, found mostly in computer vision. Whereas most machine learning-based object categorization algorithms require training on hundreds or thousands of examples, one-shot learning aims to classify objects from one, or only a few, examples. The term few-shot learning is also used for these problems, especially when more than one example is needed."
Ranking (information retrieval),"Ranking of query is one of the fundamental problems in information retrieval (IR), the scientific/engineering discipline behind search engines. Given a query q and a collection D of documents that match the query, the problem is to rank, that is, sort, the documents in D according to some criterion so that the ""best"" results appear early in the result list displayed to the user. Ranking in terms of information retrieval is an important concept in computer science and is used in many different applications such as search engine queries and recommender systems. A majority of search engines use ranking algorithms to provide users with accurate and relevant results."
Go (programming language),"Go is a statically typed, compiled programming language designed at Google by Robert Griesemer, Rob Pike, and Ken Thompson. It is syntactically similar to C, but with memory safety, garbage collection, structural typing, and CSP-style concurrency. It is often referred to as Golang because of its former domain name, golang.org, but its proper name is Go.There are two major implementations:

Google's self-hosting ""gc"" compiler toolchain, targeting multiple operating systems and WebAssembly.
gofrontend, a frontend to other compilers, with the libgo library. With GCC the combination is gccgo; with LLVM the combination is gollvm.A third-party source-to-source compiler, GopherJS, compiles Go to JavaScript for front-end web development."
Bayesian network,"A Bayesian network (also known as a Bayes network, Bayes net, belief network, or decision network) is a probabilistic graphical model that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG). Bayesian networks are ideal for taking an event that occurred and predicting the likelihood that any one of several possible known causes was the contributing factor.  For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases.
Efficient algorithms can perform inference and learning in Bayesian networks. Bayesian networks that model sequences of variables (e.g. speech signals or protein sequences) are called dynamic Bayesian networks. Generalizations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams."
Time series,"In mathematics, a time series is a series of data points indexed (or listed or graphed) in time order.  Most commonly, a time series is a sequence taken at successive equally spaced points in time. Thus it is a sequence of discrete-time data. Examples of time series are heights of ocean tides, counts of sunspots, and the daily closing value of the Dow Jones Industrial Average.
A time series is very frequently plotted via a run chart (which is a temporal line chart). Time series are used in statistics, signal processing, pattern recognition, econometrics, mathematical finance, weather forecasting, earthquake prediction, electroencephalography, control engineering, astronomy, communications engineering, and largely in any domain of applied science and engineering which involves temporal measurements.
Time series analysis comprises methods for analyzing time series data in order to extract meaningful statistics and other characteristics of the data. Time series forecasting is the use of a model to predict future values based on previously observed values. While regression analysis is often employed in such a way as to test relationships between one or more different time series, this type of analysis is not usually called ""time series analysis"", which refers in particular to relationships between different points in time within a single series.
Time series data have a natural temporal ordering.  This makes time series analysis distinct from cross-sectional studies, in which there is no natural ordering of the observations (e.g. explaining people's wages by reference to their respective education levels, where the individuals' data could be entered in any order).  Time series analysis is also distinct from spatial data analysis where the observations typically relate to geographical locations (e.g. accounting for house prices by the location as well as the intrinsic characteristics of the houses). A stochastic model for a time series will generally reflect the fact that observations close together in time will be more closely related than observations further apart. In addition, time series models will often make use of the natural one-way ordering of time so that values for a given period will be expressed as deriving in some way from past values, rather than from future values (see time reversibility).
Time series analysis can be applied to real-valued, continuous data, discrete numeric data, or discrete symbolic data (i.e. sequences of characters, such as letters and words in the English language).

"
List of statistical software,Statistical software are specialized computer programs for analysis in statistics and econometrics.
language modeling approach,"A language model is a probability distribution over sequences of words. Given any sequence of words of length m, a language model assigns a probability 
  
    
      
        P
        (
        
          w
          
            1
          
        
        ,
        …
        ,
        
          w
          
            m
          
        
        )
      
    
    {\displaystyle P(w_{1},\ldots ,w_{m})}
   to the whole sequence. Language models generate probabilities by training on text corpora in one or many languages. Given that languages can be used to express an infinite variety of valid sentences (the property of digital infinity), language modeling faces the problem of assigning non-zero probabilities to linguistically valid sequences that may never be encountered in the training data. Several modelling approaches have been designed to surmount this problem, such as applying the Markov assumption or using neural architectures such as recurrent neural networks or transformers.
Language models are useful for a variety of problems in computational linguistics; from initial applications in speech recognition to ensure nonsensical (i.e. low-probability) word sequences are not predicted, to wider use in machine translation (e.g. scoring candidate translations), natural language generation (generating more human-like text), part-of-speech tagging, parsing, Optical Character Recognition, handwriting recognition, grammar induction, information retrieval, and other applications.
Language models are used in information retrieval in the query likelihood model. There, a separate language model is associated with each document in a collection. Documents are ranked based on the probability of the query Q in the document's language model 
  
    
      
        
          M
          
            d
          
        
      
    
    {\displaystyle M_{d}}
  : 
  
    
      
        P
        (
        Q
        ∣
        
          M
          
            d
          
        
        )
      
    
    {\displaystyle P(Q\mid M_{d})}
  . Commonly, the unigram language model is used for this purpose."
Language model,"A language model is a probability distribution over sequences of words. Given any sequence of words of length m, a language model assigns a probability 
  
    
      
        P
        (
        
          w
          
            1
          
        
        ,
        …
        ,
        
          w
          
            m
          
        
        )
      
    
    {\displaystyle P(w_{1},\ldots ,w_{m})}
   to the whole sequence. Language models generate probabilities by training on text corpora in one or many languages. Given that languages can be used to express an infinite variety of valid sentences (the property of digital infinity), language modeling faces the problem of assigning non-zero probabilities to linguistically valid sequences that may never be encountered in the training data. Several modelling approaches have been designed to surmount this problem, such as applying the Markov assumption or using neural architectures such as recurrent neural networks or transformers.
Language models are useful for a variety of problems in computational linguistics; from initial applications in speech recognition to ensure nonsensical (i.e. low-probability) word sequences are not predicted, to wider use in machine translation (e.g. scoring candidate translations), natural language generation (generating more human-like text), part-of-speech tagging, parsing, Optical Character Recognition, handwriting recognition, grammar induction, information retrieval, and other applications.
Language models are used in information retrieval in the query likelihood model. There, a separate language model is associated with each document in a collection. Documents are ranked based on the probability of the query Q in the document's language model 
  
    
      
        
          M
          
            d
          
        
      
    
    {\displaystyle M_{d}}
  : 
  
    
      
        P
        (
        Q
        ∣
        
          M
          
            d
          
        
        )
      
    
    {\displaystyle P(Q\mid M_{d})}
  . Commonly, the unigram language model is used for this purpose."
Systems modeling,"Systems modeling or system modeling is the interdisciplinary study of the use of models to conceptualize and construct systems in business and IT development.A common type of systems modeling is function modeling, with specific techniques such as the Functional Flow Block Diagram and IDEF0. These models can be extended using functional decomposition, and can be linked to requirements models for further systems partition.
Contrasting the functional modeling, another type of systems modeling is architectural modeling which uses the systems architecture to conceptually model the structure, behavior, and more views of a system.
The Business Process Modeling Notation (BPMN), a graphical representation for specifying business processes in a workflow, can also be considered to be a systems modeling language."
Unified Modeling Language,"The Unified Modeling Language (UML) is a general-purpose, developmental modeling language in the field of software engineering that is intended to provide a standard way to visualize the design of a system.The creation of UML was originally motivated by the desire to standardize the disparate notational systems and approaches to software design. It was developed at Rational Software in 1994–1995, with further development led by them through 1996.In 1997, UML was adopted as a standard by the Object Management Group (OMG), and has been managed by this organization ever since. In 2005, UML was also published by the International Organization for Standardization (ISO) as an approved ISO standard. Since then the standard has been periodically revised to cover the latest revision of UML. In software engineering, most practitioners do not use UML, but instead produce informal hand drawn diagrams; these diagrams, however, often include elements from UML.: 536 "
Lifecycle Modeling Language,"The Lifecycle Modeling Language (LML) is an open-standard modeling language designed for systems engineering. It supports the full lifecycle: conceptual, utilization, support and retirement stages. Along with the integration of all lifecycle disciplines including, program management, systems and design engineering, verification and validation, deployment and maintenance into one framework.
LML was originally designed by the LML steering committee. The specification was published October 17, 2013.
This is a modeling language like UML and SysML that supports additional project management uses such as risk analysis and scheduling. LML uses common language to define its modeling elements such as entity, attribute, schedule, cost, and relationship."
Model-driven engineering,"Model-driven engineering (MDE) is a software development methodology that focuses on creating and exploiting domain models, which are conceptual models of all the topics related to a specific problem. Hence, it highlights and aims at abstract representations of the knowledge and activities that govern a particular application domain, rather than the computing (i.e. algorithmic) concepts."
Data modeling,Data modeling in software engineering is the process of creating a data model for an information system by applying certain formal techniques.
Dimensional modeling,"Dimensional modeling (DM) is part of the Business Dimensional Lifecycle methodology developed by Ralph Kimball which includes a set of methods, techniques and concepts for use in data warehouse design.: 1258–1260   The approach focuses on identifying the key business processes within a business and modelling and implementing these first before adding additional business processes, as a bottom-up approach.: 1258–1260   An alternative approach from Inmon advocates a top down design of the model of all the enterprise data using tools such as entity-relationship modeling (ER).: 1258–1260 

"
Object-modeling technique,"The object-modeling technique (OMT) is an object modeling approach for software modeling and designing.  It was developed around 1991 by Rumbaugh, Blaha, Premerlani, Eddy and Lorensen as a method to develop object-oriented systems and to support object-oriented programming. OMT describes object model or static structure of the system.
OMT was developed as an approach to software development. The purposes of modeling according to Rumbaugh are:
testing physical entities before building them (simulation),
communication with customers,
visualization (alternative presentation of information), and
reduction of complexity.OMT has proposed three main types of models:

Object model: The object model represents the static and most stable phenomena in the modeled domain. Main concepts are classes and associations with attributes and operations. Aggregation and generalization (with multiple inheritance) are predefined relationships.
Dynamic model: The dynamic model represents a state/transition view on the model. Main concepts are states, transitions between states, and events to trigger transitions. Actions can be modeled as occurring within states. Generalization and aggregation (concurrency) are predefined relationships.
Functional model: The functional model handles the process perspective of the model, corresponding roughly to data flow diagrams. Main concepts are process, data store, data flow, and actors.OMT is a predecessor of the Unified Modeling Language (UML). Many OMT modeling elements are common to UML.
Functional Model in OMT:
In brief, a functional model in OMT defines the function of the whole internal processes in a model with the help of ""Data Flow Diagrams (DFDs)"". It details how processes are performed independently."
Information model,"An information model in software engineering is a representation of concepts and the relationships, constraints, rules, and operations to specify data semantics for a chosen domain of discourse. Typically it specifies relations between kinds of things, but may also include relations with individual things. It can provide sharable, stable, and organized structure of information requirements or knowledge for the domain context."
Object-role modeling,"Object-role modeling (ORM) is used to model the semantics of a universe of discourse. ORM is often used for data modeling and software engineering.
An object-role model uses graphical symbols that are based on first order predicate logic and set theory to enable the modeler to create an unambiguous definition of an arbitrary universe of discourse. Attribute free, the predicates of an ORM Model lend themselves to the analysis and design of graph database models in as much as ORM was originally conceived to benefit relational database design.
The term ""object-role model"" was coined in the 1970s and ORM based tools have been used for more than 30 years – principally for data modeling. More recently ORM has been used to model business rules, XML-Schemas, data warehouses, requirements engineering and web forms."
ad hoc retrieval,"The Text REtrieval Conference (TREC) is an ongoing series of workshops focusing on a list of different information retrieval (IR) research areas, or tracks. It is co-sponsored by the National Institute of Standards and Technology (NIST) and the Intelligence Advanced Research Projects Activity (part of the office of the Director of National Intelligence), and began in 1992 as part of the TIPSTER Text program. Its purpose is to support and encourage research within the information retrieval community by providing the infrastructure necessary for large-scale evaluation of text retrieval methodologies and to increase the speed of lab-to-product transfer of technology.
TREC's evaluation protocols have improved many search technologies.  A 2010 study estimated that ""without TREC, U.S. Internet users would have spent up to 3.15 billion additional hours using web search engines between 1999 and 2009."" Hal Varian the Chief Economist at Google wrote that ""The TREC data revitalized research on information retrieval. Having a standard, widely available, and carefully constructed set of data laid the groundwork for further innovation in this field.""Each track has a challenge wherein NIST provides participating groups with data sets and test problems. Depending on track, test problems might be questions, topics, or target extractable features. Uniform scoring is performed so the systems can be fairly evaluated. After evaluation of the results, a workshop provides a place for participants to collect together thoughts and ideas and present current and future research work.Text Retrieval Conference started in 1992, funded by DARPA (US Defense Advanced Research Project) and run by NIST. Its purpose was to support research within the information retrieval community by providing the infrastructure necessary for large-scale evaluation of text retrieval methodologies."
Text Retrieval Conference,"The Text REtrieval Conference (TREC) is an ongoing series of workshops focusing on a list of different information retrieval (IR) research areas, or tracks. It is co-sponsored by the National Institute of Standards and Technology (NIST) and the Intelligence Advanced Research Projects Activity (part of the office of the Director of National Intelligence), and began in 1992 as part of the TIPSTER Text program. Its purpose is to support and encourage research within the information retrieval community by providing the infrastructure necessary for large-scale evaluation of text retrieval methodologies and to increase the speed of lab-to-product transfer of technology.
TREC's evaluation protocols have improved many search technologies.  A 2010 study estimated that ""without TREC, U.S. Internet users would have spent up to 3.15 billion additional hours using web search engines between 1999 and 2009."" Hal Varian the Chief Economist at Google wrote that ""The TREC data revitalized research on information retrieval. Having a standard, widely available, and carefully constructed set of data laid the groundwork for further innovation in this field.""Each track has a challenge wherein NIST provides participating groups with data sets and test problems. Depending on track, test problems might be questions, topics, or target extractable features. Uniform scoring is performed so the systems can be fairly evaluated. After evaluation of the results, a workshop provides a place for participants to collect together thoughts and ideas and present current and future research work.Text Retrieval Conference started in 1992, funded by DARPA (US Defense Advanced Research Project) and run by NIST. Its purpose was to support research within the information retrieval community by providing the infrastructure necessary for large-scale evaluation of text retrieval methodologies."
Lemur Project,"The Lemur Project is a collaboration between the Center for Intelligent Information Retrieval at the University of Massachusetts Amherst and the Language Technologies Institute at Carnegie Mellon University. The Lemur Project develops search engines, browser toolbars, text analysis tools, and data resources that support research and development of information retrieval and text mining software. The project is best known for its Indri and Galago search engines, the ClueWeb09 and ClueWeb12 datasets, and the RankLib learning-to-rank library. The software and datasets are used widely in scientific and research applications, as well as in some commercial applications.
The Lemur Project's software development philosophy emphasizes state-of-the-art accuracy, flexibility, and efficiency. For example, the Indri search engine provides accurate search for large text collections 'out of the box', and data is stored in an accessible manner to support development of new retrieval strategies. Software from the Lemur Project is distributed under open-source licenses that provide flexibility to scientists and software developers.
The programming languages used to create Lemur are C, C++, and Java, and it comes along with the source files and build instructions. The provided source code can be modified for the purpose of developing new libraries. It is compatible with various operating systems which include Linux and Windows."
International Society for Music Information Retrieval,"The International Society for Music Information Retrieval (ISMIR) is an international forum for research on the organization of music-related data. It started as an informal group steered by an ad hoc committee in 2000 which established a yearly symposium - whence ""ISMIR"", which meant International Symposium on Music Information Retrieval. It was turned into a conference in 2002 while retaining the acronym. ISMIR was incorporated in Canada on July 4, 2008."
Query language,"Query languages, data query languages or database query languages (DQL) are computer languages used to make queries in databases and information systems. A well known example is the Structured Query Language (SQL)."
RetrievalWare,RetrievalWare is an enterprise search engine emphasizing natural language processing and semantic networks which was commercially available from 1992 to 2007 and is especially known for its use by government intelligence agencies.
Object–relational mapping,"Object–relational mapping (ORM, O/RM, and O/R mapping tool) in computer science is a programming technique for converting data between type systems using object-oriented programming languages. This creates, in effect, a ""virtual object database"" that can be used from within the programming language. There are both free and commercial packages available that perform object–relational mapping, although some programmers opt to construct their own ORM tools.
In object-oriented programming, data-management tasks act on objects that are almost always non-scalar values. For example, consider an address book entry that represents a single person along with zero or more phone numbers and zero or more addresses. This could be modeled in an object-oriented implementation by a ""Person object"" with an attribute/field to hold each data item that the entry comprises: the person's name, a list of phone numbers, and a list of addresses. The list of phone numbers would itself contain ""PhoneNumber objects"" and so on. Each such address-book entry is treated as a single object by the programming language (it can be referenced by a single variable containing a pointer to the object, for instance). Various methods can be associated with the object, such as methods to return the preferred phone number, the home address, and so on.
By contrast, many popular database products, such as SQL, are not object-oriented and can only store and manipulate scalar values such as integers and strings organized within tables. The programmer must either convert the object values into groups of simpler values for storage in the database (and convert them back upon retrieval), or only use simple scalar values within the program. Object–relational mapping implements the first approach.The heart of the problem involves translating the logical representation of the objects into an atomized form that is capable of being stored in the database while preserving the properties of the objects and their relationships so that they can be reloaded as objects when needed. If this storage and retrieval functionality is implemented, the objects are said to be persistent."
Azure Data Explorer,"Azure Data Explorer is a fully-managed big data analytics cloud platform and data-exploration service, developed by Microsoft, that ingests structured, semi-structured (like JSON) and unstructured data (like free-text). The service then stores this data and answers analytic ad hoc queries on it with seconds of latency. It is a full text indexing and retrieval database, including time series analysis capabilities and regular expression evaluation and text parsing.It is offered as Platform as a Service (PaaS) as part of Microsoft Azure platform. The product was announced by Microsoft in 2018."
Geographic information retrieval,"Information retrieval (IR) in computing and information science is the process of obtaining information system resources that are relevant to an information need from a collection of those resources.  Searches can be based on full-text or other content-based indexing. Information retrieval is the science of searching for information in a document, searching for documents themselves, and also searching for the metadata that describes data, and for databases of texts, images or sounds.
Automated information retrieval systems are used to reduce what has been called information overload. An IR system is a software system that provides access to books, journals and other documents; stores and manages those documents. Web search engines are the most visible IR applications."
Cheng Xiang Zhai,ChengXiang Zhai is a computer scientist. He is a Donald Biggar Willett Professor in Engineering in the Department of Computer Science at the University of Illinois at Urbana-Champaign.
Wi-Fi,"Wi-Fi () is a family of wireless network protocols, based on the IEEE 802.11 family of standards, which are commonly used for local area networking of devices and Internet access, allowing nearby digital devices to exchange data by radio waves.  These are the most widely used computer networks in the world, used globally in home and small office networks to link desktop and laptop computers, tablet computers, smartphones, smart TVs, printers, and smart speakers together and to a wireless router to connect them to the Internet, and in wireless access points in public places like coffee shops, hotels, libraries and airports to provide visitors with Internet access for their mobile devices.
Wi-Fi is a trademark of the non-profit Wi-Fi Alliance, which restricts the use of the term Wi-Fi Certified to products that successfully complete interoperability certification testing. As of 2017, the Wi-Fi Alliance consisted of more than 800 companies from around the world. As of 2019, over 3.05 billion Wi-Fi enabled devices are shipped globally each year.Wi-Fi uses multiple parts of the IEEE 802 protocol family and is designed to interwork seamlessly with its wired sibling, Ethernet. Compatible devices can network through wireless access points to each other as well as to wired devices and the Internet. The different versions of Wi-Fi are specified by various IEEE 802.11 protocol standards, with the different radio technologies determining radio bands, and the maximum ranges, and speeds that may be achieved. Wi-Fi most commonly uses the 2.4 gigahertz (120 mm) UHF and 5 gigahertz (60 mm) SHF radio bands; these bands are subdivided into multiple channels. Channels can be shared between networks but, within range, only one transmitter can transmit on a channel at a time.

Wi-Fi's radio bands have relatively high absorption and work best for line-of-sight use. Many common obstructions such as walls, pillars, home appliances, etc. may greatly reduce range, but this also helps minimize interference between different networks in crowded environments. An access point range of about 20 metres (66 feet) indoors while some access points claim up to a 150-metre (490-foot) range outdoors. Hotspot coverage can be as small as a single room with walls that block radio waves, or as large as many square kilometres using many overlapping access points with roaming permitted between them. Over time the speed and spectral efficiency of Wi-Fi have increased. As of 2019, some versions of Wi-Fi, running on suitable hardware at close range, can achieve speeds of 9.6 Gbit/s (gigabit per second).

"
document space,"A document is a written, drawn, presented, or memorialized representation of thought, often the manifestation of non-fictional, as well as fictional, content.  The word originates from the Latin Documentum, which denotes a ""teaching"" or ""lesson"": the verb doceō denotes ""to teach"". In the past, the word was usually used to denote written proof useful as evidence of a truth or fact. In the computer age, ""document"" usually denotes a primarily textual computer file, including its structure and format, e.g. fonts, colors, and images. Contemporarily, ""document"" is not defined by its transmission medium, e.g., paper, given the existence of electronic documents. ""Documentation"" is distinct because it has more denotations than ""document"". Documents are also distinguished from ""realia"", which are three-dimensional objects that would otherwise satisfy the definition of ""document"" because they memorialize or represent thought; documents are considered more as 2-dimensional representations. While documents can have large varieties of customization, all documents can be shared freely and have the right to do so, creativity can be represented by documents, also. History, events, examples, opinions, etc. all can be expressed in documents."
Document,"A document is a written, drawn, presented, or memorialized representation of thought, often the manifestation of non-fictional, as well as fictional, content.  The word originates from the Latin Documentum, which denotes a ""teaching"" or ""lesson"": the verb doceō denotes ""to teach"". In the past, the word was usually used to denote written proof useful as evidence of a truth or fact. In the computer age, ""document"" usually denotes a primarily textual computer file, including its structure and format, e.g. fonts, colors, and images. Contemporarily, ""document"" is not defined by its transmission medium, e.g., paper, given the existence of electronic documents. ""Documentation"" is distinct because it has more denotations than ""document"". Documents are also distinguished from ""realia"", which are three-dimensional objects that would otherwise satisfy the definition of ""document"" because they memorialize or represent thought; documents are considered more as 2-dimensional representations. While documents can have large varieties of customization, all documents can be shared freely and have the right to do so, creativity can be represented by documents, also. History, events, examples, opinions, etc. all can be expressed in documents."
Electronic document,"An electronic document is any electronic media content (other than computer programs or system files) that is intended to be used in either an electronic form or as printed output. Originally, any computer data were considered as something internal — the final data output was always on paper. However, the development of computer networks has made it so that in most cases it is much more convenient to distribute electronic documents than printed ones. The improvements in electronic visual display technologies made it possible to view documents on screen instead of printing them (thus saving paper and the space required to store the printed copies).However, using electronic documents for final presentation instead of paper has created the problem of multiple incompatible file formats. Even plain text computer files are not free from this problem — e.g. under MS-DOS, most programs could not work correctly with UNIX-style text files (see newline), and for non-English speakers, the different code pages always have been a source of trouble.
Even more problems are connected with complex file formats of various word processors, spreadsheets, and graphics software. To alleviate the problem, many software companies distribute free file viewers for their proprietary file formats (one example is Adobe's Acrobat Reader). The other solution is the development of standardized non-proprietary file formats (such as HTML and OpenDocument), and electronic documents for specialized uses have specialized formats – the specialized electronic articles in physics use TeX or PostScript."
Vector space model,"Vector space model or term vector model is an algebraic model for representing text documents (and any objects, in general) as vectors of identifiers (such as index terms). It is used in information filtering, information retrieval, indexing and relevancy rankings.  Its first use was in the SMART Information Retrieval System.

"
Here document,"In computing, a here document (here-document, here-text, heredoc, hereis, here-string or here-script) is a file literal or input stream literal: it is a section of a source code file that is treated as if it were a separate file. The term is also used for a form of multiline string literals that use similar syntax, preserving line breaks and other whitespace (including indentation) in the text.
Here documents originate in the Unix shell, and are found in the Bourne shell (sh), C shell (csh), tcsh (tcsh), KornShell (ksh), Bourne Again Shell (bash), and Z shell (zsh), among others. Here document-style string literals are found in various high-level languages, notably the Perl programming language (syntax inspired by Unix shell) and languages influenced by Perl, such as PHP and Ruby. JavaScript also supports this functionality via template literals, a feature added in its 6th revision (ES6). Other high-level languages such as Python, Julia and Tcl have other facilities for multiline strings.
Here documents can be treated either as files or strings. Some shells treat them as a format string literal, allowing variable substitution and command substitution inside the literal.
The most common syntax for here documents, originating in Unix shells, is << followed by a delimiting identifier (often the word EOF or END), followed, starting on the next line, by the text to be quoted, and then closed by the same delimiting identifier on its own line. This syntax is because here documents are formally stream literals, and the content of the document is redirected to stdin (standard input) of the preceding command; the here document syntax is by analogy with the syntax for input redirection, which is < “take input from the following file”.
Other languages often use substantially similar syntax, but details of syntax and actual functionality can vary significantly. When used simply for string literals, the << does not indicate indirection, but is simply a starting delimiter convention. In some languages, such as Ruby, << is also used for input redirection, thus resulting in << being used twice if one wishes to redirect from a here document string literal."
Latent semantic analysis,"Latent semantic analysis (LSA) is a technique in natural language processing, in particular distributional semantics, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms.  LSA assumes that words that are close in meaning will occur in similar pieces of text (the distributional hypothesis).  A matrix containing word counts per document (rows represent unique words and columns represent each document) is constructed from a large piece of text and a mathematical technique called singular value decomposition (SVD) is used to reduce the number of rows while preserving the similarity structure among columns.  Documents are then compared by cosine similarity between any two columns.  Values close to 1 represent very similar documents while values close to 0 represent very dissimilar documents.An information retrieval technique using latent semantic structure was patented in 1988 (US Patent 4,839,853, now expired) by Scott Deerwester, Susan Dumais, George Furnas, Richard Harshman, Thomas Landauer, Karen Lochbaum and Lynn Streeter. In the context of its application to information retrieval, it is sometimes called latent semantic indexing (LSI).

"
XML,"Extensible Markup Language (XML) is a markup language and file format for storing, transmitting, and reconstructing arbitrary data. It defines a set of rules for encoding documents in a format that is both human-readable and machine-readable. The World Wide Web Consortium's XML 1.0 Specification of 1998 and several other related specifications—all of them free open standards—define XML.The design goals of XML emphasize simplicity, generality, and usability across the Internet. It is a textual data format with strong support via Unicode for different human languages. Although the design of XML focuses on documents, the language is widely used for the representation of arbitrary data structures such as those used in web services.
Several schema systems exist to aid in the definition of XML-based languages, while programmers have developed many application programming interfaces (APIs) to aid the processing of XML data."
Form (document),"A form is a document with spaces (also named fields or placeholders) in which to write or select, for a  series of documents with similar contents. The documents usually have the printed parts in common, except, possibly, for a serial number. 
Forms, when completed, may be a statement, a request, an order, etc.; a check may be a form. Also there are forms for taxes; filling one in is a duty to have determined how much tax one owes, and/or the form is a request for a refund. See also Tax return.
Forms may be filled out in duplicate (or triplicate, meaning three times) when the information gathered on the form needs to be distributed to several departments within an organization. This can be done using carbon paper.

"
Tf–idf,"In information retrieval, tf–idf (also TF*IDF, TFIDF, TF–IDF, or Tf–idf), short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling.
The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. tf–idf is one of the most popular term-weighting schemes today. A survey conducted in 2015 showed that 83% of text-based recommender systems in digital libraries use tf–idf.Variations of the tf–idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query. tf–idf can be successfully used for stop-words filtering in various subject fields, including text summarization and classification.
One of the simplest ranking functions is computed by summing the tf–idf for each query term; many more sophisticated ranking functions are variants of this simple model."
Multiple-document interface,"A multiple-document interface (MDI) is a graphical user interface in which multiple windows reside under a single parent window. Such systems often allow child windows to embed other windows inside them as well, creating complex nested hierarchies. This contrasts with single-document interfaces (SDI) where all windows are independent of each other.

"
Document 12-571-3570,"Document 12-571-3570 (also titled NASA No. 12 571-3570) is a hoax document originally posted to the Usenet newsgroup alt.sex on November 28, 1989. According to this document, astronauts aboard Space Shuttle mission STS-75 performed a variety of sex acts to determine which positions are most effective in zero gravity. The document goes on to report that of the 10 positions tested, six required the use of a belt and an inflatable tunnel, while four were contingent on hanging on. The document also discusses a video record of the 10 one-hour sessions in the lower deck of the shuttle, and notes that the subjects added their own personal footnotes to help scientists.
The real STS-75 mission occurred in 1996, 7 years after the text was published, clearly indicating that the document is a hoax. Nonetheless, many people have been fooled by this document and NASA has had to debunk it on several occasions. In March 2000, NASA's director of media services Brian Welch referred to the document as a ""fairly well-known 'urban legend'"".This fictional document was rediscovered and widely publicized by astronomer and scientific writer Pierre Kohler, who used it as a major source about sex experiments in space in his 2000 book, The Final Mission. Kohler conceded in his book that astronauts are mute on the subject of human sex in orbit, even if they have conducted reproduction research on South African frogs and Japanese fish."
probabilistic learning method,"In machine learning, a probabilistic classifier is a classifier that is able to predict, given an observation of an input, a probability distribution over a set of classes, rather than only outputting the most likely class that the observation should belong to. Probabilistic classifiers provide classification that can be useful in its own right or when combining classifiers into ensembles."
Probabilistic classification,"In machine learning, a probabilistic classifier is a classifier that is able to predict, given an observation of an input, a probability distribution over a set of classes, rather than only outputting the most likely class that the observation should belong to. Probabilistic classifiers provide classification that can be useful in its own right or when combining classifiers into ensembles."
Learning rate,"In machine learning and statistics, the learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function. Since it influences to what extent newly acquired information overrides old information, it metaphorically represents the speed at which a machine learning model ""learns"". In the adaptive control literature, the learning rate is commonly referred to as gain.In setting a learning rate, there is a trade-off between the rate of convergence and overshooting. While the descent direction is usually determined from the gradient of the loss function, the learning rate determines how big a step is taken in that direction. A too high learning rate will make the learning jump over minima but a too low learning rate will either take too long to converge or get stuck in an undesirable local minimum.In order to achieve faster convergence, prevent oscillations and getting stuck in undesirable local minima the learning rate is often varied during training either in accordance to a learning rate schedule or by using an adaptive learning rate. The learning rate and its adjustments may also differ per parameter, in which case it is a diagonal matrix that can be interpreted as an approximation to the inverse of the Hessian matrix in Newton's method. The learning rate is related to the step length determined by inexact line search in quasi-Newton methods and related optimization algorithms.When conducting line searches, mini-batch sub-sampling (MBSS) affect the characteristics of the loss function along which the learning rate needs to be resolved. Static MBSS keeps the mini-batch fixed along a search direction, resulting in a smooth loss function along the search direction. Dynamic MBSS updates the mini-batch at every function evaluation, resulting in a point-wise discontinuous loss function along the search direction. Line searches that adaptively resolve learning rates for static MBSS loss functions include the parabolic approximation line (PAL) search. Line searches that adaptively resolve learning rates for dynamic MBSS loss functions include probabilistic line searches, gradient-only line searches (GOLS) and quadratic approximations."
Reinforcement learning,"Reinforcement learning (RL) is an area of machine learning concerned with how intelligent agents ought to take actions in an environment in order to maximize the notion of cumulative reward. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning.
Reinforcement learning differs from supervised learning in not needing labelled input/output pairs to be presented, and in not needing sub-optimal actions to be explicitly corrected. Instead the focus is on finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge).
The environment is typically stated in the form of a Markov decision process (MDP), because many reinforcement learning algorithms for this context use dynamic programming techniques. The main difference between the classical dynamic programming methods  and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the MDP and they target large MDPs where exact methods become infeasible."
Probabilistic programming,"Probabilistic programming (PP) is a programming paradigm in which probabilistic models are specified and inference for these models is performed automatically. It represents an attempt to unify probabilistic modeling and traditional general purpose programming in order to make the former easier and more widely applicable. It can be used to create systems that help make decisions in the face of uncertainty.
Programming languages used for probabilistic programming are referred to as ""probabilistic programming languages"" (PPLs).

"
Unsupervised learning,"Supervised learning (SL) is a machine learning paradigm for problems where the available data consists of labelled examples, meaning that each data point contains features (covariates) and an associated label. The goal of supervised learning algorithms is learning a function that maps feature vectors (inputs) to labels (output), based on example input-output pairs. It infers a function from labeled training data consisting of a set of training examples. In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a ""reasonable"" way (see inductive bias). This statistical quality of an algorithm is measured through the so-called generalization error."
Outline of machine learning,"The following outline is provided as an overview of and topical guide to machine learning. Machine learning is a subfield of soft computing within computer science that evolved from the study of pattern recognition and computational learning theory in artificial intelligence. In 1959, Arthur Samuel defined machine learning as a ""field of study that gives computers the ability to learn without being explicitly programmed"". Machine learning explores the study and construction of algorithms that can learn from and make predictions on data. Such algorithms operate by building a model from an example training set of input observations in order to make data-driven predictions or decisions expressed as outputs, rather than following strictly static program instructions."
multinomial model,"In statistics, multinomial logistic regression is a classification method that generalizes logistic regression to multiclass problems, i.e. with more than two possible discrete outcomes.  That is, it is a model that is used to predict the probabilities of the different possible outcomes of a categorically distributed dependent variable, given a set of independent variables (which may be real-valued, binary-valued, categorical-valued, etc.).
Multinomial logistic regression is known by a variety of other names, including polytomous LR, multiclass LR, softmax regression, multinomial logit (mlogit), the maximum entropy (MaxEnt) classifier, and the conditional maximum entropy model."
Multinomial logistic regression,"In statistics, multinomial logistic regression is a classification method that generalizes logistic regression to multiclass problems, i.e. with more than two possible discrete outcomes.  That is, it is a model that is used to predict the probabilities of the different possible outcomes of a categorically distributed dependent variable, given a set of independent variables (which may be real-valued, binary-valued, categorical-valued, etc.).
Multinomial logistic regression is known by a variety of other names, including polytomous LR, multiclass LR, softmax regression, multinomial logit (mlogit), the maximum entropy (MaxEnt) classifier, and the conditional maximum entropy model."
Multinomial distribution,"In probability theory, the multinomial distribution is a generalization of the binomial distribution. For example, it models the probability of counts for each side of a k-sided dice rolled n times. For n independent trials each of which leads to a success for exactly one of k categories, with each category having a given fixed success probability, the multinomial distribution gives the probability of any particular combination of numbers of successes for the various categories.
When k is 2 and n is 1, the multinomial distribution is the Bernoulli distribution. When k is 2 and n is bigger than 1, it is the binomial distribution. When k is bigger than 2 and n is 1, it is the categorical distribution. The term ""multinoulli"" is sometimes used for the categorical distribution to emphasize this four-way relationship (so n determines the prefix, and k the suffix).
The Bernoulli distribution models the outcome of a single Bernoulli trial. In other words, it models whether flipping a (possibly biased) coin one time will result in either a success (obtaining a head) or failure (obtaining a tail). The binomial distribution generalizes this to the number of heads from performing n independent flips (Bernoulli trials) of the same coin. The multinomial distribution models the outcome of n experiments, where the outcome of each trial has a categorical distribution, such as rolling a k-sided dice n times.
Let k be a fixed finite number. Mathematically, we have k possible mutually exclusive outcomes, with corresponding probabilities p1, ..., pk, and n independent trials. Since the k outcomes are mutually exclusive and one must occur we have pi ≥ 0 for i = 1, ..., k and 
  
    
      
        
          ∑
          
            i
            =
            1
          
          
            k
          
        
        
          p
          
            i
          
        
        =
        1
      
    
    {\displaystyle \sum _{i=1}^{k}p_{i}=1}
  .  Then if the random variables Xi indicate the number of times outcome number i is observed over the n trials, the vector X = (X1, ..., Xk) follows a multinomial distribution with parameters n and p, where p = (p1, ..., pk). While the trials are independent, their outcomes Xi are dependent because they must be summed to n."
Naive Bayes classifier,"In statistics, naive Bayes classifiers are a family of simple ""probabilistic classifiers"" based on applying Bayes' theorem with strong (naive) independence assumptions between the features (see Bayes classifier). They are among the simplest Bayesian network models, but coupled with kernel density estimation, they can achieve high accuracy levels.Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features/predictors) in a learning problem. Maximum-likelihood training can be done by evaluating a closed-form expression,: 718  which takes linear time, rather than by expensive iterative approximation as used for many other types of classifiers.
In the statistics literature, naive Bayes models are known under a variety of names, including simple Bayes and independence Bayes. All these names reference the use of Bayes' theorem in the classifier's decision rule, but naive Bayes is not (necessarily) a Bayesian method."
Multinomial probit,"In statistics and econometrics, the multinomial probit model is a generalization of the probit model used when there are several possible categories that the dependent variable can fall into. As such, it is an alternative to the multinomial logit model  as one method of multiclass classification. It is not to be confused with the multivariate probit model, which is used to model correlated binary outcomes for more than one independent variable."
Latent variable model,"A latent variable model is a statistical model that relates a set of observable variables (also called manifest variables or indicators) to a set of latent variables.
It is assumed that the responses on the indicators or manifest variables are the result of an individual's position on the latent variable(s), and that the manifest variables have nothing in common after controlling for the latent variable (local independence).
Different types of the latent variable models can be grouped according to whether the manifest and latent variables are categorical or continuous:
The Rasch model represents the simplest form of item response theory.  Mixture models are central to latent profile analysis.
In factor analysis and latent trait analysis the latent variables are treated  as continuous normally distributed variables, and in latent profile analysis  and latent class analysis as from a multinomial distribution.  The manifest variables in factor analysis and latent profile analysis  are continuous and in most cases, their conditional distribution given the latent variables  is assumed to be normal. In latent trait analysis and latent class analysis,  the manifest variables are discrete. These variables could be dichotomous, ordinal or nominal variables.  Their conditional distributions are assumed to be binomial or multinomial.
Because the distribution of a continuous latent variable can be approximated by a discrete distribution,  the distinction between continuous and discrete variables turns out not to be fundamental at all. Therefore, there may be a psychometrical latent variable, but not a psychological psychometric variable.Give example of ""psychometrical latent variable"" and ""psychological psychometric variable"""
Dirichlet-multinomial distribution,"In probability theory and statistics, the Dirichlet-multinomial distribution is a family of discrete multivariate probability distributions on a finite support of non-negative integers. It is also called the Dirichlet compound multinomial distribution (DCM) or multivariate Pólya distribution (after George Pólya). It is a compound probability distribution, where a probability vector p is drawn from a Dirichlet distribution with parameter vector 
  
    
      
        
          α
        
      
    
    {\displaystyle {\boldsymbol {\alpha }}}
  , and an observation drawn from a multinomial distribution with probability vector p and number of trials n. The Dirichlet parameter vector captures the prior belief about the situation and can be seen as a pseudocount: observations of each outcome that occur before the actual data is collected. The compounding corresponds to a Pólya urn scheme. It is frequently encountered in Bayesian statistics, machine learning, empirical Bayes methods and classical statistics as an overdispersed multinomial distribution.
It reduces to the categorical distribution as a special case when n = 1.  It also approximates the multinomial distribution arbitrarily well for large α. The Dirichlet-multinomial is a multivariate extension of the beta-binomial distribution, as the multinomial and Dirichlet distributions are multivariate versions of the binomial distribution and beta distributions, respectively."
Generalized linear model,"In statistics, a generalized linear model (GLM) is a flexible generalization of ordinary linear regression. The GLM generalizes linear regression by allowing the linear model to be related to the response variable via a link function and by allowing the magnitude of the variance of each measurement to be a function of its predicted value.
Generalized linear models were formulated by John Nelder and Robert Wedderburn as a way of unifying various other statistical models, including linear regression, logistic regression and Poisson regression. They proposed an iteratively reweighted least squares method for maximum likelihood estimation (MLE) of the model parameters. MLE remains popular and is the default method on many statistical computing packages. Other approaches, including Bayesian regression and least squares fitting to variance stabilized responses, have been developed."
Fixed effects model,"In statistics, a fixed effects model is a statistical model in which the model parameters are fixed or non-random quantities. This is in contrast to random effects models and mixed models in which all or some of the model parameters are random variables. In many applications including econometrics and biostatistics a fixed effects model refers to a regression model in which the group means are fixed (non-random) as opposed to a random effects model in which the group means are a random sample from a population. Generally, data can be grouped according to several observed factors. The group means could be modeled as fixed or random effects for each grouping. In a fixed effects model each group mean is a group-specific fixed quantity.
In panel data where longitudinal observations exist for the same subject, fixed effects represent the subject-specific means. In panel data analysis the term fixed effects estimator (also known as the within estimator) is used to refer to an estimator for the coefficients in the regression model including those fixed effects (one time-invariant intercept for each subject)."
Multilevel model,"Multilevel models (also known as hierarchical linear models, linear mixed-effect model, mixed models, nested data models, random coefficient, random-effects models, random parameter models, or split-plot designs) are statistical models of parameters that vary at more than one level. An example could be a model of student performance that contains measures for individual students as well as measures for classrooms within which the students are grouped. These models can be seen as generalizations of linear models (in particular, linear regression), although they can also extend to non-linear models.  These models became much more popular after sufficient computing power and software became available.Multilevel models are particularly appropriate for research designs where data for participants are organized at more than one level (i.e., nested data). The units of analysis are usually individuals (at a lower level) who are nested within contextual/aggregate units (at a higher level). While the lowest level of data in multilevel models is usually an individual, repeated measurements of individuals may also be examined. As such, multilevel models provide an alternative type of analysis for univariate or multivariate analysis of repeated measures. Individual differences in growth curves may be examined. Furthermore, multilevel models can be used as an alternative to ANCOVA, where scores on the dependent variable are adjusted for covariates (e.g. individual differences) before testing treatment differences. Multilevel models are able to analyze these experiments without the assumptions of homogeneity-of-regression slopes that is required by ANCOVA.Multilevel models can be used on data with many levels, although 2-level models are the most common and the rest of this article deals only with these. The dependent variable must be examined at the lowest level of analysis."
Random effects model,"In statistics, a random effects model, also called a variance components model, is a statistical model where the model parameters are random variables. It is a kind of hierarchical linear model, which assumes that the data being analysed are drawn from a hierarchy of different populations whose differences relate to that hierarchy. A random effects model is a special case of a mixed model.
Contrast this to the biostatistics definitions, as biostatisticians use ""fixed"" and ""random"" effects to respectively refer to the population-average and subject-specific effects (and where the latter are generally assumed to be unknown, latent variables)."
vocabulary,"A vocabulary is a set of familiar words within a person's language. A vocabulary, usually developed with age, serves as a useful and fundamental tool for communication and acquiring knowledge. Acquiring an extensive vocabulary is one of the largest challenges in learning a second language."
Vocabulary,"A vocabulary is a set of familiar words within a person's language. A vocabulary, usually developed with age, serves as a useful and fundamental tool for communication and acquiring knowledge. Acquiring an extensive vocabulary is one of the largest challenges in learning a second language."
English language,"English is a West Germanic language of the Indo-European language family, with its earliest forms spoken by the inhabitants of early medieval England. It is named after the Angles, one of the ancient Germanic peoples that migrated to the island of Great Britain. English is genealogically West Germanic, closest related to the Low Saxon and Frisian languages; however, its vocabulary is also distinctively influenced by dialects of French (about 29% of modern English words) and Latin (also about 29%), plus some grammar and a small amount of core vocabulary influenced by Old Norse (a North Germanic language). Speakers of English are called Anglophones.
The earliest forms of English, collectively known as Old English, evolved from a group of West Germanic (Ingvaeonic) dialects brought to Great Britain by Anglo-Saxon settlers in the 5th century and further mutated by Norse-speaking Viking settlers starting in the 8th and 9th centuries. Middle English began in the late 11th century after the Norman conquest of England, when considerable French (especially Old Norman) and Latin-derived vocabulary was incorporated into English over some three hundred years. Early Modern English began in the late 15th century with the start of the Great Vowel Shift and the Renaissance trend of borrowing further Latin and Greek words and roots into English, concurrent with the introduction of the printing press to London. This era notably culminated in the King James Bible and plays of William Shakespeare.Modern English grammar is the result of a gradual change from a typical Indo-European dependent-marking pattern, with a rich inflectional morphology and relatively free word order, to a mostly analytic pattern with little inflection, and a fairly fixed subject–verb–object word order. Modern English relies more on auxiliary verbs and word order for the expression of complex tenses, aspect and mood, as well as passive constructions, interrogatives and some negation.
Modern English has spread around the world since the 17th century as a consequence of the worldwide influence of the British Empire and the United States of America. Through all types of printed and electronic media of these countries, English has become the leading language of international discourse and the lingua franca in many regions and professional contexts such as science, navigation and law. English is the most spoken language in the world and the third-most spoken native language in the world, after Standard Chinese and Spanish. It is the most widely learned second language and is either the official language or one of the official languages in 59 sovereign states. There are more people who have learned English as a second language than there are native speakers. As of 2005, it was estimated that there were over 2 billion speakers of English. English is the majority native language in the United Kingdom, the United States, Canada, Australia, New Zealand and the Republic of Ireland (see Anglosphere), and is widely spoken in some areas of the Caribbean, Africa, South Asia, Southeast Asia, and Oceania. It is a co-official language of the United Nations, the European Union and many other world and regional international organisations. It is the most widely spoken Germanic language, accounting for at least 70% of speakers of this Indo-European branch."
Vocabulary development,"Vocabulary development is a process by which people acquire words. Babbling shifts towards meaningful speech as infants grow and produce their first words around the age of one year. In early word learning, infants build their vocabulary slowly. By the age of 18 months, infants can typically produce about 50 words and begin to make word combinations.
In order to build their vocabularies, infants must learn about the meanings that words carry. The mapping problem asks how infants correctly learn to attach words to referents. Constraints theories, domain-general views, social-pragmatic accounts, and an emergentist coalition model have been proposed to account for the mapping problem.
From an early age, infants use language to communicate. Caregivers and other family members use language to teach children how to act in society. In their interactions with peers, children have the opportunity to learn about unique conversational roles. Through pragmatic directions, adults often offer children cues for understanding the meaning of words.
Throughout their school years, children continue to build their vocabulary. In particular, children begin to learn abstract words. Beginning around age 3–5, word learning takes place both in conversation and through reading. Word learning often involves physical context, builds on prior knowledge, takes place in social context, and includes semantic support. The phonological loop and serial order short-term memory may both play an important role in vocabulary development."
Japanese language,"Japanese (日本語, Nihongo, [ɲihoŋɡo] (listen)) is spoken natively by about 128 million people, primarily by Japanese people and primarily in Japan, the only country where it is the national language. Japanese belongs to the Japonic or Japanese-Ryukyuan language family. There have been many attempts to group the Japonic languages with other families such as the Ainu, Austroasiatic, Koreanic, and the now-discredited Altaic, but none of these proposals has gained widespread acceptance.
Little is known of the language's prehistory, or when it first appeared in Japan. Chinese documents from the 3rd century AD recorded a few Japanese words, but substantial Old Japanese texts did not appear until the 8th century. From the Heian period (794–1185), there was a massive influx of Sino-Japanese vocabulary into the language, affecting the phonology of Early Middle Japanese. Late Middle Japanese (1185–1600) saw extensive grammatical changes and the first appearance of European loanwords. The basis of the standard dialect moved from the Kansai region to the Edo region (modern Tokyo) in the Early Modern Japanese period (early 17th century–mid 19th century). Following the end of Japan's self-imposed isolation in 1853, the flow of loanwords from European languages increased significantly, and words from English roots have proliferated.
Japanese is an agglutinative, mora-timed language with relatively simple phonotactics, a pure vowel system, phonemic vowel and consonant length, and a lexically significant pitch-accent. Word order is normally subject–object–verb with particles marking the grammatical function of words, and sentence structure is topic–comment. Sentence-final particles are used to add emotional or emphatic impact, or form questions. Nouns have no grammatical number or gender, and there are no articles. Verbs are conjugated, primarily for tense and voice, but not person. Japanese adjectives are also conjugated. Japanese has a complex system of honorifics, with verb forms and vocabulary to indicate the relative status of the speaker, the listener, and persons mentioned.
The Japanese writing system combines Chinese characters, known as kanji (漢字, 'Han characters'), with two unique syllabaries (or moraic scripts) derived by the Japanese from the more complex Chinese characters: hiragana (ひらがな or 平仮名, 'simple characters') and katakana (カタカナ or 片仮名, 'partial characters'). Latin script (rōmaji ローマ字) is also used in a limited fashion (such as for imported acronyms) in Japanese writing. The numeral system uses mostly Arabic numerals, but also traditional Chinese numerals."
Korean language,"Korean (South Korean: 한국어, hangugeo; North Korean: 조선말, chosŏnmal) is the native language for about 80 million people, mostly of Korean descent. It is the official and national language of both North Korea and South Korea (geographically Korea), but over the past 74 years of political division, the two Koreas have developed some noticeable vocabulary differences. Beyond Korea, the language is recognised as a minority language in parts of China, namely Jilin Province, and specifically Yanbian Prefecture and Changbai County. It is also spoken by Sakhalin Koreans in parts of Sakhalin, the Russian island just north of Japan, and by the Koryo-saramcode: kor promoted to code: ko  in parts of Central Asia. The language has a few extinct relatives which—along with the Jeju language (Jejuan) of Jeju Island and Korean itself—form the compact Koreanic language family. Even so, Jejuan and Korean are not mutually intelligible with each other. The linguistic homeland of Korean is suggested to be somewhere in contemporary Northeast China. The hierarchy of the society from which the language originates deeply influences the language, leading to a system of speech levels and honorifics indicative of the formality of any given situation.
Modern Korean is written in the Korean script (한글; Hangul in South Korea, 조선글; Chosŏn'gŭl in North Korea), a system developed during the 15th century for that purpose, although it did not become the primary script until the 20th century. The script uses 24 basic letters (jamo) and 27 complex letters formed from the basic ones. When first recorded in historical texts, Korean was only a spoken language; all written records were maintained in Classical Chinese, which, even when spoken, is not intelligible to someone who speaks only Korean. Later, Chinese characters adapted to the Korean language, Hanja (漢字), were used to write the language for most of Korea's history and are still used to a limited extent in South Korea, most prominently in the humanities and the study of historical texts.
Since the turn of the 21st century, aspects of Korean culture have spread to other countries through globalization and cultural exports. As such, interest in Korean language acquisition (as a foreign language) is also generated by longstanding alliances, military involvement, and diplomacy, such as between South Korea–United States, China–North Korea and North Korea–Russia since the end of World War II and the Korean War. Along with other languages such as Chinese and Arabic, Korean is ranked at the top difficulty level for English speakers by the U.S. Department of Defense.

"
French language,"French (français [fʁɑ̃sɛ] or langue française [lɑ̃ɡ fʁɑ̃sɛːz]) is a Romance language of the Indo-European family. It descended from the Vulgar Latin of the Roman Empire, as did all Romance languages. French evolved from Gallo-Romance, the Latin spoken in Gaul, and more specifically in Northern Gaul. Its closest relatives are the other langues d'oïl—languages historically spoken in northern France and in southern Belgium, which French (Francien) largely supplanted. French was also influenced by native Celtic languages of Northern Roman Gaul like Gallia Belgica and by the (Germanic) Frankish language of the post-Roman Frankish invaders. Today, owing to France's past overseas expansion, there are numerous French-based creole languages, most notably Haitian Creole. A French-speaking person or nation may be referred to as Francophone in both English and French.
French is an official language in 29 countries across multiple continents, most of which are members of the Organisation internationale de la Francophonie (OIF), the community of 84 countries which share the official use or teaching of French. French is also one of six official languages used in the United Nations. It is spoken as a first language (in descending order of the number of speakers) in France; Canada (especially in the provinces of Quebec, Ontario, and New Brunswick, as well as other Francophone regions); Belgium (Wallonia and the Brussels-Capital Region); western Switzerland (specifically the cantons forming the Romandy region); parts of Luxembourg; parts of the United States (the states of Louisiana, Maine, New Hampshire and Vermont); Monaco; the Aosta Valley region of Italy; and various communities elsewhere.In 2015, approximately 40% of the francophone population (including L2 and partial speakers) lived in Europe, 36% in sub-Saharan Africa and the Indian Ocean, 15% in North Africa and the Middle East, 8% in the Americas, and 1% in Asia and Oceania. French is the second most widely spoken mother tongue in the European Union. Of Europeans who speak other languages natively, approximately one-fifth are able to speak French as a second language. French is the second most taught foreign language in the EU. All institutions of the EU use French as a working language along with English and German; in certain institutions, French is the sole working language (e.g. at the Court of Justice of the European Union). French is also the 18th most natively spoken language in the world, fifth most spoken language by total number of speakers and the second or third most studied language worldwide (with about 120 million learners as of 2017). As a result of French and Belgian colonialism from the 16th century onward, French was introduced to new territories in the Americas, Africa and Asia. Most second-language speakers reside in Francophone Africa, in particular Gabon, Algeria, Morocco, Tunisia, Mauritius, Senegal and Ivory Coast.French is estimated to have about 76 million native speakers; about 235 million daily, fluent speakers; and another 77–110 million secondary speakers who speak it as a second language to varying degrees of proficiency, mainly in Africa. According to the OIF, approximately 321 million people worldwide are ""able to speak the language"", without specifying the criteria for this estimation or whom it encompasses. According to a demographic projection led by the Université Laval and the Réseau Démographie de l'Agence universitaire de la Francophonie, the total number of French speakers will reach approximately 500 million in 2025 and 650 million by 2050. OIF estimates 700 million by 2050, 80% of whom will be in Africa.French has a long history as an international language of literature and scientific standards and is a primary or second language of many international organisations including the United Nations, the European Union, the North Atlantic Treaty Organization, the World Trade Organization, the International Olympic Committee, and the International Committee of the Red Cross. In 2011, Bloomberg Businessweek ranked French the third most useful language for business, after English and Standard Mandarin Chinese."
Dutch language,"Dutch (Nederlands [ˈneːdərlɑnts] (listen)) is a West Germanic language spoken by about 25 million people as a first language and 5 million as a second language. It is the third most widely spoken Germanic language, after its close relatives  German and English. Afrikaans is a separate but somewhat mutually intelligible daughter language spoken, to some degree, by at least 16 million people, mainly in South Africa and Namibia, evolving from the Cape Dutch dialects of Southern Africa. The dialects used in Belgium (including Flemish) and in Suriname, meanwhile, are all guided by the Dutch Language Union.
In Europe, most of the population of the Netherlands (where it is the only official language spoken countrywide) and about 60% of the population of Belgium (as one of three official languages) speak Dutch. Outside the Low Countries, Dutch is the native language of the majority of the population of the South American country of Suriname, a former Dutch colony, where it also holds an official status, as it does in the Caribbean island countries of Aruba, Curaçao and Sint Maarten, which are constituent countries of the Kingdom of the Netherlands.  Up to half a million native speakers reside in the United States, Canada and Australia combined, and historical linguistic minorities on the verge of extinction remain in parts of France, Germany and Indonesia.Dutch is one of the closest relatives of both German and English and is colloquially said to be ""roughly in between"" them. Dutch, like English, has not undergone the High German consonant shift, does not use Germanic umlaut as a grammatical marker, has largely abandoned the use of the subjunctive, and has levelled much of its morphology, including most of its case system. Features shared with German include the survival of two to three grammatical genders—albeit with few grammatical consequences—as well as the use of modal particles, final-obstruent devoicing, and a similar word order. Dutch vocabulary is mostly Germanic; it incorporates slightly more Romance loans than German but far fewer than English."
Bengali vocabulary,"Bengali (বাংলা Bangla) is one of the Eastern Indo-Aryan (Magadhan) languages, evolved from Magadhi Prakrit and Pali languages native to the Indian subcontinent. The core of Bengali vocabulary is thus etymologically of Magadhi Prakrit and Pali languages. However, centuries of major borrowing and reborrowing from Arabic, Persian, Turkish, Sanskrit, Austroasiatic languages and other languages has led to the adoption of a wide range of words with foreign origins; thus making the origins of borrowed and re-borrowed words in the Bengali vocabulary numerous and diverse, due to centuries of contact with various languages."
Singlish vocabulary,"Singlish is the English-based creole or patois spoken colloquially in Singapore. English is one of Singapore's official languages, along with Malay (which is also the National Language), Mandarin, and Tamil. Although English is the lexifier language, Singlish has its unique slang and syntax, which are more pronounced in informal speech. It is usually a mixture of English, Mandarin, Tamil, Malay, and other local dialects like Hokkien, Cantonese or Teochew. There are a few loan words from these languages i.e. pek chek is often taken as being annoyed or frustrated and originate from the Hokkien dialect. It is used in casual contexts between Singaporeans, but is avoided in formal events when certain Singlish phrases may be considered unedifying. Singapore English can be broken into two subcategories. Standard Singapore English (SSE) and Colloquial Singapore English (CSE) or Singlish as many locals call it. The relationship between SSE and Singlish is viewed as a diglossia, in which SSE is restricted to be used in situations of formality where Singlish/CSE is used in most other circumstances.Some of the most popular Singlish terms have been added to the Oxford English Dictionary (OED) since 2000, including wah, sabo, lepak, shiok and hawker centre. On 11 February 2015, kiasu was chosen as OED's Word of the Day."
XML,"Extensible Markup Language (XML) is a markup language and file format for storing, transmitting, and reconstructing arbitrary data. It defines a set of rules for encoding documents in a format that is both human-readable and machine-readable. The World Wide Web Consortium's XML 1.0 Specification of 1998 and several other related specifications—all of them free open standards—define XML.The design goals of XML emphasize simplicity, generality, and usability across the Internet. It is a textual data format with strong support via Unicode for different human languages. Although the design of XML focuses on documents, the language is widely used for the representation of arbitrary data structures such as those used in web services.
Several schema systems exist to aid in the definition of XML-based languages, while programmers have developed many application programming interfaces (APIs) to aid the processing of XML data."
documents,"A document is a written, drawn, presented, or memorialized representation of thought, often the manifestation of non-fictional, as well as fictional, content.  The word originates from the Latin Documentum, which denotes a ""teaching"" or ""lesson"": the verb doceō denotes ""to teach"". In the past, the word was usually used to denote written proof useful as evidence of a truth or fact. In the computer age, ""document"" usually denotes a primarily textual computer file, including its structure and format, e.g. fonts, colors, and images. Contemporarily, ""document"" is not defined by its transmission medium, e.g., paper, given the existence of electronic documents. ""Documentation"" is distinct because it has more denotations than ""document"". Documents are also distinguished from ""realia"", which are three-dimensional objects that would otherwise satisfy the definition of ""document"" because they memorialize or represent thought; documents are considered more as 2-dimensional representations. While documents can have large varieties of customization, all documents can be shared freely and have the right to do so, creativity can be represented by documents, also. History, events, examples, opinions, etc. all can be expressed in documents."
Document,"A document is a written, drawn, presented, or memorialized representation of thought, often the manifestation of non-fictional, as well as fictional, content.  The word originates from the Latin Documentum, which denotes a ""teaching"" or ""lesson"": the verb doceō denotes ""to teach"". In the past, the word was usually used to denote written proof useful as evidence of a truth or fact. In the computer age, ""document"" usually denotes a primarily textual computer file, including its structure and format, e.g. fonts, colors, and images. Contemporarily, ""document"" is not defined by its transmission medium, e.g., paper, given the existence of electronic documents. ""Documentation"" is distinct because it has more denotations than ""document"". Documents are also distinguished from ""realia"", which are three-dimensional objects that would otherwise satisfy the definition of ""document"" because they memorialize or represent thought; documents are considered more as 2-dimensional representations. While documents can have large varieties of customization, all documents can be shared freely and have the right to do so, creativity can be represented by documents, also. History, events, examples, opinions, etc. all can be expressed in documents."
False document,A false document is a technique by which an author aims to increase verisimilitude in a work of fiction by inventing and inserting or mentioning documents that appear to be factual. The goal of a false document is to convince an audience that what is being presented is factual.
Book of Documents,"The Book of Documents (Shūjīng, earlier Shu King) or Classic of History, also known as the Shangshu (“Venerated Documents”), is one of the Five Classics of ancient Chinese literature.  It is a collection of rhetorical prose attributed to figures of ancient China, and served as the foundation of Chinese political philosophy for over 2,000 years.
The Book of Documents was the subject of one of China's oldest literary controversies, between proponents of different versions of the text. A version was preserved from Qin Shi Huang's burning of books and burying of scholars by scholar Fu Sheng, in 29 sections (pian 篇). This group of texts were referred to as ""Modern Script"" jinwen 今文, because written with the script in use at the beginning of the Western Han dyansty. According to Western Han dynasty documents, new textual material was  discovered in the wall of Confucius' family estate in Qufu by his descendant Kong Anguo in the late 2nd century BC. This new material was referred to as ""Old Script"" guwen 古文, because written in the script that predated the standardization of Chinese script enforced during the Qing dynasty. Compared to the ""Modern Script"" texts, the ""Old Script"" material had 16 more texts. However, this seems to have been lost at the end of the Eastern Han dynasty, while the ""Modern Script"" text enjoyed circulation, in particular in Ouyang Gao’s 歐陽高 (a. 136 BCE) study of it, the Ouyang Shangshu 歐陽尚書. This was the basis of studies by Ma Rong and Zheng Xuan in the Easter Han Dynasty. By the end of the second century CE, there was knowledge that the Shangshu at some point included more than the ""Modern Script"" text. This likely prompted scholars to recreate the ""Old Script"" texts said to have once belonged to the Shangshu, a process that culminated with the presentation of a 58 section (59 if the preface is included in the count) Shangshu to the Eastern Jin court, in 317 CE, by Mei Ze 梅頤.
This version was accepted, among doubts, and later was canonized as part of Kong Yingda's project. It was only in the 17th century that Qing dynasty scholar Yan Ruoqu demonstrated that the ""Old Script"" were actually fabrications ""reconstructed"" in the 3rd or 4th centuries AD.
In the transmitted edition, texts are grouped into four sections representing different eras: the semi-mythical reign of Yu the Great, and the Xia, Shang and Zhou dynasties.  The Zhou section accounts for over half the text.  Some of its New Text chapters are among the earliest examples of Chinese prose, recording speeches from the early years of the Zhou dynasty in the late 11th century BC.  Although the other three sections purport to record earlier material, most scholars believe that even the New Script chapters in these sections were composed later than those in the Zhou section, with chapters relating to the earliest periods being as recent as the 4th or 3rd centuries BC."
Halloween documents,"The Halloween documents comprise a series of confidential Microsoft memoranda on potential strategies relating to free software, open-source software, and to Linux in particular, and a series of media responses to these memoranda. Both the leaked documents and the responses were published by Eric S. Raymond in 1998.The documents are associated with Halloween because many of them were originally leaked close to October 31 in different years.

"
Electronic document,"An electronic document is any electronic media content (other than computer programs or system files) that is intended to be used in either an electronic form or as printed output. Originally, any computer data were considered as something internal — the final data output was always on paper. However, the development of computer networks has made it so that in most cases it is much more convenient to distribute electronic documents than printed ones. The improvements in electronic visual display technologies made it possible to view documents on screen instead of printing them (thus saving paper and the space required to store the printed copies).However, using electronic documents for final presentation instead of paper has created the problem of multiple incompatible file formats. Even plain text computer files are not free from this problem — e.g. under MS-DOS, most programs could not work correctly with UNIX-style text files (see newline), and for non-English speakers, the different code pages always have been a source of trouble.
Even more problems are connected with complex file formats of various word processors, spreadsheets, and graphics software. To alleviate the problem, many software companies distribute free file viewers for their proprietary file formats (one example is Adobe's Acrobat Reader). The other solution is the development of standardized non-proprietary file formats (such as HTML and OpenDocument), and electronic documents for specialized uses have specialized formats – the specialized electronic articles in physics use TeX or PostScript."
Historical document,"Historical documents are original documents that contain important historical information about a person, place, or event and can thus serve as primary sources as important ingredients of the historical methodology.
Significant historical documents can be deeds, laws, accounts of battles (often given by the victors or persons sharing their viewpoint), or the exploits of the powerful. Though these documents are of historical interest, they do not detail the daily lives of ordinary people, or the way society functioned. Anthropologists, historians and archeologists generally are more interested in documents that describe the day-to-day lives of ordinary people, indicating what they ate, their interaction with other members of their households and social groups, and their states of mind. It is this information that allows them to try to understand and describe the way society was functioning at any particular time in history. Greek ostraka provide good examples of historical documents from ""among the common people"".
Many documents that are produced today, such as personal letters, pictures, contracts, newspapers, and medical records, would be considered valuable historical documents in the future. However most of these will be lost in the future since they are either printed on ordinary paper which has a limited lifespan, or even stored in digital formats, then lost track over time.
Some companies and government entities are attempting to increase the number of documents that will survive the passage of time, by taking into account the preservation issues, and either printing documents in a manner that would increase the likelihood of them surviving indefinitely, or placing selected documents in time capsules or other special storage environments."
Document management system,"A document management system (DMS) is usually a computerized system used to store, share, track and manage files or documents. Some systems include history tracking where a log of the various versions created and modified by different users is recorded. The term has some overlap with the concepts of content management systems. It is often viewed as a component of enterprise content management (ECM) systems and related to digital asset management, document imaging, workflow systems and records management systems."
Constitution,"Institutions are humanly devised structures of rules and norms that shape and constrain individual behavior. All definitions of institutions generally entail that there is a level of persistence and continuity. Laws, rules, social conventions and norms are all examples of institutions. Institutions vary in their level of formality and informality.Institutions are a principal object of study in social sciences such as political science, anthropology, economics, and sociology (the latter described by Émile Durkheim as the ""science of institutions, their genesis and their functioning""). Primary or meta-institutions are institutions such as the family or money that are broad enough to encompass sets of related institutions. Institutions are also a central concern for law, the formal mechanism for political rule-making and enforcement. Historians study and document the founding, growth, decay and development of institutions as part of political, economic and cultural history."
Identity document,
Google Docs,"Google Docs is an online word processor included as part of the free, web-based Google Docs Editors suite offered by Google, which also includes: Google Sheets, Google Slides, Google Drawings, Google Forms, Google Sites and Google Keep. Google Docs is accessible via an internet browser as a web-based application and is also available as a mobile app on Android and iOS and as a desktop application on Google's ChromeOS.
Google Docs allows users to create and edit documents online while collaborating with other users in real time. Edits are tracked by the user making the edit, with a revision history presenting changes. An editor's position is highlighted with an editor-specific color and cursor, and a permissions system regulates what users can do. Updates have introduced features using machine learning, including ""Explore"", offering search results based on the contents of a document, and ""Action items"", allowing users to assign tasks to other users.Google Docs supports opening and saving documents in the standard OpenDocument format as well as in Rich text format, plain Unicode text, zipped HTML, and Microsoft Word. Exporting to PDF and EPUB formats are implemented."
binary independence model,The Binary Independence Model (BIM) in computing and information science is a probabilistic information retrieval technique. The model makes some simple assumptions to make the estimation of document/query similarity probable and feasible.
Binary Independence Model,The Binary Independence Model (BIM) in computing and information science is a probabilistic information retrieval technique. The model makes some simple assumptions to make the estimation of document/query similarity probable and feasible.
Okapi BM25,"In information retrieval, Okapi BM25 (BM is an abbreviation of best matching) is a ranking function used by search engines to estimate the relevance of documents to a given search query. It is based on the probabilistic retrieval framework developed in the 1970s and 1980s by Stephen E. Robertson, Karen Spärck Jones, and others.
The name of the actual ranking function is BM25. The fuller name, Okapi BM25, includes the name of the first system to use it, which was the Okapi information retrieval system, implemented at London's City University in the 1980s and 1990s. BM25 and its newer variants, e.g. BM25F (a version of BM25 that can take document structure and anchor text into account), represent TF-IDF-like retrieval functions used in document retrieval."
Probabilistic relevance model,"The probabilistic relevance model was devised by Stephen E. Robertson and Karen Spärck Jones as a framework for  probabilistic models to come. It is a formalism of information retrieval useful to derive ranking functions used by search engines and  web search engines in order to rank matching documents according to their relevance to a given search query.
It is a theoretical model estimating the probability that a document dj is relevant to a query q. The model assumes that this probability of relevance depends on the query and document representations. Furthermore, it assumes that there is a portion of all documents that is preferred by the user as the answer set for query q. Such an ideal answer set is called R and should maximize the overall probability of relevance to that user. The prediction is that documents in this set R are relevant to the query, while documents not present in the set are non-relevant.

  
    
      
        s
        i
        m
        (
        
          d
          
            j
          
        
        ,
        q
        )
        =
        
          
            
              P
              (
              R
              
                |
              
              
                
                  
                    
                      d
                      →
                    
                  
                
                
                  j
                
              
              )
            
            
              P
              (
              
                
                  
                    R
                    ¯
                  
                
              
              
                |
              
              
                
                  
                    
                      d
                      →
                    
                  
                
                
                  j
                
              
              )
            
          
        
      
    
    {\displaystyle sim(d_{j},q)={\frac {P(R|{\vec {d}}_{j})}{P({\bar {R}}|{\vec {d}}_{j})}}}"
Logistic regression,"In statistics, the logistic model (or logit model) is a statistical model that models the probability of an event taking place by having the log-odds for the event be a linear combination of one or more independent variables. In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model (the coefficients in the linear combination). Formally, in binary logistic regression there is a single binary dependent variable, coded by an indicator variable, where the two values are labeled ""0"" and ""1"", while the independent variables can each be a binary variable (two classes, coded by an indicator variable) or a continuous variable (any real value). The corresponding probability of the value labeled ""1"" can vary between 0 (certainly the value ""0"") and 1 (certainly the value ""1""), hence the labeling; the function that converts log-odds to probability is the logistic function, hence the name. The unit of measurement for the log-odds scale is called a logit, from logistic unit, hence the alternative names. See § Background and § Definition for formal mathematics, and § Example for a worked example.
Binary variables are widely used in statistics to model the probability of a certain class or event taking place, such as the probability of a team winning, of a patient being healthy, etc. (see § Applications), and the logistic model has been the most commonly used model for binary regression since about 1970. Binary variables can be generalized to categorical variables when there are more than two possible values (e.g. whether an image is of a cat, dog, lion, etc.), and the binary logistic regression generalized to multinomial logistic regression. If the multiple categories are ordered, one can use the ordinal logistic regression (for example the proportional odds ordinal logistic model). See § Extensions for further extensions. The logistic regression model itself simply models probability of output in terms of input and does not perform statistical classification (it is not a classifier), though it can be used to make a classifier, for instance by choosing a cutoff value and classifying inputs with probability greater than the cutoff as one class, below the cutoff as the other; this is a common way to make a binary classifier.
Analogous linear models for binary variables with a different sigmoid function instead of the logistic function (to convert the linear combination to a probability) can also be used, most notably the probit model; see § Alternatives. The defining characteristic of the logistic model is that increasing one of the independent variables multiplicatively scales the odds of the given outcome at a constant rate, with each independent variable having its own parameter; for a binary dependent variable this generalizes the odds ratio. More abstractly, the logistic function is the natural parameter for the Bernoulli distribution, and in this sense is the ""simplest"" way to convert a real number to a probability. In particular, it maximizes entropy (minimizes added information), and in this sense makes the fewest assumptions of the data being modeled; see § Maximum entropy.
The parameters of a logistic regression are most commonly estimated by maximum-likelihood estimation (MLE). This does not have a closed-form expression, unlike linear least squares; see § Model fitting. Logistic regression by MLE plays a similarly basic role for binary or categorical responses as linear regression by ordinary least squares (OLS) plays for scalar responses: it is a simple, well-analyzed baseline model; see § Comparison with linear regression for discussion. The logistic regression as a general statistical model was originally developed and popularized primarily by Joseph Berkson, beginning in Berkson (1944), where he coined ""logit""; see § History."
class membership,"In machine learning, a probabilistic classifier is a classifier that is able to predict, given an observation of an input, a probability distribution over a set of classes, rather than only outputting the most likely class that the observation should belong to. Probabilistic classifiers provide classification that can be useful in its own right or when combining classifiers into ensembles."
Probabilistic classification,"In machine learning, a probabilistic classifier is a classifier that is able to predict, given an observation of an input, a probability distribution over a set of classes, rather than only outputting the most likely class that the observation should belong to. Probabilistic classifiers provide classification that can be useful in its own right or when combining classifiers into ensembles."
Latent class model,"In statistics, a latent class model (LCM) relates a set of observed (usually discrete) multivariate variables to a set of latent variables. It is a type of latent variable model. It is called a latent class model because the latent variable is discrete. A class is characterized by a pattern of conditional probabilities that indicate the chance that variables take on certain values.
Latent class analysis (LCA) is a subset of structural equation modeling, used to find groups or subtypes of cases in multivariate categorical data. These subtypes are called ""latent classes"".Confronted with a situation as follows, a researcher might choose to use LCA to understand the data: Imagine that symptoms a-d have been measured in a range of patients with diseases X, Y, and Z, and that disease X is associated with the presence of symptoms a, b, and c, disease Y with symptoms b, c, d, and disease Z with symptoms a, c and d.
The LCA will attempt to detect the presence of latent classes (the disease entities), creating patterns of association in the symptoms. As in factor analysis, the LCA can also be used to classify case according to their maximum likelihood class membership.Because the criterion for solving the LCA is to achieve latent classes within which there is no longer any association of one symptom with another (because the class is the disease which causes their association), and the set of diseases a patient has (or class a case is a member of) causes the symptom association, the symptoms will be ""conditionally independent"", i.e., conditional on class membership, they are no longer related."
Social class,"A social class is a grouping of people into a set of hierarchical social categories, the most common being the upper, middle and lower classes. Membership in a social class can for example be dependent on education, wealth, occupation, income, and belonging to a particular subculture or social network.""Class"" is a subject of analysis for sociologists, political scientists, anthropologists and social historians. The term has a wide range of sometimes conflicting meanings, and there is no broad consensus on a definition of ""class"". Some people argue that due to social mobility, class boundaries do not exist. In common parlance, the term ""social class"" is usually synonymous with ""socio-economic class"", defined as ""people having the same social, economic, cultural, political or educational status"", e.g., ""the working class""; ""an emerging professional class"". However, academics distinguish social class from socioeconomic status, using the former to refer to one's relatively stable sociocultural background and the latter to refer to one's current social and economic situation which is consequently more changeable over time.The precise measurements of what determines social class in society have varied over time. Karl Marx thought ""class"" was defined by one's relationship to the means of production (their relations of production). His understanding of classes in modern capitalist society is that the proletariat work but do not own the means of production, and the bourgeoisie, those who invest and live off the surplus generated by the proletariat's operation of the means of production, do not work at all. This contrasts with the view of the sociologist Max Weber, who argued that ""class"" is determined by economic position, in contrast to ""social status"" or ""Stand"" which is determined by social prestige rather than simply just relations of production. The term ""class"" is etymologically derived from the Latin classis, which was used by census takers to categorize citizens by wealth in order to determine military service obligations.In the late 18th century, the term ""class"" began to replace classifications such as estates, rank and orders as the primary means of organizing society into hierarchical divisions. This corresponded to a general decrease in significance ascribed to hereditary characteristics and increase in the significance of wealth and income as indicators of position in the social hierarchy.

"
K-nearest neighbors algorithm,"In statistics, the k-nearest neighbors algorithm (k-NN) is a non-parametric supervised learning method first developed by Evelyn Fix and Joseph Hodges in 1951, and later expanded by Thomas Cover. It is used for classification and regression. In both cases, the input consists of the k closest training examples in a data set. The output depends on whether k-NN is used for classification or regression:

In k-NN classification, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.In k-NN regression, the output is the property value for the object. This value is the average of the values of k nearest neighbors.k-NN is a type of classification where the function is only approximated locally and all computation is deferred until function evaluation. Since this algorithm relies on distance for classification, if the features represent different physical units or come in vastly different scales then normalizing the training data can improve its accuracy dramatically.Both for classification and regression, a useful technique can be to assign weights to the contributions of the neighbors, so that the nearer neighbors contribute more to the average than the more distant ones. For example, a common weighting scheme consists in giving each neighbor a weight of 1/d, where d is the distance to the neighbor.The neighbors are taken from a set of objects for which the class (for k-NN classification) or the object property value (for k-NN regression) is known. This can be thought of as the training set for the algorithm, though no explicit training step is required.
A peculiarity of the k-NN algorithm is that it is sensitive to the local structure of the data."
Posterior probability,"The posterior probability is a type of conditional probability that results from updating the prior probability with information summarized by the likelihood, through an application of Bayes' theorem. From an epistemological perspective, the posterior probability contains everything there is to know about an uncertain proposition (such as a scientific hypothesis, or parameter values), given prior knowledge and a mathematical model describing the observations available at a particular time. After the arrival of new information, the current posterior probability may serve as the prior in another round of Bayesian updating.In the context of Bayesian statistics, the posterior probability distribution usually describes the epistemic uncertainty about statistical parameters conditional on a collection of observed data. From a given posterior distribution, various point and interval estimates can be derived, such as the maximum a posteriori (MAP) or the highest posterior density interval (HPDI). But while conceptually simple, the posterior distribution is generally not tractable and therefore needs to be either analytically or numerically approximated."
Calibration (statistics),"There are two main uses of the term calibration in statistics that denote special types of statistical inference problems. ""Calibration"" can mean

a reverse process to regression, where instead of a future dependent variable being predicted from known explanatory variables, a known observation of the dependent variables is used to predict a corresponding explanatory variable;
procedures in statistical classification to determine class membership probabilities which assess the uncertainty of a given new observation belonging to each of the already established classes.In addition, ""calibration"" is used in statistics with the usual general meaning of calibration. For example, model calibration can be also used to refer to Bayesian inference about the value of a model's parameters, given some data set, or more generally to any type of fitting of a statistical model.
As Philip Dawid puts it, ""a forecaster is well calibrated if, for example, of those events to which he assigns a probability 30 percent, the long-run proportion that actually occurs turns out to be 30 percent""."
Socialist Revolutionary Party,"The Socialist Revolutionary Party, or the Party of Socialist-Revolutionaries (the SRs, СР, or Esers, эсеры, esery; Russian: Партия социалистов-революционеров, ПСР), was a major political party in late Imperial Russia, and both phases of the Russian Revolution and early Soviet Russia.
The SRs were agrarian socialists and supporters of a democratic socialist Russian republic. The ideological heirs of the Narodniks, the SRs won a mass following among the Russian peasantry by endorsing the overthrow of the Tsar and the redistribution of land to the peasants. The SRs boycotted the elections to the First Duma following the Revolution of 1905 alongside the Russian Social Democratic Labour Party, but chose to run in the elections to the Second Duma and received the majority of the few seats allotted to the peasantry. Following the 1907 coup, the SRs boycotted all subsequent Dumas until the fall of the Tsar in the February Revolution of March 1917. Controversially, the party leadership endorsed the Russian Provisional Government and participated in multiple coalitions with liberal and social-democratic parties, while a radical faction within the SRs rejected the Provisional Government's authority in favor of the Congress of Soviets and began to drift towards the Bolsheviks. These divisions would ultimately result in the party splitting over the course of the summer of 1917 into the Right and Left SRs. Meanwhile, Alexander Kerensky, one of the leaders of the February Revolution and the second and last head of the Provisional Government (July–November 1917) was a nominal member of the SR party but in practice acted independently of its decisions.
By November 1917, the Provisional Government had been widely discredited by its failure to withdraw from World War I, implement land reform or convene a Constituent Assembly to draft a Constitution, leaving the soviet councils in de facto control of the country. The Bolsheviks thus moved to hand power to the 2nd Congress of Soviets in the October Revolution. After a few weeks of deliberation, the Left SRs ultimately formed a coalition government with the Bolsheviks – the Council of People's Commissioners – from November 1917 to March 1918 while the Right SRs boycotted the Soviets and denounced the Revolution as an illegal coup. The SRs obtained a majority in the subsequent elections to the Russian Constituent Assembly, with most of the party's seats going to the Right faction. Citing outdated voter-rolls which did not acknowledge the party split, and the Assembly's conflicts with the Congress of Soviets, the Bolshevik-Left SR government moved to dissolve the Constituent Assembly by force in January 1918.The Left SRs left their coalition with the Bolsheviks in March 1918 in protest against the signing of the Treaty of Brest-Litovsk. An uprising against the Bolsheviks by the leadership of the Left SRs in July 1918 resulted in the immediate arrest of most of the party's members. Most of the Left SRs who opposed the uprising were gradually freed and allowed to keep their government positions, but were unable to organize a new central organ and gradually splintered into multiple pro-Bolshevik parties, which would all ultimately merge with the Russian Communist Party (Bolsheviks) by 1921. The Right SRs supported the Whites during the Russian Civil War of 1917–1922, but the White movement's anti-socialist leadership increasingly marginalized and ultimately purged them. A small Right SR remnant, still calling itself the Socialist Revolutionary Party, continued to operate in exile from 1923 to 1940 as a member of the Labour and Socialist International."
Social transformation,"In sociology, social transformation is a somewhat ambiguous term that has two broad definitions.
One definition of social transformation is the process by which an individual alters the socially ascribed social status of their parents into a socially achieved status for themselves (status transformation). Another definition refers to large scale social change as in cultural reforms or transformations (societal transformation). The first occurs with the individual, the second with the social system."
Church membership,"Church membership, in Christianity, is the state of belonging to a local church congregation, which in most cases, simultaneously makes one a member of a Christian denomination and the universal Christian Church. Christian theologians have taught that church membership is commanded in the Bible. The process of becoming a church member varies based on the Christian denomination. Those preparing to become full members of a church are known variously as catechumens, candidates or probationers depending on the Christian denomination and the sacramental status of the individual."
Agrarian socialism,
vector space classification,"In mathematics and physics, a vector space (also called a linear space) is a set whose elements, often called vectors, may be added together and multiplied (""scaled"") by numbers called scalars. Scalars are often real numbers, but can be  complex numbers or, more generally, elements of any field. The operations of vector addition and scalar multiplication must satisfy certain requirements, called vector axioms. The terms real vector space and complex vector space are often used to specify the nature of the scalars: real coordinate space or complex coordinate space.
Vector spaces generalize Euclidean vectors, which allow modeling of physical quantities, such as forces and velocity, that have not only a magnitude, but also a direction. The concept of vector spaces is fundamental for linear algebra, together with the concept of matrix, which allows computing in vector spaces. This provides a concise and synthetic way for manipulating and studying systems of linear equations.
Vector spaces are characterized by their dimension, which, roughly speaking, specifies the number of independent directions in the space. This means that, for two vector spaces with the same dimension, the properties that depend only on the vector-space structure are exactly the same (technically the vector spaces are isomorphic). A vector space is finite-dimensional if its dimension is a natural number. Otherwise, it is infinite-dimensional, and its dimension is an infinite cardinal. Finite-dimensional vector spaces occur naturally in geometry and related areas. Infinite-dimensional vector spaces occur in many areas of mathematics. For example, polynomial rings are countably infinite-dimensional vector spaces, and many function spaces have the cardinality of the continuum as a dimension.
Many vector spaces that are considered in mathematics are also endowed with other structures. This is the case of algebras, which include field extensions, polynomial rings, associative algebras and Lie algebras. This is also the case of topological vector spaces, which include function spaces, inner product spaces, normed spaces, Hilbert spaces and Banach spaces."
Vector space,"In mathematics and physics, a vector space (also called a linear space) is a set whose elements, often called vectors, may be added together and multiplied (""scaled"") by numbers called scalars. Scalars are often real numbers, but can be  complex numbers or, more generally, elements of any field. The operations of vector addition and scalar multiplication must satisfy certain requirements, called vector axioms. The terms real vector space and complex vector space are often used to specify the nature of the scalars: real coordinate space or complex coordinate space.
Vector spaces generalize Euclidean vectors, which allow modeling of physical quantities, such as forces and velocity, that have not only a magnitude, but also a direction. The concept of vector spaces is fundamental for linear algebra, together with the concept of matrix, which allows computing in vector spaces. This provides a concise and synthetic way for manipulating and studying systems of linear equations.
Vector spaces are characterized by their dimension, which, roughly speaking, specifies the number of independent directions in the space. This means that, for two vector spaces with the same dimension, the properties that depend only on the vector-space structure are exactly the same (technically the vector spaces are isomorphic). A vector space is finite-dimensional if its dimension is a natural number. Otherwise, it is infinite-dimensional, and its dimension is an infinite cardinal. Finite-dimensional vector spaces occur naturally in geometry and related areas. Infinite-dimensional vector spaces occur in many areas of mathematics. For example, polynomial rings are countably infinite-dimensional vector spaces, and many function spaces have the cardinality of the continuum as a dimension.
Many vector spaces that are considered in mathematics are also endowed with other structures. This is the case of algebras, which include field extensions, polynomial rings, associative algebras and Lie algebras. This is also the case of topological vector spaces, which include function spaces, inner product spaces, normed spaces, Hilbert spaces and Banach spaces."
Support vector machine,"In machine learning, support vector machines (SVMs, also support vector networks) are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis. Developed at AT&T Bell Laboratories by Vladimir Vapnik with colleagues (Boser et al., 1992, Guyon et al., 1993, Cortes and Vapnik, 1995, Vapnik et al., 1997) SVMs are one of the most robust prediction methods, being based on statistical learning frameworks or VC theory proposed by Vapnik (1982, 1995) and Chervonenkis (1974). Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting). SVM maps training examples to points in space so as to maximise the width of the gap between the two categories. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.
In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.
When data are unlabelled, supervised learning is not possible, and an unsupervised learning approach is required, which attempts to find natural clustering of the data to groups, and then map new data to these formed groups. The support vector clustering algorithm, created by Hava Siegelmann and Vladimir Vapnik, applies the statistics of support vectors, developed in the support vector machines algorithm, to categorize unlabeled data."
Vector space model,"Vector space model or term vector model is an algebraic model for representing text documents (and any objects, in general) as vectors of identifiers (such as index terms). It is used in information filtering, information retrieval, indexing and relevancy rankings.  Its first use was in the SMART Information Retrieval System.

"
Feature (machine learning),"In machine learning and pattern recognition, a feature is an individual measurable property or characteristic of a phenomenon. Choosing informative, discriminating and independent features is a crucial element of effective algorithms in pattern recognition,  classification and regression. Features are usually numeric, but structural features such as strings and graphs are used in syntactic pattern recognition. The concept of ""feature"" is related to that of explanatory variable used in statistical techniques such as linear regression."
decision boundaries,"In a statistical-classification problem with two classes, a decision boundary or decision surface is a hypersurface that partitions the underlying vector space into two sets, one for each class. The classifier will classify all the points on one side of the decision boundary as belonging to one class and all those on the other side as belonging to the other class.
A decision boundary is the region of a problem space in which the output label of a classifier is ambiguous.If the decision surface is a hyperplane, then the classification problem is linear, and the classes are linearly separable.
Decision boundaries are not always clear cut. That is, the transition from one class in the feature space to another is not discontinuous, but gradual. This effect is common in fuzzy logic based classification algorithms, where membership in one class or another is ambiguous.
Decision boundaries can be approximations of optimal stopping boundaries.  The decision boundary is the set of points of that hyperplane that pass through zero.  For example, the angle between a vector and points in a set must be zero for points that are on or close to the decision boundary. Decision boundary instability can be incorporated with generalization error as a standard for selecting the most accurate and stable classifier."
Decision boundary,"In a statistical-classification problem with two classes, a decision boundary or decision surface is a hypersurface that partitions the underlying vector space into two sets, one for each class. The classifier will classify all the points on one side of the decision boundary as belonging to one class and all those on the other side as belonging to the other class.
A decision boundary is the region of a problem space in which the output label of a classifier is ambiguous.If the decision surface is a hyperplane, then the classification problem is linear, and the classes are linearly separable.
Decision boundaries are not always clear cut. That is, the transition from one class in the feature space to another is not discontinuous, but gradual. This effect is common in fuzzy logic based classification algorithms, where membership in one class or another is ambiguous.
Decision boundaries can be approximations of optimal stopping boundaries.  The decision boundary is the set of points of that hyperplane that pass through zero.  For example, the angle between a vector and points in a set must be zero for points that are on or close to the decision boundary. Decision boundary instability can be incorporated with generalization error as a standard for selecting the most accurate and stable classifier."
K-nearest neighbors algorithm,"In statistics, the k-nearest neighbors algorithm (k-NN) is a non-parametric supervised learning method first developed by Evelyn Fix and Joseph Hodges in 1951, and later expanded by Thomas Cover. It is used for classification and regression. In both cases, the input consists of the k closest training examples in a data set. The output depends on whether k-NN is used for classification or regression:

In k-NN classification, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.In k-NN regression, the output is the property value for the object. This value is the average of the values of k nearest neighbors.k-NN is a type of classification where the function is only approximated locally and all computation is deferred until function evaluation. Since this algorithm relies on distance for classification, if the features represent different physical units or come in vastly different scales then normalizing the training data can improve its accuracy dramatically.Both for classification and regression, a useful technique can be to assign weights to the contributions of the neighbors, so that the nearer neighbors contribute more to the average than the more distant ones. For example, a common weighting scheme consists in giving each neighbor a weight of 1/d, where d is the distance to the neighbor.The neighbors are taken from a set of objects for which the class (for k-NN classification) or the object property value (for k-NN regression) is known. This can be thought of as the training set for the algorithm, though no explicit training step is required.
A peculiarity of the k-NN algorithm is that it is sensitive to the local structure of the data."
Perceptron,"In machine learning, the perceptron (or McCulloch-Pitts neuron) is an algorithm for supervised learning of binary classifiers.  A binary classifier is a function which can decide whether or not an input, represented by a vector of numbers, belongs to some specific class.  It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector."
Weak supervision,"Weak supervision is a branch of machine learning where noisy, limited, or imprecise sources are used to provide supervision signal for labeling large amounts of training data in a supervised learning setting. This approach alleviates the burden of obtaining hand-labeled data sets, which can be costly or impractical. Instead, inexpensive weak labels are employed with the understanding that they are imperfect, but can nonetheless be used to create a strong predictive model."
Discriminative model,"Discriminative models, also referred to as conditional models, are a class of logistical models used for classification or regression. They distinguish decision boundaries through observed data, such as pass/fail, win/lose, alive/dead or healthy/sick.
Typical discriminative models include logistic regression (LR), conditional random fields (CRFs) (specified over an undirected graph), decision trees, and many others. Typical generative model approaches include naive Bayes classifiers, Gaussian mixture models, variational autoencoders, generative adversarial networks and others."
Hyperplane,"In geometry, a hyperplane is a subspace whose dimension is one less than that of its ambient space.  For example, if a space is 3-dimensional then its hyperplanes are the 2-dimensional planes, while if the space is 2-dimensional, its hyperplanes are the 1-dimensional lines.  This notion can be used in any general space in which the concept of the dimension of a subspace is defined.
In different settings, hyperplanes may have different properties.  For instance, a hyperplane of an n-dimensional affine space is a flat subset with dimension n − 1 and it separates the space into two half spaces. While a hyperplane of an n-dimensional projective space does not have this property.
The difference in dimension between a subspace S and its ambient space X is known as the codimension of S with respect to X.  Therefore, a necessary and sufficient condition for S to be a hyperplane in X is for S to have codimension one in X.

"
Quantization (signal processing),"Quantization, in mathematics and digital signal processing, is the process of mapping input values from a large set (often a continuous set) to output values in a (countable) smaller set, often with a finite number of elements.  Rounding and truncation are typical examples of quantization processes.  Quantization is involved to some degree in nearly all digital signal processing, as the process of representing a signal in digital form ordinarily involves rounding.  Quantization also forms the core of essentially all lossy compression algorithms.
The difference between an input value and its quantized value (such as round-off error) is referred to as quantization error.  A device or algorithmic function that performs quantization is called a quantizer.  An analog-to-digital converter is an example of a quantizer."
Personal boundaries,"Personal boundaries or the act of setting boundaries is a life skill that has been popularized by self help authors and support groups since the mid 1980s. It is the practice of openly communicating and asserting personal values as way to preserve and protect against having them compromised or violated. The term ""boundary"" is a metaphor – with in-bounds meaning acceptable and out-of-bounds meaning unacceptable. Without values and boundaries our identities become diffused and often controlled by the definitions offered by others. The concept of boundaries has been widely adopted by the counseling profession."
Decision Sciences,"Decision Sciences is a peer-reviewed academic journal covering research about decision making within the boundaries of an organization, as well as decisions involving inter-firm coordination. According to the 2010 Journal Citation Reports, Decision Sciences is ranked 40th out of 140 journals in the category ""Management"" and ranked in the B category (on a scale from A+ to D) by the scientific journal ranking JOURQUAL (German Academic Association for Business Research/VHB) in 2015. Decision Sciences is published by Wiley-Blackwell on behalf of the Decision Sciences Institute. The current editors are Xenophon Koufteros (since 2019) and Sri Talluri (since 2020).
Decision Sciences is associated with the Decision Sciences Journal of Innovative Education."
Canada–France Maritime Boundary Case,The Canada–France Maritime Boundary Case was a dispute between Canada and France that was decided in 1992 by an arbitral tribunal created by the parties to resolve the dispute. The decision established the extent of the Exclusive Economic Zone of the French territory of Saint Pierre and Miquelon.
vector space classification,"In mathematics and physics, a vector space (also called a linear space) is a set whose elements, often called vectors, may be added together and multiplied (""scaled"") by numbers called scalars. Scalars are often real numbers, but can be  complex numbers or, more generally, elements of any field. The operations of vector addition and scalar multiplication must satisfy certain requirements, called vector axioms. The terms real vector space and complex vector space are often used to specify the nature of the scalars: real coordinate space or complex coordinate space.
Vector spaces generalize Euclidean vectors, which allow modeling of physical quantities, such as forces and velocity, that have not only a magnitude, but also a direction. The concept of vector spaces is fundamental for linear algebra, together with the concept of matrix, which allows computing in vector spaces. This provides a concise and synthetic way for manipulating and studying systems of linear equations.
Vector spaces are characterized by their dimension, which, roughly speaking, specifies the number of independent directions in the space. This means that, for two vector spaces with the same dimension, the properties that depend only on the vector-space structure are exactly the same (technically the vector spaces are isomorphic). A vector space is finite-dimensional if its dimension is a natural number. Otherwise, it is infinite-dimensional, and its dimension is an infinite cardinal. Finite-dimensional vector spaces occur naturally in geometry and related areas. Infinite-dimensional vector spaces occur in many areas of mathematics. For example, polynomial rings are countably infinite-dimensional vector spaces, and many function spaces have the cardinality of the continuum as a dimension.
Many vector spaces that are considered in mathematics are also endowed with other structures. This is the case of algebras, which include field extensions, polynomial rings, associative algebras and Lie algebras. This is also the case of topological vector spaces, which include function spaces, inner product spaces, normed spaces, Hilbert spaces and Banach spaces."
Vector space,"In mathematics and physics, a vector space (also called a linear space) is a set whose elements, often called vectors, may be added together and multiplied (""scaled"") by numbers called scalars. Scalars are often real numbers, but can be  complex numbers or, more generally, elements of any field. The operations of vector addition and scalar multiplication must satisfy certain requirements, called vector axioms. The terms real vector space and complex vector space are often used to specify the nature of the scalars: real coordinate space or complex coordinate space.
Vector spaces generalize Euclidean vectors, which allow modeling of physical quantities, such as forces and velocity, that have not only a magnitude, but also a direction. The concept of vector spaces is fundamental for linear algebra, together with the concept of matrix, which allows computing in vector spaces. This provides a concise and synthetic way for manipulating and studying systems of linear equations.
Vector spaces are characterized by their dimension, which, roughly speaking, specifies the number of independent directions in the space. This means that, for two vector spaces with the same dimension, the properties that depend only on the vector-space structure are exactly the same (technically the vector spaces are isomorphic). A vector space is finite-dimensional if its dimension is a natural number. Otherwise, it is infinite-dimensional, and its dimension is an infinite cardinal. Finite-dimensional vector spaces occur naturally in geometry and related areas. Infinite-dimensional vector spaces occur in many areas of mathematics. For example, polynomial rings are countably infinite-dimensional vector spaces, and many function spaces have the cardinality of the continuum as a dimension.
Many vector spaces that are considered in mathematics are also endowed with other structures. This is the case of algebras, which include field extensions, polynomial rings, associative algebras and Lie algebras. This is also the case of topological vector spaces, which include function spaces, inner product spaces, normed spaces, Hilbert spaces and Banach spaces."
Support vector machine,"In machine learning, support vector machines (SVMs, also support vector networks) are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis. Developed at AT&T Bell Laboratories by Vladimir Vapnik with colleagues (Boser et al., 1992, Guyon et al., 1993, Cortes and Vapnik, 1995, Vapnik et al., 1997) SVMs are one of the most robust prediction methods, being based on statistical learning frameworks or VC theory proposed by Vapnik (1982, 1995) and Chervonenkis (1974). Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting). SVM maps training examples to points in space so as to maximise the width of the gap between the two categories. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.
In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.
When data are unlabelled, supervised learning is not possible, and an unsupervised learning approach is required, which attempts to find natural clustering of the data to groups, and then map new data to these formed groups. The support vector clustering algorithm, created by Hava Siegelmann and Vladimir Vapnik, applies the statistics of support vectors, developed in the support vector machines algorithm, to categorize unlabeled data."
Vector space model,"Vector space model or term vector model is an algebraic model for representing text documents (and any objects, in general) as vectors of identifiers (such as index terms). It is used in information filtering, information retrieval, indexing and relevancy rankings.  Its first use was in the SMART Information Retrieval System.

"
Feature (machine learning),"In machine learning and pattern recognition, a feature is an individual measurable property or characteristic of a phenomenon. Choosing informative, discriminating and independent features is a crucial element of effective algorithms in pattern recognition,  classification and regression. Features are usually numeric, but structural features such as strings and graphs are used in syntactic pattern recognition. The concept of ""feature"" is related to that of explanatory variable used in statistical techniques such as linear regression."
k nearest neighbors,"In statistics, the k-nearest neighbors algorithm (k-NN) is a non-parametric supervised learning method first developed by Evelyn Fix and Joseph Hodges in 1951, and later expanded by Thomas Cover. It is used for classification and regression. In both cases, the input consists of the k closest training examples in a data set. The output depends on whether k-NN is used for classification or regression:

In k-NN classification, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.In k-NN regression, the output is the property value for the object. This value is the average of the values of k nearest neighbors.k-NN is a type of classification where the function is only approximated locally and all computation is deferred until function evaluation. Since this algorithm relies on distance for classification, if the features represent different physical units or come in vastly different scales then normalizing the training data can improve its accuracy dramatically.Both for classification and regression, a useful technique can be to assign weights to the contributions of the neighbors, so that the nearer neighbors contribute more to the average than the more distant ones. For example, a common weighting scheme consists in giving each neighbor a weight of 1/d, where d is the distance to the neighbor.The neighbors are taken from a set of objects for which the class (for k-NN classification) or the object property value (for k-NN regression) is known. This can be thought of as the training set for the algorithm, though no explicit training step is required.
A peculiarity of the k-NN algorithm is that it is sensitive to the local structure of the data."
K-nearest neighbors algorithm,"In statistics, the k-nearest neighbors algorithm (k-NN) is a non-parametric supervised learning method first developed by Evelyn Fix and Joseph Hodges in 1951, and later expanded by Thomas Cover. It is used for classification and regression. In both cases, the input consists of the k closest training examples in a data set. The output depends on whether k-NN is used for classification or regression:

In k-NN classification, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.In k-NN regression, the output is the property value for the object. This value is the average of the values of k nearest neighbors.k-NN is a type of classification where the function is only approximated locally and all computation is deferred until function evaluation. Since this algorithm relies on distance for classification, if the features represent different physical units or come in vastly different scales then normalizing the training data can improve its accuracy dramatically.Both for classification and regression, a useful technique can be to assign weights to the contributions of the neighbors, so that the nearer neighbors contribute more to the average than the more distant ones. For example, a common weighting scheme consists in giving each neighbor a weight of 1/d, where d is the distance to the neighbor.The neighbors are taken from a set of objects for which the class (for k-NN classification) or the object property value (for k-NN regression) is known. This can be thought of as the training set for the algorithm, though no explicit training step is required.
A peculiarity of the k-NN algorithm is that it is sensitive to the local structure of the data."
Nearest neighbor search,"Nearest neighbor search (NNS), as a form of proximity search,  is the optimization problem of finding the point in a given set that is closest (or most similar) to a given point. Closeness is typically expressed in terms of a dissimilarity function: the less similar the objects, the larger the function values. 
Formally, the nearest-neighbor (NN) search problem is defined as follows: given a set S of points in a space M and a query point q ∈ M, find the closest point in S to q. Donald Knuth in vol. 3 of The Art of Computer Programming (1973) called it the post-office problem, referring to an application of assigning to a residence the nearest post office. A direct generalization of this problem is a k-NN search, where we need to find the k closest points.
Most commonly M is a  metric space and dissimilarity is expressed as a distance metric, which is symmetric and satisfies the triangle inequality. Even more common, M is taken to be the d-dimensional vector space where dissimilarity is measured using the Euclidean distance, Manhattan distance or other distance metric. However, the dissimilarity function can be arbitrary. One example is asymmetric Bregman divergence, for which the triangle inequality does not hold."
Nearest neighbor graph,"The nearest neighbor graph (NNG) is a directed graph defined for a set of points in a metric space, such as the Euclidean distance in the plane. The NNG has a vertex for each point, and a directed edge from p to q whenever q is a nearest neighbor of p, a point whose distance from p is minimum among all the given points other than p itself.In many uses of these graphs, the directions of the edges are ignored and the NNG is defined instead as an undirected graph. However, the nearest neighbor relation is not a symmetric one, i.e., p from the definition is not necessarily a nearest neighbor for q. In theoretical discussions of algorithms a kind of general position is often assumed, namely, the nearest (k-nearest) neighbor is unique for each object. In implementations of the algorithms it is necessary to bear in mind that this is not always the case. For situations in which it is necessary to make the nearest neighbor for each object unique, the set P may be indexed and in the case of a tie the object with, e.g., the largest index may be taken as the nearest neighbor.The k-nearest neighbor graph (k-NNG) is a graph in which two vertices p and q are connected by an edge, if the distance between p and  q is among the k-th smallest distances from p to other objects from P. The NNG is a special case of the k-NNG, namely it is the 1-NNG. k-NNGs obey a separator theorem: they can be partitioned into two subgraphs of at most n(d + 1)/(d + 2) vertices each by the removal of O(k1/dn1 − 1/d) points.Another variation is the farthest neighbor graph (FNG), in which each point is connected by an edge to the farthest point from it, instead of the nearest point.
NNGs for points in the plane as well as in multidimensional spaces find applications, e.g., in data compression, motion planning, and facilities location. In statistical analysis, the nearest-neighbor chain algorithm based on following paths in this graph can be used to find hierarchical clusterings quickly. Nearest neighbor graphs are also a subject of computational geometry.
The method can be used to induce a graph on nodes with unknown connectivity."
Nearest neighbor,"Nearest neighbor search (NNS), as a form of proximity search,  is the optimization problem of finding the point in a given set that is closest (or most similar) to a given point. Closeness is typically expressed in terms of a dissimilarity function: the less similar the objects, the larger the function values. 
Formally, the nearest-neighbor (NN) search problem is defined as follows: given a set S of points in a space M and a query point q ∈ M, find the closest point in S to q. Donald Knuth in vol. 3 of The Art of Computer Programming (1973) called it the post-office problem, referring to an application of assigning to a residence the nearest post office. A direct generalization of this problem is a k-NN search, where we need to find the k closest points.
Most commonly M is a  metric space and dissimilarity is expressed as a distance metric, which is symmetric and satisfies the triangle inequality. Even more common, M is taken to be the d-dimensional vector space where dissimilarity is measured using the Euclidean distance, Manhattan distance or other distance metric. However, the dissimilarity function can be arbitrary. One example is asymmetric Bregman divergence, for which the triangle inequality does not hold."
Large margin nearest neighbor,"Large margin nearest neighbor (LMNN) classification is a statistical machine learning algorithm for metric learning. It learns a pseudometric designed for k-nearest neighbor classification. The algorithm is based on semidefinite programming, a sub-class of convex optimization.
The goal of supervised learning (more specifically classification) is to learn a decision rule that can categorize data instances into pre-defined classes. The  k-nearest neighbor rule assumes a training data set of labeled instances (i.e. the classes are known). It classifies a new data instance with the class obtained from the majority vote of the k closest (labeled) training instances. Closeness is measured with a pre-defined metric. Large margin nearest neighbors is an algorithm that learns this global (pseudo-)metric in a supervised fashion to improve the classification accuracy of the k-nearest neighbor rule."
K-d tree,"In computer science, a k-d tree (short for k-dimensional tree) is a space-partitioning data structure for organizing points in a k-dimensional space. k-d trees are a useful data structure for several applications, such as searches involving a multidimensional search key (e.g. range searches and nearest neighbor searches) and creating point clouds. k-d trees are a special case of binary space partitioning trees.

"
Bias–variance tradeoff,"In statistics and machine learning, the bias–variance tradeoff is the property of a model that the variance of the parameter estimated across samples can be reduced by increasing the bias in the estimated parameters.
The bias–variance dilemma or bias–variance problem is the conflict in trying to simultaneously minimize these two sources of error that prevent supervised learning algorithms from generalizing beyond their training set:
The bias error is an error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).
The variance is an error from sensitivity to small fluctuations in the training set. High variance may result from an algorithm modeling the random noise in the training data (overfitting).The bias–variance decomposition is a way of analyzing a learning algorithm's expected generalization error with respect to a particular problem as a sum of three terms, the bias, variance, and a quantity called the irreducible error, resulting from noise in the problem itself."
K-means clustering,"k-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells. k-means clustering minimizes within-cluster variances (squared Euclidean distances), but not regular Euclidean distances, which would be the more difficult Weber problem: the mean optimizes squared errors, whereas only the geometric median minimizes Euclidean distances. For instance, better Euclidean solutions can be found using k-medians and k-medoids.
The problem is computationally difficult (NP-hard); however, efficient heuristic algorithms converge quickly to a local optimum. These are usually similar to the expectation-maximization algorithm for mixtures of Gaussian distributions via an iterative refinement approach employed by both k-means and Gaussian mixture modeling. They both use cluster centers to model the data; however, k-means clustering tends to find clusters of comparable spatial extent, while the Gaussian mixture model allows clusters to have different shapes.
The unsupervised k-means algorithm has a loose relationship to the k-nearest neighbor classifier, a popular supervised machine learning technique for classification that is often confused with k-means due to the name. Applying the 1-nearest neighbor classifier to the cluster centers obtained by k-means classifies new data into the existing clusters. This is known as nearest centroid classifier or Rocchio algorithm.

"
Curse of dimensionality,"The curse of dimensionality refers to various phenomena that arise when analyzing and organizing data in high-dimensional spaces that do not occur in low-dimensional settings such as the three-dimensional physical space of everyday experience.  The expression was coined by Richard E. Bellman when considering problems in dynamic programming.Dimensionally cursed phenomena occur in domains such as numerical analysis, sampling, combinatorics, machine learning, data mining and databases. The common theme of these problems is that when the dimensionality increases, the volume of the space increases so fast that the available data become sparse. In order to obtain a reliable result, the amount of data needed often grows exponentially with the dimensionality. Also, organizing and searching data often relies on detecting areas where objects form groups with similar properties; in high dimensional data, however, all objects appear to be sparse and dissimilar in many ways, which prevents common data organization strategies from being efficient."
Local outlier factor,"In anomaly detection, the local outlier factor (LOF) is an algorithm proposed by Markus M. Breunig, Hans-Peter Kriegel, Raymond T. Ng and Jörg Sander in 2000 for finding anomalous data points by measuring the local deviation of a given data point with respect to its neighbours.LOF shares some concepts with DBSCAN and OPTICS such as the concepts of ""core distance"" and ""reachability distance"", which are used for local density estimation."
two-class linear classifiers,"In the field of machine learning, the goal of statistical classification is to use an object's characteristics to identify which class (or group) it belongs to.  A linear classifier achieves this by making a classification decision based on the value of a linear combination of the characteristics.  An object's characteristics are also known as feature values and are typically presented to the machine in a vector called a feature vector. Such classifiers work well for practical problems such as document classification, and more generally for problems with many variables (features), reaching accuracy levels comparable to non-linear classifiers while taking less time to train and use."
Linear classifier,"In the field of machine learning, the goal of statistical classification is to use an object's characteristics to identify which class (or group) it belongs to.  A linear classifier achieves this by making a classification decision based on the value of a linear combination of the characteristics.  An object's characteristics are also known as feature values and are typically presented to the machine in a vector called a feature vector. Such classifiers work well for practical problems such as document classification, and more generally for problems with many variables (features), reaching accuracy levels comparable to non-linear classifiers while taking less time to train and use."
Naive Bayes classifier,"In statistics, naive Bayes classifiers are a family of simple ""probabilistic classifiers"" based on applying Bayes' theorem with strong (naive) independence assumptions between the features (see Bayes classifier). They are among the simplest Bayesian network models, but coupled with kernel density estimation, they can achieve high accuracy levels.Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features/predictors) in a learning problem. Maximum-likelihood training can be done by evaluating a closed-form expression,: 718  which takes linear time, rather than by expensive iterative approximation as used for many other types of classifiers.
In the statistics literature, naive Bayes models are known under a variety of names, including simple Bayes and independence Bayes. All these names reference the use of Bayes' theorem in the classifier's decision rule, but naive Bayes is not (necessarily) a Bayesian method."
Perceptron,"In machine learning, the perceptron (or McCulloch-Pitts neuron) is an algorithm for supervised learning of binary classifiers.  A binary classifier is a function which can decide whether or not an input, represented by a vector of numbers, belongs to some specific class.  It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector."
Quadratic classifier,"In statistics, a quadratic classifier is a statistical classifier that uses a quadratic decision surface to separate measurements of two or more classes of objects or events. It is a more general version of the linear classifier."
Generative model,"In statistical classification, two main approaches are called the generative approach and the discriminative approach. These compute classifiers by different approaches, differing in the degree of statistical modelling. Terminology is inconsistent, but three major types can be distinguished, following Jebara (2004):

A generative model is a statistical model of the joint probability distribution 
  
    
      
        P
        (
        X
        ,
        Y
        )
      
    
    {\displaystyle P(X,Y)}
   on given observable variable X and target variable Y;
A discriminative model is a model of the conditional probability 
  
    
      
        P
        (
        Y
        ∣
        X
        =
        x
        )
      
    
    {\displaystyle P(Y\mid X=x)}
   of the target Y, given an observation x; and
Classifiers computed without using a probability model are also referred to loosely as ""discriminative"".The distinction between these last two classes is not consistently made; Jebara (2004) refers to these three classes as generative learning, conditional learning, and discriminative learning, but Ng & Jordan (2002) only distinguish two classes, calling them generative classifiers (joint distribution) and discriminative classifiers (conditional distribution or no distribution), not distinguishing between the latter two classes. Analogously, a classifier based on a generative model is a generative classifier, while a classifier based on a discriminative model is a discriminative classifier, though this term also refers to classifiers that are not based on a model.
Standard examples of each, all of which are linear classifiers, are:

generative classifiers:
naive Bayes classifier and
linear discriminant analysis
discriminative model:
logistic regressionIn application to classification, one wishes to go from an observation x to a label y (or probability distribution on labels). One can compute this directly, without using a probability distribution (distribution-free classifier); one can estimate the probability of a label given an observation, 
  
    
      
        P
        (
        Y
        
          |
        
        X
        =
        x
        )
      
    
    {\displaystyle P(Y|X=x)}
   (discriminative model), and base classification on that; or one can estimate the joint distribution 
  
    
      
        P
        (
        X
        ,
        Y
        )
      
    
    {\displaystyle P(X,Y)}
   (generative model), from that compute the conditional probability 
  
    
      
        P
        (
        Y
        
          |
        
        X
        =
        x
        )
      
    
    {\displaystyle P(Y|X=x)}
  , and then base classification on that. These are increasingly indirect, but increasingly probabilistic, allowing more domain knowledge and probability theory to be applied. In practice different approaches are used, depending on the particular problem, and hybrids can combine strengths of multiple approaches."
Statistical classification,"In statistics, classification is the problem of identifying which of a set of categories (sub-populations) an observation (or observations) belongs to.  Examples are assigning a given email to the ""spam"" or ""non-spam"" class, and assigning a diagnosis to a given patient based on observed characteristics of the patient (sex, blood pressure, presence or absence of certain symptoms, etc.).
Often, the individual observations are analyzed into a set of quantifiable properties, known variously as explanatory variables or features.  These properties may variously be categorical (e.g. ""A"", ""B"", ""AB"" or ""O"", for blood type), ordinal (e.g. ""large"", ""medium"" or ""small""), integer-valued (e.g. the number of occurrences of a particular word in an email) or real-valued (e.g. a measurement of blood pressure). Other classifiers work by comparing observations to previous observations by means of a similarity or distance function.
An algorithm that implements classification, especially in a concrete implementation, is known as a classifier.  The term ""classifier"" sometimes also refers to the mathematical function, implemented by a classification algorithm, that maps input data to a category.
Terminology across fields is quite varied. In statistics, where classification is often done with logistic regression or a similar procedure, the properties of observations are termed explanatory variables (or independent variables, regressors, etc.), and the categories to be predicted are known as outcomes, which are considered to be possible values of the dependent variable.  In machine learning, the observations are often known as instances, the explanatory variables are termed features (grouped into a feature vector), and the possible categories to be predicted are classes.  Other fields may use different terminology: e.g. in community ecology, the term ""classification"" normally refers to cluster analysis."
Linear discriminant analysis,"Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics and other fields, to find a linear combination of features that characterizes or separates two or more classes of objects or events. The resulting combination may be used as a linear classifier, or, more commonly, for dimensionality reduction before later classification.
LDA is closely related to analysis of variance (ANOVA) and regression analysis, which also attempt to express one dependent variable as a linear combination of other features or measurements. However, ANOVA uses categorical independent variables and a continuous dependent variable, whereas discriminant analysis has continuous independent variables and a categorical dependent variable (i.e. the class label). Logistic regression and probit regression are more similar to LDA than ANOVA is, as they also explain a categorical variable by the values of continuous independent variables. These other methods are preferable in applications where it is not reasonable to assume that the independent variables are normally distributed, which is a fundamental assumption of the LDA method.
LDA is also closely related to principal component analysis (PCA) and factor analysis in that they both look for linear combinations of variables which best explain the data. LDA explicitly attempts to model the difference between the classes of data. PCA, in contrast, does not take into account any difference in class, and factor analysis builds the feature combinations based on differences rather than similarities. Discriminant analysis is also different from factor analysis in that it is not an interdependence technique: a distinction between independent variables and dependent variables (also called criterion variables) must be made.
LDA works when the measurements made on independent variables for each observation are continuous quantities. When dealing with categorical independent variables, the equivalent technique is discriminant correspondence analysis.Discriminant analysis is used when groups are known a priori (unlike in cluster analysis). Each case must have a score on one or more quantitative predictor measures, and a score on a group measure. In simple terms, discriminant function analysis is classification - the act of distributing things into groups, classes or categories of the same type."
Linear separability,"In Euclidean geometry, linear separability is a property of two sets of points. This is most easily visualized in two dimensions (the Euclidean plane) by thinking of one set of points as being colored blue and the other set of points as being colored red. These two sets are linearly separable if there exists at least one line in the plane with all of the blue points on one side of the line and all the red points on the other side. This idea immediately generalizes to higher-dimensional Euclidean spaces if the line is replaced by a hyperplane.
The problem of determining if a pair of sets is linearly separable and finding a separating hyperplane if they are, arises in several areas.  In statistics and machine learning, classifying certain types of data is a problem for which good algorithms exist that are based on this concept."
Ensemble learning,"In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.
Unlike a statistical ensemble in statistical mechanics, which is usually infinite, a machine learning ensemble consists of only a concrete finite set of alternative models, but typically allows for much more flexible structure to exist among those alternatives."
Support vector machine,"In machine learning, support vector machines (SVMs, also support vector networks) are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis. Developed at AT&T Bell Laboratories by Vladimir Vapnik with colleagues (Boser et al., 1992, Guyon et al., 1993, Cortes and Vapnik, 1995, Vapnik et al., 1997) SVMs are one of the most robust prediction methods, being based on statistical learning frameworks or VC theory proposed by Vapnik (1982, 1995) and Chervonenkis (1974). Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting). SVM maps training examples to points in space so as to maximise the width of the gap between the two categories. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.
In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.
When data are unlabelled, supervised learning is not possible, and an unsupervised learning approach is required, which attempts to find natural clustering of the data to groups, and then map new data to these formed groups. The support vector clustering algorithm, created by Hava Siegelmann and Vladimir Vapnik, applies the statistics of support vectors, developed in the support vector machines algorithm, to categorize unlabeled data."
bias-variance tradeoff,"In statistics and machine learning, the bias–variance tradeoff is the property of a model that the variance of the parameter estimated across samples can be reduced by increasing the bias in the estimated parameters.
The bias–variance dilemma or bias–variance problem is the conflict in trying to simultaneously minimize these two sources of error that prevent supervised learning algorithms from generalizing beyond their training set:
The bias error is an error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).
The variance is an error from sensitivity to small fluctuations in the training set. High variance may result from an algorithm modeling the random noise in the training data (overfitting).The bias–variance decomposition is a way of analyzing a learning algorithm's expected generalization error with respect to a particular problem as a sum of three terms, the bias, variance, and a quantity called the irreducible error, resulting from noise in the problem itself."
Bias–variance tradeoff,"In statistics and machine learning, the bias–variance tradeoff is the property of a model that the variance of the parameter estimated across samples can be reduced by increasing the bias in the estimated parameters.
The bias–variance dilemma or bias–variance problem is the conflict in trying to simultaneously minimize these two sources of error that prevent supervised learning algorithms from generalizing beyond their training set:
The bias error is an error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).
The variance is an error from sensitivity to small fluctuations in the training set. High variance may result from an algorithm modeling the random noise in the training data (overfitting).The bias–variance decomposition is a way of analyzing a learning algorithm's expected generalization error with respect to a particular problem as a sum of three terms, the bias, variance, and a quantity called the irreducible error, resulting from noise in the problem itself."
Supervised learning,"Supervised learning (SL) is a machine learning paradigm for problems where the available data consists of labelled examples, meaning that each data point contains features (covariates) and an associated label. The goal of supervised learning algorithms is learning a function that maps feature vectors (inputs) to labels (output), based on example input-output pairs. It infers a function from labeled training data consisting of a set of training examples. In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a ""reasonable"" way (see inductive bias). This statistical quality of an algorithm is measured through the so-called generalization error."
Overfitting,"In mathematical modeling, overfitting is ""the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit to additional data or predict future observations reliably"". An overfitted model is a mathematical model that contains more parameters than can be justified by the data. The essence of overfitting is to have unknowingly extracted some of the residual variation (i.e., the noise) as if that variation represented underlying model structure.: 45 Underfitting occurs when a mathematical model cannot adequately capture the underlying structure of the data. An under-fitted model is a model where some parameters or terms that would appear in a correctly specified model are missing. Under-fitting would occur, for example, when fitting a linear model to non-linear data. Such a model will tend to have poor predictive performance.
The possibility of over-fitting exists because the criterion used for selecting the model is not the same as the criterion used to judge the suitability of a model. For example, a model might be selected by maximizing its performance on some set of training data, and yet its suitability might be determined by its ability to perform well on unseen data; then over-fitting occurs when a model begins to ""memorize"" training data rather than ""learning"" to generalize from a trend. 
As an extreme example, if the number of parameters is the same as or greater than the number of observations, then a model can perfectly predict the training data simply by memorizing the data in its entirety. (For an illustration, see Figure 2.) Such a model, though, will typically fail severely when making predictions. 
The potential for overfitting depends not only on the number of parameters and data but also the conformability of the model structure with the data shape, and the magnitude of model error compared to the expected level of noise or error in the data. Even when the fitted model does not have an excessive number of parameters, it is to be expected that the fitted relationship will appear to perform less well on a new data set than on the data set used for fitting (a phenomenon sometimes known as shrinkage). In particular, the value of the coefficient of determination will shrink relative to the original data.
To lessen the chance or amount of overfitting, several techniques are available (e.g., model comparison, cross-validation, regularization, early stopping, pruning, Bayesian priors, or dropout). The basis of some techniques is either (1) to explicitly penalize overly complex models or (2) to test the model's ability to generalize by evaluating its performance on a set of data not used for training, which is assumed to approximate the typical unseen data that a model will encounter."
Mean squared error,"In statistics, the mean squared error (MSE) or mean squared deviation (MSD) of an estimator (of a procedure for estimating an unobserved quantity) measures the average of the squares of the errors—that is, the average squared difference between the estimated values and the actual value. MSE is a risk function, corresponding to the expected value of the squared error loss. The fact that MSE is almost always strictly positive (and not zero) is because of randomness or because the estimator does not account for information that could produce a more accurate estimate. In machine learning, specifically empirical risk minimization, MSE may refer to the empirical risk (the average loss on an observed data set), as an estimate of the true MSE (the true risk: the average loss on the actual population distribution).
The MSE is a measure of the quality of an estimator.  As it is derived from the square of Euclidean distance, it is always a positive value that decreases as the error approaches zero.
The MSE is the second moment (about the origin) of the error, and thus incorporates both the variance of the estimator (how widely spread the estimates are from one data sample to another) and its bias (how far off the average estimated value is from the true value). For an unbiased estimator, the MSE is the variance of the estimator. Like the variance, MSE has the same units of measurement as the square of the quantity being estimated. In an analogy to standard deviation, taking the square root of MSE yields the root-mean-square error or root-mean-square deviation (RMSE or RMSD), which has the same units as the quantity being estimated; for an unbiased estimator, the RMSE is the square root of the variance, known as the standard error."
Less-is-more effect,The less-is-more effect refers to the finding that heuristic decision strategies can yield more accurate judgments than alternative strategies that use more pieces of information. Understanding these effects is part of the study of ecological rationality.
Exponential distribution,"In probability theory and statistics, the exponential distribution is the probability distribution of the time between events in a Poisson point process, i.e., a process in which events occur continuously and independently at a constant average rate. It is a particular case of the gamma distribution. It is the continuous analogue of the geometric distribution, and it has the key property of being memoryless. In addition to being used for the analysis of Poisson point processes it is found in various other contexts.
The exponential distribution is not the same as the class of exponential families of distributions. This is a large class of probability distributions that includes the exponential distribution as one of its members, but also includes many other distributions, like the normal, binomial, gamma, and Poisson distributions."
Trade-off,"A trade-off (or tradeoff) is a situational decision that involves diminishing or losing one quality, quantity, or property of a set or design in return for gains in other aspects. In simple terms, a tradeoff is where one thing increases, and another must decrease. Tradeoffs stem from limitations of many origins, including simple physics – for instance, only a certain volume of objects can fit into a given space, so a full container must remove some items in order to accept any more, and vessels can carry a few large items or multiple small items. Tradeoffs also commonly refer to different configurations of a single item, such as the tuning of strings on a guitar to enable different notes to be played, as well as an allocation of time and attention towards different tasks.
The concept of a tradeoff suggests a tactical or strategic choice made with full comprehension of the advantages and disadvantages of each setup. An economic example is the decision to invest in stocks, which are risky but carry great potential return, versus bonds, which are generally safer but with lower potential returns."
Double descent,"In statistics and machine learning, double descent is the phenomenon where a statistical model with a small number of parameters and a model with an extremely large number of parameters have a small error, but a model whose number of parameters is about the same as the number of data points used to train the model will have a large error. This phenomenon seems to contradict the bias-variance tradeoff in classical statistics, which states that having too many parameters will yield an extremely large error."
Ridge regression,"Ridge regression is a method of estimating the coefficients of multiple-regression models in scenarios where the independent variables are highly correlated. It has been used in many fields including econometrics, chemistry, and engineering. Also known as Tikhonov regularization, named for Andrey Tikhonov, it is a method of regularization of ill-posed problems. it is particularly useful to mitigate the problem of multicollinearity in linear regression, which commonly occurs in models with large numbers of parameters. In general, the method provides improved efficiency in parameter estimation problems in exchange for a tolerable amount of bias (see bias–variance tradeoff).The theory was first introduced by Hoerl and Kennard in 1970 in their Technometrics papers “RIDGE regressions: biased estimation of nonorthogonal problems” and “RIDGE regressions: applications in nonorthogonal problems”. This was the result of ten years of research into the field of ridge analysis.Ridge regression was developed as a possible solution to the imprecision of least square estimators when linear regression models have some multicollinear (highly correlated) independent variables—by creating a ridge regression estimator (RR). This provides a more precise ridge parameters estimate, as its variance and mean square estimator are often smaller than the least square estimators previously derived."
Minimum-variance unbiased estimator,"In statistics a minimum-variance unbiased estimator (MVUE) or uniformly minimum-variance unbiased estimator (UMVUE) is an unbiased estimator that has lower variance than any other unbiased estimator for all possible values of the parameter.
For practical statistics problems, it is important to determine the MVUE if one exists, since less-than-optimal procedures would naturally be avoided, other things being equal. This has led to substantial development of statistical theory related to the problem of optimal estimation.
While combining the constraint of unbiasedness with the desirability metric of least variance leads to good results in most practical settings—making MVUE a natural starting point for a broad range of analyses—a targeted specification may perform better for a given problem; thus, MVUE is not always the best stopping point."
support vector machines,"In machine learning, support vector machines (SVMs, also support vector networks) are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis. Developed at AT&T Bell Laboratories by Vladimir Vapnik with colleagues (Boser et al., 1992, Guyon et al., 1993, Cortes and Vapnik, 1995, Vapnik et al., 1997) SVMs are one of the most robust prediction methods, being based on statistical learning frameworks or VC theory proposed by Vapnik (1982, 1995) and Chervonenkis (1974). Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting). SVM maps training examples to points in space so as to maximise the width of the gap between the two categories. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.
In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.
When data are unlabelled, supervised learning is not possible, and an unsupervised learning approach is required, which attempts to find natural clustering of the data to groups, and then map new data to these formed groups. The support vector clustering algorithm, created by Hava Siegelmann and Vladimir Vapnik, applies the statistics of support vectors, developed in the support vector machines algorithm, to categorize unlabeled data."
Support vector machine,"In machine learning, support vector machines (SVMs, also support vector networks) are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis. Developed at AT&T Bell Laboratories by Vladimir Vapnik with colleagues (Boser et al., 1992, Guyon et al., 1993, Cortes and Vapnik, 1995, Vapnik et al., 1997) SVMs are one of the most robust prediction methods, being based on statistical learning frameworks or VC theory proposed by Vapnik (1982, 1995) and Chervonenkis (1974). Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting). SVM maps training examples to points in space so as to maximise the width of the gap between the two categories. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.
In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.
When data are unlabelled, supervised learning is not possible, and an unsupervised learning approach is required, which attempts to find natural clustering of the data to groups, and then map new data to these formed groups. The support vector clustering algorithm, created by Hava Siegelmann and Vladimir Vapnik, applies the statistics of support vectors, developed in the support vector machines algorithm, to categorize unlabeled data."
Least-squares support vector machine,"In machine learning, support vector machines (SVMs, also support vector networks) are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis. Developed at AT&T Bell Laboratories by Vladimir Vapnik with colleagues (Boser et al., 1992, Guyon et al., 1993, Cortes and Vapnik, 1995, Vapnik et al., 1997) SVMs are one of the most robust prediction methods, being based on statistical learning frameworks or VC theory proposed by Vapnik (1982, 1995) and Chervonenkis (1974). Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting). SVM maps training examples to points in space so as to maximise the width of the gap between the two categories. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.
In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.
When data are unlabelled, supervised learning is not possible, and an unsupervised learning approach is required, which attempts to find natural clustering of the data to groups, and then map new data to these formed groups. The support vector clustering algorithm, created by Hava Siegelmann and Vladimir Vapnik, applies the statistics of support vectors, developed in the support vector machines algorithm, to categorize unlabeled data."
Kernel method,"In machine learning, kernel machines are a class of algorithms for pattern analysis, whose best known member is the support-vector machine (SVM). The general task of pattern analysis is to find and study general types of relations (for example clusters, rankings, principal components, correlations, classifications) in datasets. For many algorithms that solve these tasks, the data in raw representation have to be explicitly transformed into feature vector representations via a user-specified feature map: in contrast, kernel methods require only a user-specified kernel, i.e., a similarity function over all pairs of data points computed using Inner products. The feature map in kernel machines is infinite dimensional but only requires a finite dimensional matrix from user-input according to the Representer theorem. Kernel machines are slow to compute for datasets larger than a couple of thousand examples without parallel processing. 
Kernel methods  owe their name to the use of kernel functions, which enable them to operate in a high-dimensional, implicit feature space without ever computing the coordinates of the data in that space, but rather by simply computing the inner products between the images of all pairs of data in the feature space.  This operation is often computationally cheaper than the explicit computation of the coordinates.  This approach is called the ""kernel trick"". Kernel functions have been introduced for sequence data, graphs, text, images, as well as vectors.
Algorithms capable of operating with kernels include the kernel perceptron, support-vector machines (SVM), Gaussian processes, principal components analysis (PCA), canonical correlation analysis, ridge regression, spectral clustering, linear adaptive filters and many others.
Most kernel algorithms are based on convex optimization or eigenproblems and are statistically well-founded. Typically, their statistical properties are analyzed using statistical learning theory (for example, using Rademacher complexity)."
Structured support vector machine,"The structured support-vector machine is a machine learning algorithm that generalizes the Support-Vector Machine (SVM) classifier.  Whereas the SVM classifier supports binary classification, multiclass classification and regression, the structured SVM allows training of a classifier for general structured output labels.
As an example, a sample instance might be a natural language sentence, and the output label is an annotated parse tree.  Training a classifier consists of showing pairs of correct sample and output label pairs.  After training, the structured SVM model allows one to predict for new sample instances the corresponding output label; that is, given a natural language sentence, the classifier can produce the most likely parse tree.

"
Relevance vector machine,"In mathematics, a Relevance Vector Machine (RVM) is a machine learning technique that uses Bayesian inference to obtain parsimonious solutions for regression and probabilistic classification.
The RVM has an identical functional form to the support vector machine, but provides probabilistic classification.
It is actually equivalent to a Gaussian process model with covariance function:

  
    
      
        k
        (
        
          x
        
        ,
        
          
            x
            ′
          
        
        )
        =
        
          ∑
          
            j
            =
            1
          
          
            N
          
        
        
          
            1
            
              α
              
                j
              
            
          
        
        φ
        (
        
          x
        
        ,
        
          
            x
          
          
            j
          
        
        )
        φ
        (
        
          
            x
          
          ′
        
        ,
        
          
            x
          
          
            j
          
        
        )
      
    
    {\displaystyle k(\mathbf {x} ,\mathbf {x'} )=\sum _{j=1}^{N}{\frac {1}{\alpha _{j}}}\varphi (\mathbf {x} ,\mathbf {x} _{j})\varphi (\mathbf {x} ',\mathbf {x} _{j})}
  where 
  
    
      
        φ
      
    
    {\displaystyle \varphi }
   is the kernel function (usually Gaussian), 
  
    
      
        
          α
          
            j
          
        
      
    
    {\displaystyle \alpha _{j}}
   are the variances of the prior on the weight vector

  
    
      
        w
        ∼
        N
        (
        0
        ,
        
          α
          
            −
            1
          
        
        I
        )
      
    
    {\displaystyle w\sim N(0,\alpha ^{-1}I)}
  , and 
  
    
      
        
          
            x
          
          
            1
          
        
        ,
        …
        ,
        
          
            x
          
          
            N
          
        
      
    
    {\displaystyle \mathbf {x} _{1},\ldots ,\mathbf {x} _{N}}
   are the input vectors of the training set.Compared to that of support vector machines (SVM), the Bayesian formulation of the RVM avoids the set of free parameters of the SVM (that usually require cross-validation-based post-optimizations). However RVMs use an expectation maximization (EM)-like learning method and are therefore at risk of local minima. This is unlike the standard sequential minimal optimization (SMO)-based algorithms employed by SVMs, which are guaranteed to find a global optimum (of the convex problem).
The relevance vector machine was patented in the United States by Microsoft (patent expired September 4, 2019)."
Elastic net regularization,"In statistics and, in particular, in the fitting of linear or logistic regression models, the elastic net is a regularized regression method that linearly combines the L1 and L2 penalties of the lasso and ridge methods."
Supervised learning,"Supervised learning (SL) is a machine learning paradigm for problems where the available data consists of labelled examples, meaning that each data point contains features (covariates) and an associated label. The goal of supervised learning algorithms is learning a function that maps feature vectors (inputs) to labels (output), based on example input-output pairs. It infers a function from labeled training data consisting of a set of training examples. In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a ""reasonable"" way (see inductive bias). This statistical quality of an algorithm is measured through the so-called generalization error."
Multiclass classification,"In machine learning and statistical classification, multiclass classification or multinomial classification is the problem of classifying instances into one of three or more classes (classifying instances into one of two classes is called binary classification).
While many classification algorithms (notably multinomial logistic regression) naturally permit the use of more than two classes, some are by nature binary algorithms; these can, however, be turned into multinomial classifiers by a variety of strategies.
Multiclass classification should not be confused with multi-label classification, where multiple labels are to be predicted for each instance.

"
Isabelle Guyon,"Isabelle Guyon (French pronunciation: ​[izabɛl ɡɥijɔ̃]; born August 15, 1961) is a French-born researcher in machine learning known for her work on support-vector machines, artificial neural networks and bioinformatics. She is a Chair Professor at the University of Paris-Saclay.She is considered to be a pioneer in the field, with her contribution to the support-vector machines with Vladimir Vapnik and Bernhard Boser."
Radial basis function kernel,"In machine learning, the radial basis function kernel, or RBF kernel, is a popular kernel function used in various kernelized learning algorithms. In particular, it is commonly used in support vector machine classification.The RBF kernel on two samples 
  
    
      
        
          x
        
        ∈
        
          
            R
          
          
            k
          
        
      
    
    {\displaystyle \mathbf {x} \in \mathbb {R} ^{k}}
   and x', represented as feature vectors in some input space, is defined as

  
    
      
        K
        (
        
          x
        
        ,
        
          
            x
            ′
          
        
        )
        =
        exp
        ⁡
        
          (
          
            −
            
              
                
                  ‖
                  
                    x
                  
                  −
                  
                    
                      x
                      ′
                    
                  
                  
                    ‖
                    
                      2
                    
                  
                
                
                  2
                  
                    σ
                    
                      2
                    
                  
                
              
            
          
          )
        
      
    
    {\displaystyle K(\mathbf {x} ,\mathbf {x'} )=\exp \left(-{\frac {\|\mathbf {x} -\mathbf {x'} \|^{2}}{2\sigma ^{2}}}\right)}
  
  
    
      
        
          ‖
          
            x
          
          −
          
            
              x
              ′
            
          
          
            ‖
            
              2
            
          
        
      
    
    {\displaystyle \textstyle \|\mathbf {x} -\mathbf {x'} \|^{2}}
   may be recognized as the squared Euclidean distance between the two feature vectors. 
  
    
      
        σ
      
    
    {\displaystyle \sigma }
   is a free parameter. An equivalent definition involves a parameter 
  
    
      
        
          γ
          =
          
            
              
                1
                
                  2
                  
                    σ
                    
                      2
                    
                  
                
              
            
          
        
      
    
    {\displaystyle \textstyle \gamma ={\tfrac {1}{2\sigma ^{2}}}}
  :

  
    
      
        K
        (
        
          x
        
        ,
        
          
            x
            ′
          
        
        )
        =
        exp
        ⁡
        (
        −
        γ
        ‖
        
          x
        
        −
        
          
            x
            ′
          
        
        
          ‖
          
            2
          
        
        )
      
    
    {\displaystyle K(\mathbf {x} ,\mathbf {x'} )=\exp(-\gamma \|\mathbf {x} -\mathbf {x'} \|^{2})}
  Since the value of the RBF kernel decreases with distance and ranges between zero (in the limit) and one (when x = x'), it has a ready interpretation as a similarity measure.
The feature space of the kernel has an infinite number of dimensions; for 
  
    
      
        σ
        =
        1
      
    
    {\displaystyle \sigma =1}
  , its expansion using the multinomial theorem is:

  
    
      
        
          
            
              
                exp
                ⁡
                
                  (
                  
                    −
                    
                      
                        1
                        2
                      
                    
                    ‖
                    
                      x
                    
                    −
                    
                      
                        x
                        ′
                      
                    
                    
                      ‖
                      
                        2
                      
                    
                  
                  )
                
              
              
                
                =
                exp
                ⁡
                (
                
                  
                    2
                    2
                  
                
                
                  
                    x
                  
                  
                    ⊤
                  
                
                
                  
                    x
                    ′
                  
                
                −
                
                  
                    1
                    2
                  
                
                ‖
                
                  x
                
                
                  ‖
                  
                    2
                  
                
                −
                
                  
                    1
                    2
                  
                
                ‖
                
                  
                    x
                    ′
                  
                
                
                  ‖
                  
                    2
                  
                
                )
              
            
            
              
              
                
                =
                exp
                ⁡
                (
                
                  
                    x
                  
                  
                    ⊤
                  
                
                
                  
                    x
                    ′
                  
                
                )
                exp
                ⁡
                (
                −
                
                  
                    1
                    2
                  
                
                ‖
                
                  x
                
                
                  ‖
                  
                    2
                  
                
                )
                exp
                ⁡
                (
                −
                
                  
                    1
                    2
                  
                
                ‖
                
                  
                    x
                    ′
                  
                
                
                  ‖
                  
                    2
                  
                
                )
              
            
            
              
              
                
                =
                
                  ∑
                  
                    j
                    =
                    0
                  
                  
                    ∞
                  
                
                
                  
                    
                      (
                      
                        
                          x
                        
                        
                          ⊤
                        
                      
                      
                        
                          x
                          ′
                        
                      
                      
                        )
                        
                          j
                        
                      
                    
                    
                      j
                      !
                    
                  
                
                exp
                ⁡
                
                  (
                  
                    −
                    
                      
                        1
                        2
                      
                    
                    ‖
                    
                      x
                    
                    
                      ‖
                      
                        2
                      
                    
                  
                  )
                
                exp
                ⁡
                
                  (
                  
                    −
                    
                      
                        1
                        2
                      
                    
                    ‖
                    
                      
                        x
                        ′
                      
                    
                    
                      ‖
                      
                        2
                      
                    
                  
                  )
                
              
            
            
              
              
                
                =
                
                  ∑
                  
                    j
                    =
                    0
                  
                  
                    ∞
                  
                
                
                
                  ∑
                  
                    
                      n
                      
                        1
                      
                    
                    +
                    
                      n
                      
                        2
                      
                    
                    +
                    ⋯
                    +
                    
                      n
                      
                        k
                      
                    
                    =
                    j
                  
                
                exp
                ⁡
                
                  (
                  
                    −
                    
                      
                        1
                        2
                      
                    
                    ‖
                    
                      x
                    
                    
                      ‖
                      
                        2
                      
                    
                  
                  )
                
                
                  
                    
                      
                        x
                        
                          1
                        
                        
                          
                            n
                            
                              1
                            
                          
                        
                      
                      ⋯
                      
                        x
                        
                          k
                        
                        
                          
                            n
                            
                              k
                            
                          
                        
                      
                    
                    
                      
                        n
                        
                          1
                        
                      
                      !
                      ⋯
                      
                        n
                        
                          k
                        
                      
                      !
                    
                  
                
                exp
                ⁡
                
                  (
                  
                    −
                    
                      
                        1
                        2
                      
                    
                    ‖
                    
                      
                        x
                        ′
                      
                    
                    
                      ‖
                      
                        2
                      
                    
                  
                  )
                
                
                  
                    
                      
                        
                          
                            x
                            ′
                          
                        
                        
                          1
                        
                        
                          
                            n
                            
                              1
                            
                          
                        
                      
                      ⋯
                      
                        
                          
                            x
                            ′
                          
                        
                        
                          k
                        
                        
                          
                            n
                            
                              k
                            
                          
                        
                      
                    
                    
                      
                        n
                        
                          1
                        
                      
                      !
                      ⋯
                      
                        n
                        
                          k
                        
                      
                      !
                    
                  
                
              
            
            
              
              
                
                =
                ⟨
                φ
                (
                
                  x
                
                )
                ,
                φ
                (
                
                  
                    x
                    ′
                  
                
                )
                ⟩
              
            
          
        
      
    
    {\displaystyle {\begin{alignedat}{2}\exp \left(-{\frac {1}{2}}\|\mathbf {x} -\mathbf {x'} \|^{2}\right)&=\exp({\frac {2}{2}}\mathbf {x} ^{\top }\mathbf {x'} -{\frac {1}{2}}\|\mathbf {x} \|^{2}-{\frac {1}{2}}\|\mathbf {x'} \|^{2})\\&=\exp(\mathbf {x} ^{\top }\mathbf {x'} )\exp(-{\frac {1}{2}}\|\mathbf {x} \|^{2})\exp(-{\frac {1}{2}}\|\mathbf {x'} \|^{2})\\&=\sum _{j=0}^{\infty }{\frac {(\mathbf {x} ^{\top }\mathbf {x'} )^{j}}{j!}}\exp \left(-{\frac {1}{2}}\|\mathbf {x} \|^{2}\right)\exp \left(-{\frac {1}{2}}\|\mathbf {x'} \|^{2}\right)\\&=\sum _{j=0}^{\infty }\quad \sum _{n_{1}+n_{2}+\dots +n_{k}=j}\exp \left(-{\frac {1}{2}}\|\mathbf {x} \|^{2}\right){\frac {x_{1}^{n_{1}}\cdots x_{k}^{n_{k}}}{\sqrt {n_{1}!\cdots n_{k}!}}}\exp \left(-{\frac {1}{2}}\|\mathbf {x'} \|^{2}\right){\frac {{x'}_{1}^{n_{1}}\cdots {x'}_{k}^{n_{k}}}{\sqrt {n_{1}!\cdots n_{k}!}}}\\&=\langle \varphi (\mathbf {x} ),\varphi (\mathbf {x'} )\rangle \end{alignedat}}}
  
  
    
      
        φ
        (
        
          x
        
        )
        =
        exp
        ⁡
        
          (
          
            −
            
              
                1
                2
              
            
            ‖
            
              x
            
            
              ‖
              
                2
              
            
          
          )
        
        
          (
          
            
              a
              
                
                  l
                  
                    0
                  
                
              
              
                (
                0
                )
              
            
            ,
            
              a
              
                1
              
              
                (
                1
                )
              
            
            ,
            …
            ,
            
              a
              
                
                  l
                  
                    1
                  
                
              
              
                (
                1
                )
              
            
            ,
            …
            ,
            
              a
              
                1
              
              
                (
                j
                )
              
            
            ,
            …
            ,
            
              a
              
                
                  l
                  
                    j
                  
                
              
              
                (
                j
                )
              
            
            ,
            …
          
          )
        
      
    
    {\displaystyle \varphi (\mathbf {x} )=\exp \left(-{\frac {1}{2}}\|\mathbf {x} \|^{2}\right)\left(a_{l_{0}}^{(0)},a_{1}^{(1)},\dots ,a_{l_{1}}^{(1)},\dots ,a_{1}^{(j)},\dots ,a_{l_{j}}^{(j)},\dots \right)}
  where 
  
    
      
        
          l
          
            j
          
        
        =
        
          
            
              
                (
              
              
                
                  k
                  +
                  j
                  −
                  1
                
                j
              
              
                )
              
            
          
        
      
    
    {\displaystyle l_{j}={\tbinom {k+j-1}{j}}}
  ,

  
    
      
        
          a
          
            l
          
          
            (
            j
            )
          
        
        =
        
          
            
              
                x
                
                  1
                
                
                  
                    n
                    
                      1
                    
                  
                
              
              ⋯
              
                x
                
                  k
                
                
                  
                    n
                    
                      k
                    
                  
                
              
            
            
              
                n
                
                  1
                
              
              !
              ⋯
              
                n
                
                  k
                
              
              !
            
          
        
        
        
          |
        
        
        
          n
          
            1
          
        
        +
        
          n
          
            2
          
        
        +
        ⋯
        +
        
          n
          
            k
          
        
        =
        j
        ∧
        1
        ≤
        l
        ≤
        
          l
          
            j
          
        
      
    
    {\displaystyle a_{l}^{(j)}={\frac {x_{1}^{n_{1}}\cdots x_{k}^{n_{k}}}{\sqrt {n_{1}!\cdots n_{k}!}}}\quad |\quad n_{1}+n_{2}+\dots +n_{k}=j\wedge 1\leq l\leq l_{j}}"
classifier effectiveness,"In machine learning, support vector machines (SVMs, also support vector networks) are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis. Developed at AT&T Bell Laboratories by Vladimir Vapnik with colleagues (Boser et al., 1992, Guyon et al., 1993, Cortes and Vapnik, 1995, Vapnik et al., 1997) SVMs are one of the most robust prediction methods, being based on statistical learning frameworks or VC theory proposed by Vapnik (1982, 1995) and Chervonenkis (1974). Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting). SVM maps training examples to points in space so as to maximise the width of the gap between the two categories. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.
In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.
When data are unlabelled, supervised learning is not possible, and an unsupervised learning approach is required, which attempts to find natural clustering of the data to groups, and then map new data to these formed groups. The support vector clustering algorithm, created by Hava Siegelmann and Vladimir Vapnik, applies the statistics of support vectors, developed in the support vector machines algorithm, to categorize unlabeled data."
Support vector machine,"In machine learning, support vector machines (SVMs, also support vector networks) are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis. Developed at AT&T Bell Laboratories by Vladimir Vapnik with colleagues (Boser et al., 1992, Guyon et al., 1993, Cortes and Vapnik, 1995, Vapnik et al., 1997) SVMs are one of the most robust prediction methods, being based on statistical learning frameworks or VC theory proposed by Vapnik (1982, 1995) and Chervonenkis (1974). Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting). SVM maps training examples to points in space so as to maximise the width of the gap between the two categories. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.
In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.
When data are unlabelled, supervised learning is not possible, and an unsupervised learning approach is required, which attempts to find natural clustering of the data to groups, and then map new data to these formed groups. The support vector clustering algorithm, created by Hava Siegelmann and Vladimir Vapnik, applies the statistics of support vectors, developed in the support vector machines algorithm, to categorize unlabeled data."
Ensemble learning,"In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.
Unlike a statistical ensemble in statistical mechanics, which is usually infinite, a machine learning ensemble consists of only a concrete finite set of alternative models, but typically allows for much more flexible structure to exist among those alternatives."
Bristol stool scale,"The Bristol stool scale is a diagnostic medical tool designed to classify the form of human faeces into seven categories. It is used in both clinical and experimental fields.It was developed at the Bristol Royal Infirmary as a clinical assessment tool in 1997, and is widely used as a research tool to evaluate the effectiveness of treatments for various diseases of the bowel, as well as a clinical communication aid; including being part of the diagnostic triad for irritable bowel syndrome."
Margin classifier,"In machine learning, a margin classifier is a classifier which is able to give an associated distance from the decision boundary for each example.  For instance, if a linear classifier (e.g. perceptron or linear discriminant analysis) is used, the distance (typically euclidean distance, though others may be used) of an example from the separating hyperplane is the margin of that example.
The notion of margin is important in several machine learning classification algorithms, as it can be used to bound the generalization error of the classifier.  These bounds are frequently shown using the VC dimension.  Of particular prominence is the generalization error bound on boosting algorithms and support vector machines."
Data classification (business intelligence),"In business intelligence, data classification has close ties to data clustering, but where data clustering is descriptive, data classification is predictive. In essence data classification consists of using variables with known values to predict the unknown or future values of other variables. It can be used in e.g. direct marketing, insurance fraud detection or medical diagnosis.The first step in doing a data classification is to cluster the data set used for category training, to create the wanted number of categories. An algorithm, called the classifier, is then used on the categories, creating a descriptive model for each. These models can then be used to categorize new items in the created classification system."
F-score,"In statistical analysis of binary classification, the F-score or F-measure is a measure of a test's accuracy. It is calculated from the precision and recall of the test, where the precision is the number of true positive results divided by the number of all positive results, including those not identified correctly, and the recall is the number of true positive results divided by the number of all samples that should have been identified as positive. Precision is also known as positive predictive value, and recall is also known as sensitivity in diagnostic binary classification. 
The F1 score is the harmonic mean of the precision and recall. The more generic 
  
    
      
        
          F
          
            β
          
        
      
    
    {\displaystyle F_{\beta }}
   score applies additional weights, valuing one of precision or recall more than the other.
The highest possible value of an F-score is 1.0, indicating perfect precision and recall, and the lowest possible value is 0, if either precision or recall are zero."
Search engine optimization,"Search engine optimization (SEO) is the process of improving the quality and quantity of website traffic to a website or a web page from search engines. SEO targets unpaid traffic (known as ""natural"" or ""organic"" results) rather than direct traffic or paid traffic. Unpaid traffic may originate from different kinds of searches, including image search, video search, academic search, news search, and industry-specific vertical search engines. 
As an Internet marketing strategy, SEO considers how search engines work, the computer-programmed algorithms that dictate search engine behavior, what people search for, the actual search terms or keywords typed into search engines, and which search engines are preferred by their targeted audience. SEO is performed because a website will receive more visitors from a search engine when websites rank higher on the search engine results page (SERP). These visitors can then potentially be converted into customers."
Paleontology,"Paleontology (), also spelled palaeontology or palæontology, is the scientific study of life that existed prior to, and sometimes including, the start of the Holocene epoch (roughly 11,700 years before present). It includes the study of fossils to classify organisms and study their interactions with each other and their environments (their paleoecology). Paleontological observations have been documented as far back as the 5th century BC. The science became established in the 18th century as a result of Georges Cuvier's work on comparative anatomy, and developed rapidly in the 19th century. The term itself originates from Greek παλαιός ('palaios', ""old, ancient""), ὄν ('on', (gen. 'ontos'), ""being, creature""), and λόγος ('logos', ""speech, thought, study"").Paleontology lies on the border between biology and geology, but differs from archaeology in that it excludes the study of anatomically modern humans. It now uses techniques drawn from a wide range of sciences, including biochemistry, mathematics, and engineering. Use of all these techniques has enabled paleontologists to discover much of the evolutionary history of life, almost all the way back to when Earth became capable of supporting life, nearly 4 billion years ago. As knowledge has increased, paleontology has developed specialised sub-divisions, some of which focus on different types of fossil organisms while others study ecology and environmental history, such as ancient climates.
Body fossils and trace fossils are the principal types of evidence about ancient life, and geochemical evidence has helped to decipher the evolution of life before there were organisms large enough to leave body fossils. Estimating the dates of these remains is essential but difficult: sometimes adjacent rock layers allow radiometric dating, which provides absolute dates that are accurate to within 0.5%, but more often paleontologists have to rely on relative dating by solving the ""jigsaw puzzles"" of biostratigraphy (arrangement of rock layers from youngest to oldest). Classifying ancient organisms is also difficult, as many do not fit well into the Linnaean taxonomy classifying living organisms, and paleontologists more often use cladistics to draw up evolutionary ""family trees"". The final quarter of the 20th century saw the development of molecular phylogenetics, which investigates how closely organisms are related by measuring the similarity of the DNA in their genomes. Molecular phylogenetics has also been used to estimate the dates when species diverged, but there is controversy about the reliability of the molecular clock on which such estimates depend."
Precision and recall,"In pattern recognition, information retrieval, object detection and classification (machine learning), precision and recall are performance metrics that apply to data retrieved from a collection, corpus or sample space.
Precision (also called positive predictive value) is the fraction of relevant instances among the retrieved instances, while recall (also known as sensitivity) is the fraction of relevant instances that were retrieved. Both precision and recall are therefore based on relevance.
Consider a computer program for recognizing dogs (the relevant element) in a digital photograph. Upon processing a picture which contains ten cats and twelve dogs, the program identifies eight dogs. Of the eight elements identified as dogs, only five actually are dogs (true positives), while the other three are cats (false positives). Seven dogs were missed (false negatives), and seven cats were correctly excluded (true negatives). The program's precision is then 5/8 (true positives / selected elements) while its recall is 5/12 (true positives / relevant elements).
When a search engine returns 30 pages, only 20 of which are relevant, while failing to return 40 additional relevant pages, its precision is 20/30 = 2/3, which tells us how valid the results are, while its recall is 20/60 = 1/3, which tells us how complete the results are.
Adopting a hypothesis-testing approach from statistics, in which, in this case, the null hypothesis is that a given item is irrelevant, i.e., not a dog, absence of type I and type II errors (i.e. perfect specificity and sensitivity of 100% each) corresponds respectively to perfect precision (no false positive) and perfect recall (no false negative).  
More generally, recall is simply the complement of the type II error rate, i.e. one minus the type II error rate. Precision is related to the type I error rate, but in a slightly more complicated way, as it also depends upon the prior distribution of seeing a relevant vs an irrelevant item.
The above cat and dog example contained 8 − 5 = 3 type I errors (false positives) out of 10 total cats (true negatives), for a type I error rate of 3/10, and 12 − 5 = 7 type II errors, for a type II error rate of 7/12.  Precision can be seen as a measure of quality, and recall as a measure of quantity. 
Higher precision means that an algorithm returns more relevant results than irrelevant ones, and high recall means that an algorithm returns most of the relevant results (whether or not irrelevant ones are also returned)."
ATEX directive,"The ATEX directives are two EU directives describing the minimum safety requirements for workplaces and equipment used in explosive atmospheres. The name is an initialization of the French term Appareils destinés à être utilisés en ATmosphères EXplosibles (French for ""Equipment intended for use in explosive atmospheres"")."
boosted decision trees,"Gradient boosting is a machine learning technique used in regression and classification tasks, among others. It gives a prediction model in the form of an ensemble of weak prediction models, which are typically decision trees. When a decision tree is the weak learner, the resulting algorithm is called gradient-boosted trees; it usually outperforms random forest. A gradient-boosted trees model is built in a stage-wise fashion as in other boosting methods, but it generalizes the other methods by allowing optimization of an arbitrary differentiable loss function."
Gradient boosting,"Gradient boosting is a machine learning technique used in regression and classification tasks, among others. It gives a prediction model in the form of an ensemble of weak prediction models, which are typically decision trees. When a decision tree is the weak learner, the resulting algorithm is called gradient-boosted trees; it usually outperforms random forest. A gradient-boosted trees model is built in a stage-wise fashion as in other boosting methods, but it generalizes the other methods by allowing optimization of an arbitrary differentiable loss function."
decision boundary,"In a statistical-classification problem with two classes, a decision boundary or decision surface is a hypersurface that partitions the underlying vector space into two sets, one for each class. The classifier will classify all the points on one side of the decision boundary as belonging to one class and all those on the other side as belonging to the other class.
A decision boundary is the region of a problem space in which the output label of a classifier is ambiguous.If the decision surface is a hyperplane, then the classification problem is linear, and the classes are linearly separable.
Decision boundaries are not always clear cut. That is, the transition from one class in the feature space to another is not discontinuous, but gradual. This effect is common in fuzzy logic based classification algorithms, where membership in one class or another is ambiguous.
Decision boundaries can be approximations of optimal stopping boundaries.  The decision boundary is the set of points of that hyperplane that pass through zero.  For example, the angle between a vector and points in a set must be zero for points that are on or close to the decision boundary. Decision boundary instability can be incorporated with generalization error as a standard for selecting the most accurate and stable classifier."
Decision boundary,"In a statistical-classification problem with two classes, a decision boundary or decision surface is a hypersurface that partitions the underlying vector space into two sets, one for each class. The classifier will classify all the points on one side of the decision boundary as belonging to one class and all those on the other side as belonging to the other class.
A decision boundary is the region of a problem space in which the output label of a classifier is ambiguous.If the decision surface is a hyperplane, then the classification problem is linear, and the classes are linearly separable.
Decision boundaries are not always clear cut. That is, the transition from one class in the feature space to another is not discontinuous, but gradual. This effect is common in fuzzy logic based classification algorithms, where membership in one class or another is ambiguous.
Decision boundaries can be approximations of optimal stopping boundaries.  The decision boundary is the set of points of that hyperplane that pass through zero.  For example, the angle between a vector and points in a set must be zero for points that are on or close to the decision boundary. Decision boundary instability can be incorporated with generalization error as a standard for selecting the most accurate and stable classifier."
K-nearest neighbors algorithm,"In statistics, the k-nearest neighbors algorithm (k-NN) is a non-parametric supervised learning method first developed by Evelyn Fix and Joseph Hodges in 1951, and later expanded by Thomas Cover. It is used for classification and regression. In both cases, the input consists of the k closest training examples in a data set. The output depends on whether k-NN is used for classification or regression:

In k-NN classification, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.In k-NN regression, the output is the property value for the object. This value is the average of the values of k nearest neighbors.k-NN is a type of classification where the function is only approximated locally and all computation is deferred until function evaluation. Since this algorithm relies on distance for classification, if the features represent different physical units or come in vastly different scales then normalizing the training data can improve its accuracy dramatically.Both for classification and regression, a useful technique can be to assign weights to the contributions of the neighbors, so that the nearer neighbors contribute more to the average than the more distant ones. For example, a common weighting scheme consists in giving each neighbor a weight of 1/d, where d is the distance to the neighbor.The neighbors are taken from a set of objects for which the class (for k-NN classification) or the object property value (for k-NN regression) is known. This can be thought of as the training set for the algorithm, though no explicit training step is required.
A peculiarity of the k-NN algorithm is that it is sensitive to the local structure of the data."
Weak supervision,"Weak supervision is a branch of machine learning where noisy, limited, or imprecise sources are used to provide supervision signal for labeling large amounts of training data in a supervised learning setting. This approach alleviates the burden of obtaining hand-labeled data sets, which can be costly or impractical. Instead, inexpensive weak labels are employed with the understanding that they are imperfect, but can nonetheless be used to create a strong predictive model."
Perceptron,"In machine learning, the perceptron (or McCulloch-Pitts neuron) is an algorithm for supervised learning of binary classifiers.  A binary classifier is a function which can decide whether or not an input, represented by a vector of numbers, belongs to some specific class.  It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector."
Quantization (signal processing),"Quantization, in mathematics and digital signal processing, is the process of mapping input values from a large set (often a continuous set) to output values in a (countable) smaller set, often with a finite number of elements.  Rounding and truncation are typical examples of quantization processes.  Quantization is involved to some degree in nearly all digital signal processing, as the process of representing a signal in digital form ordinarily involves rounding.  Quantization also forms the core of essentially all lossy compression algorithms.
The difference between an input value and its quantized value (such as round-off error) is referred to as quantization error.  A device or algorithmic function that performs quantization is called a quantizer.  An analog-to-digital converter is an example of a quantizer."
Hyperplane,"In geometry, a hyperplane is a subspace whose dimension is one less than that of its ambient space.  For example, if a space is 3-dimensional then its hyperplanes are the 2-dimensional planes, while if the space is 2-dimensional, its hyperplanes are the 1-dimensional lines.  This notion can be used in any general space in which the concept of the dimension of a subspace is defined.
In different settings, hyperplanes may have different properties.  For instance, a hyperplane of an n-dimensional affine space is a flat subset with dimension n − 1 and it separates the space into two half spaces. While a hyperplane of an n-dimensional projective space does not have this property.
The difference in dimension between a subspace S and its ambient space X is known as the codimension of S with respect to X.  Therefore, a necessary and sufficient condition for S to be a hyperplane in X is for S to have codimension one in X.

"
Kernel method,"In machine learning, kernel machines are a class of algorithms for pattern analysis, whose best known member is the support-vector machine (SVM). The general task of pattern analysis is to find and study general types of relations (for example clusters, rankings, principal components, correlations, classifications) in datasets. For many algorithms that solve these tasks, the data in raw representation have to be explicitly transformed into feature vector representations via a user-specified feature map: in contrast, kernel methods require only a user-specified kernel, i.e., a similarity function over all pairs of data points computed using Inner products. The feature map in kernel machines is infinite dimensional but only requires a finite dimensional matrix from user-input according to the Representer theorem. Kernel machines are slow to compute for datasets larger than a couple of thousand examples without parallel processing. 
Kernel methods  owe their name to the use of kernel functions, which enable them to operate in a high-dimensional, implicit feature space without ever computing the coordinates of the data in that space, but rather by simply computing the inner products between the images of all pairs of data in the feature space.  This operation is often computationally cheaper than the explicit computation of the coordinates.  This approach is called the ""kernel trick"". Kernel functions have been introduced for sequence data, graphs, text, images, as well as vectors.
Algorithms capable of operating with kernels include the kernel perceptron, support-vector machines (SVM), Gaussian processes, principal components analysis (PCA), canonical correlation analysis, ridge regression, spectral clustering, linear adaptive filters and many others.
Most kernel algorithms are based on convex optimization or eigenproblems and are statistically well-founded. Typically, their statistical properties are analyzed using statistical learning theory (for example, using Rademacher complexity)."
machine learning methods,"Machine learning (ML) is a field of inquiry devoted to understanding and building methods that 'learn', that is, methods that leverage data to improve performance on some set of tasks. It is seen as a part of artificial intelligence. Machine learning algorithms build a model based on sample data, known as training data, in order to make predictions or decisions without being explicitly programmed to do so. Machine learning algorithms are used in a wide variety of applications, such as in medicine, email filtering, speech recognition, agriculture, and computer vision, where it is difficult or unfeasible to develop conventional algorithms to perform the needed tasks. 
A subset of machine learning is closely related to computational statistics, which focuses on making predictions using computers, but not all machine learning is statistical learning. The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning. Data mining is a related field of study, focusing on exploratory data analysis through unsupervised learning. Some implementations of machine learning use data and neural networks in a way that mimics the working of a biological brain.  In its application across business problems, machine learning is also referred to as predictive analytics."
flat clustering,"Document clustering (or text clustering) is the application of cluster analysis to textual documents. It has applications in automatic document organization, topic extraction and fast information retrieval or filtering."
Document clustering,"Document clustering (or text clustering) is the application of cluster analysis to textual documents. It has applications in automatic document organization, topic extraction and fast information retrieval or filtering."
Chinese whispers (clustering method),"Chinese whispers is a clustering method used in network science named after the famous whispering game. Clustering methods are basically used to identify communities of nodes or links in a given network. This algorithm was designed by Chris Biemann and Sven Teresniak in 2005. The name comes from the fact that the process can be modeled as a separation of communities where the nodes send the same type of information to each other.Chinese whispers is a hard partitioning, randomized, flat clustering (no hierarchical relations between clusters) method. The random property means that running the process on the same network several times can lead to different results, while because of hard partitioning one node can only belong to one cluster at a given moment.  The original algorithm is applicable to undirected, weighted and unweighted graphs. Chinese whispers is time linear which means that it is extremely fast even if the number of nodes and links are very high in the network.

"
K-means clustering,"k-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells. k-means clustering minimizes within-cluster variances (squared Euclidean distances), but not regular Euclidean distances, which would be the more difficult Weber problem: the mean optimizes squared errors, whereas only the geometric median minimizes Euclidean distances. For instance, better Euclidean solutions can be found using k-medians and k-medoids.
The problem is computationally difficult (NP-hard); however, efficient heuristic algorithms converge quickly to a local optimum. These are usually similar to the expectation-maximization algorithm for mixtures of Gaussian distributions via an iterative refinement approach employed by both k-means and Gaussian mixture modeling. They both use cluster centers to model the data; however, k-means clustering tends to find clusters of comparable spatial extent, while the Gaussian mixture model allows clusters to have different shapes.
The unsupervised k-means algorithm has a loose relationship to the k-nearest neighbor classifier, a popular supervised machine learning technique for classification that is often confused with k-means due to the name. Applying the 1-nearest neighbor classifier to the cluster centers obtained by k-means classifies new data into the existing clusters. This is known as nearest centroid classifier or Rocchio algorithm.

"
cluster hypothesis,"In machine learning and information retrieval, the cluster hypothesis is an assumption about the nature of the data handled in those fields, which takes various forms. In information retrieval, it states that documents that are clustered together ""behave similarly with respect to relevance to information needs"". In terms of classification, it states that if points are in the same cluster, they are likely to be of the same class. There may be multiple clusters forming a single class."
Cluster hypothesis,"In machine learning and information retrieval, the cluster hypothesis is an assumption about the nature of the data handled in those fields, which takes various forms. In information retrieval, it states that documents that are clustered together ""behave similarly with respect to relevance to information needs"". In terms of classification, it states that if points are in the same cluster, they are likely to be of the same class. There may be multiple clusters forming a single class."
Relevance (information retrieval),"In information science and information retrieval, relevance denotes how well a retrieved document or set of documents meets the information need of the user. Relevance may include concerns such as timeliness, authority or novelty of the result."
Null hypothesis,"In scientific research, the null hypothesis (often denoted H0) is the claim that no difference or relationship exists between two sets of data or variables being analyzed. The null hypothesis is that any experimentally observed difference is due to chance alone, and an underlying causative relationship does not exist, hence the term ""null"". In addition to the null hypothesis, an alternative hypothesis is also developed, which claims that a relationship does exist between two variables."
Nearest centroid classifier,"In machine learning, a nearest centroid classifier or nearest prototype classifier is a classification model that assigns to observations the label of the class of training samples whose mean (centroid) is closest to the observation. When applied to text classification using word vectors containing tf*idf weights to represent documents, the nearest centroid classifier is known as the Rocchio classifier because of its similarity to the Rocchio algorithm for relevance feedback.An extended version of the nearest centroid classifier has found applications in the medical domain, specifically classification of tumors."
Statistical hypothesis testing,"A statistical hypothesis test is a method of statistical inference used to decide whether the data at hand sufficiently support a particular hypothesis.
Hypothesis testing allows us to make probabilistic statements about population parameters."
objective functions,"In mathematical optimization and decision theory, a loss function or cost function (sometimes also called an error function)  is a function that maps an event or values of one or more variables onto a real number intuitively representing some ""cost"" associated with the event. An optimization problem seeks to minimize a loss function. An objective function is either a loss function or its opposite (in specific domains, variously called a reward function, a profit function, a utility function, a fitness function, etc.), in which case it is to be maximized. The loss function could include terms from several levels of the hierarchy.
In statistics, typically a  loss function is used for parameter estimation, and the event in question is some function of the difference between estimated and true values for an instance of data. The concept, as old as Laplace, was reintroduced in statistics by Abraham Wald in the middle of the 20th century.  In the context of economics, for example, this is usually economic cost or regret.  In classification, it is the penalty for an incorrect classification of an example. In actuarial science, it is used in an insurance context to model benefits paid over premiums, particularly since the works of Harald Cramér in the 1920s. In optimal control, the loss is the penalty for failing to achieve a desired value. In financial risk management, the function is mapped to a monetary loss."
Loss function,"In mathematical optimization and decision theory, a loss function or cost function (sometimes also called an error function)  is a function that maps an event or values of one or more variables onto a real number intuitively representing some ""cost"" associated with the event. An optimization problem seeks to minimize a loss function. An objective function is either a loss function or its opposite (in specific domains, variously called a reward function, a profit function, a utility function, a fitness function, etc.), in which case it is to be maximized. The loss function could include terms from several levels of the hierarchy.
In statistics, typically a  loss function is used for parameter estimation, and the event in question is some function of the difference between estimated and true values for an instance of data. The concept, as old as Laplace, was reintroduced in statistics by Abraham Wald in the middle of the 20th century.  In the context of economics, for example, this is usually economic cost or regret.  In classification, it is the penalty for an incorrect classification of an example. In actuarial science, it is used in an insurance context to model benefits paid over premiums, particularly since the works of Harald Cramér in the 1920s. In optimal control, the loss is the penalty for failing to achieve a desired value. In financial risk management, the function is mapped to a monetary loss."
Test functions for optimization,"In applied mathematics, test functions, known as artificial landscapes, are useful to evaluate characteristics of optimization algorithms, such as:

Convergence rate.
Precision.
Robustness.
General performance.Here some test functions are presented with the aim of giving an idea about the different situations that optimization algorithms have to face when coping with these kinds of problems. In the first part, some objective functions for single-objective optimization cases are presented. In the second part, test functions with their respective Pareto fronts for multi-objective optimization problems (MOP) are given.
The artificial landscapes presented herein for single-objective optimization problems are taken from Bäck, Haupt et al. and from Rody Oldenhuis software. Given the number of problems (55 in total), just a few are presented here.
The test functions used to evaluate the algorithms for MOP were taken from Deb, Binh et al. and Binh. The software developed by Deb can be downloaded, which implements the NSGA-II procedure with GAs, or the program posted on Internet, which implements the NSGA-II procedure with ES.
Just a general form of the equation, a plot of the objective function, boundaries of the object variables and the coordinates of global minima are given herein."
Multi-objective optimization,"Multi-objective optimization (also known as multi-objective programming, vector optimization, multicriteria optimization, multiattribute optimization or Pareto optimization) is an area of multiple criteria decision making that is concerned with mathematical optimization problems involving more than one objective function to be optimized simultaneously. Multi-objective optimization has been applied in many fields of science, including engineering, economics and logistics where optimal decisions need to be taken in the presence of trade-offs between two or more conflicting objectives. Minimizing cost while maximizing comfort while buying a car, and maximizing performance whilst minimizing fuel consumption and emission of pollutants of a vehicle are examples of multi-objective optimization problems involving two and three objectives, respectively. In practical problems, there can be more than three objectives.
For a nontrivial multi-objective optimization problem, no single solution exists that simultaneously optimizes each objective. In that case, the objective functions are said to be conflicting. A solution is called nondominated, Pareto optimal, Pareto efficient or noninferior, if none of the objective functions can be improved in value without degrading some of the other objective values. Without additional subjective preference information, there may exist a (possibly infinite) number of Pareto optimal solutions, all of which are considered equally good. Researchers study multi-objective optimization problems from different viewpoints and, thus, there exist different solution philosophies and goals when setting and solving them. The goal may be to find a representative set of Pareto optimal solutions, and/or quantify the trade-offs in satisfying the different objectives, and/or finding a single solution that satisfies the subjective preferences of a human decision maker (DM).
Bicriteria optimization denotes the special case in which there are two objective functions."
Production (economics),"Production is the process of combining various inputs, both material (such as metal, wood, glass, or plastics) and immaterial (such as plans, or knowledge) in order to create output. Ideally this output will be a good or service which has value and contributes to the utility of individuals. The area of economics that focuses on production is called production theory, and it is closely related to the consumption (or consumer) theory of economics.

The production process and output directly result from productively utilising the original inputs (or factors of production). Known as primary producer goods or services, land, labour, and capital are deemed the three fundamental production factors. These primary inputs are not significantly altered in the output process, nor do they become a whole component in the product. Under classical economics, materials and energy are categorised as secondary factors as they are byproducts of land, labour and capital. Delving further, primary factors encompass all of the resourcing involved, such as land, which includes the natural resources above and below the soil. However, there is a difference in human capital and labour. In addition to the common factors of production, in different economic schools of thought, entrepreneurship and technology are sometimes considered evolved factors in production. It is common practice that several forms of controllable inputs are used to achieve the output of a product. The production function assesses the relationship between the inputs and the quantity of output.Economic well-being is created in a production process, meaning all economic activities that aim directly or indirectly to satisfy human wants and needs. The degree to which the needs are satisfied is often accepted as a measure of economic well-being. In production there are two features which explain increasing economic well-being. They are improving quality-price-ratio of goods and services and increasing incomes from growing and more efficient market production or total production which help in increasing GDP.
The most important forms of production are:

market production
public production
household productionIn order to understand the origin of economic well-being, we must understand these three production processes. All of them produce commodities which have value and contribute to the well-being of individuals.
The satisfaction of needs originates from the use of the commodities which are produced. The need satisfaction increases when the quality-price-ratio of the commodities improves and more satisfaction is achieved at less cost. Improving the quality-price-ratio of commodities is to a producer an essential way to improve the competitiveness of products but this kind of gains distributed to customers cannot be measured with production data. Improving the competitiveness of products means often to the producer lower product prices and therefore losses in incomes which are to be compensated with the growth of sales volume.
Economic well-being also increases due to the growth of incomes that are gained from the growing and more efficient market production. Market production is the only production form that creates and distributes incomes to stakeholders. Public production and household production are financed by the incomes generated in market production. Thus market production has a double role in creating well-being, i.e. the role of producing goods and services and the role of creating income. Because of this double role, market production is the “primus motor” of economic well-being and therefore here under review."
Mathematical optimization,"Mathematical optimization (alternatively spelled optimisation) or mathematical programming is the selection of a best element, with regard to some criterion, from some set of available alternatives. It is generally divided into two subfields: discrete optimization and continuous optimization. Optimization problems of sorts arise in all quantitative disciplines from computer science and engineering to operations research and economics, and the development of solution methods has been of interest in mathematics for centuries.In the more general approach, an optimization problem consists of maximizing or minimizing a real function by systematically choosing input values from within an allowed set and computing the value of the function. The generalization of optimization theory and techniques to other formulations constitutes a large area of applied mathematics. More generally, optimization includes finding ""best available"" values of some objective function given a defined domain (or input), including a variety of different types of objective functions and different types of domains."
Linear programming,"Linear programming (LP), also called linear optimization, is a method to achieve the best outcome (such as maximum profit or lowest cost) in a mathematical model whose requirements are represented by linear relationships. Linear programming is a special case of mathematical programming (also known as mathematical optimization).
More formally, linear programming is a technique for the optimization of a linear objective function, subject to linear equality and linear inequality constraints. Its feasible region is a convex polytope, which is a set defined as the intersection of finitely many half spaces, each of which is defined by a linear inequality. Its objective function is a real-valued affine (linear) function defined on this polyhedron. A linear programming algorithm finds a point in the polytope where this function has the smallest (or largest) value if such a point exists.
Linear programs are problems that can be expressed in canonical form as

  
    
      
        
          
            
              
              
                
                  Find a vector
                
              
              
              
                
                  x
                
              
            
            
              
              
                
                  that maximizes
                
              
              
              
                
                  
                    c
                  
                  
                    T
                  
                
                
                  x
                
              
            
            
              
              
                
                  subject to
                
              
              
              
                A
                
                  x
                
                ≤
                
                  b
                
              
            
            
              
              
                
                  and
                
              
              
              
                
                  x
                
                ≥
                
                  0
                
                .
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}&{\text{Find a vector}}&&\mathbf {x} \\&{\text{that maximizes}}&&\mathbf {c} ^{T}\mathbf {x} \\&{\text{subject to}}&&A\mathbf {x} \leq \mathbf {b} \\&{\text{and}}&&\mathbf {x} \geq \mathbf {0} .\end{aligned}}}
  Here the components of x are the variables to be determined, c and b are given vectors (with 
  
    
      
        
          
            c
          
          
            T
          
        
      
    
    {\displaystyle \mathbf {c} ^{T}}
   indicating that the coefficients of c are used as a single-row matrix for the purpose of forming the matrix product), and A is a given matrix. The function whose value is to be maximized or minimized (
  
    
      
        
          x
        
        ↦
        
          
            c
          
          
            T
          
        
        
          x
        
      
    
    {\displaystyle \mathbf {x} \mapsto \mathbf {c} ^{T}\mathbf {x} }
   in this case) is called the objective function. The inequalities Ax ≤ b and x ≥ 0 are the constraints which specify a convex polytope over which the objective function is to be optimized. In this context, two vectors are comparable when they have the same dimensions. If every entry in the first is less-than or equal-to the corresponding entry in the second, then it can be said that the first vector is less-than or equal-to the second vector.
Linear programming can be applied to various fields of study. It is widely used in mathematics and, to a lesser extent, in business, economics, and some engineering problems. Industries that use linear programming models include transportation, energy, telecommunications, and manufacturing. It has proven useful in modeling diverse types of problems in planning, routing, scheduling, assignment, and design."
flat clustering algorithm,"k-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells. k-means clustering minimizes within-cluster variances (squared Euclidean distances), but not regular Euclidean distances, which would be the more difficult Weber problem: the mean optimizes squared errors, whereas only the geometric median minimizes Euclidean distances. For instance, better Euclidean solutions can be found using k-medians and k-medoids.
The problem is computationally difficult (NP-hard); however, efficient heuristic algorithms converge quickly to a local optimum. These are usually similar to the expectation-maximization algorithm for mixtures of Gaussian distributions via an iterative refinement approach employed by both k-means and Gaussian mixture modeling. They both use cluster centers to model the data; however, k-means clustering tends to find clusters of comparable spatial extent, while the Gaussian mixture model allows clusters to have different shapes.
The unsupervised k-means algorithm has a loose relationship to the k-nearest neighbor classifier, a popular supervised machine learning technique for classification that is often confused with k-means due to the name. Applying the 1-nearest neighbor classifier to the cluster centers obtained by k-means classifies new data into the existing clusters. This is known as nearest centroid classifier or Rocchio algorithm.

"
K-means clustering,"k-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells. k-means clustering minimizes within-cluster variances (squared Euclidean distances), but not regular Euclidean distances, which would be the more difficult Weber problem: the mean optimizes squared errors, whereas only the geometric median minimizes Euclidean distances. For instance, better Euclidean solutions can be found using k-medians and k-medoids.
The problem is computationally difficult (NP-hard); however, efficient heuristic algorithms converge quickly to a local optimum. These are usually similar to the expectation-maximization algorithm for mixtures of Gaussian distributions via an iterative refinement approach employed by both k-means and Gaussian mixture modeling. They both use cluster centers to model the data; however, k-means clustering tends to find clusters of comparable spatial extent, while the Gaussian mixture model allows clusters to have different shapes.
The unsupervised k-means algorithm has a loose relationship to the k-nearest neighbor classifier, a popular supervised machine learning technique for classification that is often confused with k-means due to the name. Applying the 1-nearest neighbor classifier to the cluster centers obtained by k-means classifies new data into the existing clusters. This is known as nearest centroid classifier or Rocchio algorithm.

"
Expectation–maximization algorithm,"In statistics, an expectation–maximization (EM) algorithm is an iterative method to find (local) maximum likelihood or maximum a posteriori (MAP) estimates of parameters in statistical models, where the model depends on unobserved latent variables. The EM iteration alternates between performing an expectation (E) step, which creates a function for the expectation of the log-likelihood evaluated using the current estimate for the parameters, and a maximization (M) step, which computes parameters maximizing the expected log-likelihood found on the E step. These parameter-estimates are then used to determine the distribution of the latent variables in the next E step."
Chinese whispers (clustering method),"Chinese whispers is a clustering method used in network science named after the famous whispering game. Clustering methods are basically used to identify communities of nodes or links in a given network. This algorithm was designed by Chris Biemann and Sven Teresniak in 2005. The name comes from the fact that the process can be modeled as a separation of communities where the nodes send the same type of information to each other.Chinese whispers is a hard partitioning, randomized, flat clustering (no hierarchical relations between clusters) method. The random property means that running the process on the same network several times can lead to different results, while because of hard partitioning one node can only belong to one cluster at a given moment.  The original algorithm is applicable to undirected, weighted and unweighted graphs. Chinese whispers is time linear which means that it is extremely fast even if the number of nodes and links are very high in the network.

"
Consensus clustering,"Consensus clustering is a method of aggregating (potentially conflicting) results from multiple clustering algorithms. Also called cluster ensembles or aggregation of clustering (or partitions), it refers to the situation in which a number of different (input) clusterings have been obtained for a particular dataset and it is desired to find a single (consensus) clustering which is a better fit in some sense than the existing clusterings. Consensus clustering is thus the problem of reconciling clustering information about the same data set coming from different sources or from different runs of the same algorithm. When cast as an optimization problem, consensus clustering is known as median partition, and has been shown to be NP-complete, even when the number of input clusterings is three. Consensus clustering for unsupervised learning is analogous to ensemble learning in supervised learning."
Document clustering,"Document clustering (or text clustering) is the application of cluster analysis to textual documents. It has applications in automatic document organization, topic extraction and fast information retrieval or filtering."
model-based clustering,"Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters). It is a main task of exploratory data analysis, and a common technique for statistical data analysis, used in many fields, including pattern recognition, image analysis, information retrieval, bioinformatics, data compression, computer graphics and machine learning.
Cluster analysis itself is not one specific algorithm, but the general task to be solved. It can be achieved by various algorithms that differ significantly in their understanding of what constitutes a cluster and how to efficiently find them. Popular notions of clusters include groups with small distances between cluster members, dense areas of the data space, intervals or particular statistical distributions. Clustering can therefore be formulated as a multi-objective optimization problem. The appropriate clustering algorithm and parameter settings (including parameters such as the distance function to use, a density threshold or the number of expected clusters) depend on the individual data set and intended use of the results. Cluster analysis as such is not an automatic task, but an iterative process of knowledge discovery or interactive multi-objective optimization that involves trial and failure. It is often necessary to modify data preprocessing and model parameters until the result achieves the desired properties.
Besides the term clustering, there is a number of terms with similar meanings, including automatic classification, numerical taxonomy, botryology (from Greek βότρυς ""grape""), typological analysis, and community detection. The subtle differences are often in the use of the results: while in data mining, the resulting groups are the matter of interest, in automatic classification the resulting discriminative power is of interest.
Cluster analysis was originated in anthropology by Driver and Kroeber in 1932 and introduced to psychology by Joseph Zubin in 1938 and Robert Tryon in 1939 and famously used by Cattell beginning in 1943 for trait theory classification in personality psychology."
Cluster analysis,"Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters). It is a main task of exploratory data analysis, and a common technique for statistical data analysis, used in many fields, including pattern recognition, image analysis, information retrieval, bioinformatics, data compression, computer graphics and machine learning.
Cluster analysis itself is not one specific algorithm, but the general task to be solved. It can be achieved by various algorithms that differ significantly in their understanding of what constitutes a cluster and how to efficiently find them. Popular notions of clusters include groups with small distances between cluster members, dense areas of the data space, intervals or particular statistical distributions. Clustering can therefore be formulated as a multi-objective optimization problem. The appropriate clustering algorithm and parameter settings (including parameters such as the distance function to use, a density threshold or the number of expected clusters) depend on the individual data set and intended use of the results. Cluster analysis as such is not an automatic task, but an iterative process of knowledge discovery or interactive multi-objective optimization that involves trial and failure. It is often necessary to modify data preprocessing and model parameters until the result achieves the desired properties.
Besides the term clustering, there is a number of terms with similar meanings, including automatic classification, numerical taxonomy, botryology (from Greek βότρυς ""grape""), typological analysis, and community detection. The subtle differences are often in the use of the results: while in data mining, the resulting groups are the matter of interest, in automatic classification the resulting discriminative power is of interest.
Cluster analysis was originated in anthropology by Driver and Kroeber in 1932 and introduced to psychology by Joseph Zubin in 1938 and Robert Tryon in 1939 and famously used by Cattell beginning in 1943 for trait theory classification in personality psychology."
Brown clustering,"Brown clustering is a hard hierarchical agglomerative clustering problem based on distributional information proposed by Peter Brown, William A. Brown, Vincent Della Pietra, Peter V. de Souza, Jennifer Lai, and Robert Mercer. It is typically applied to text, grouping words into clusters that are assumed to be semantically related by virtue of their having been embedded in similar contexts."
Mixture model,"In statistics, a mixture model is a probabilistic model for representing the presence of subpopulations within an overall population, without requiring that an observed data set should identify the sub-population to which an individual observation belongs. Formally a mixture model corresponds to the mixture distribution that represents the probability distribution of observations in the overall population. However, while problems associated with ""mixture distributions"" relate to deriving the properties of the overall population from those of the sub-populations, ""mixture models"" are used to make statistical inferences about the properties of the sub-populations given only observations on the pooled population, without sub-population identity information.
Mixture models should not be confused with models for compositional data, i.e., data whose components are constrained to sum to a constant value (1, 100%, etc.). However, compositional models can be thought of as mixture models, where members of the population are sampled at random. Conversely, mixture models can be thought of as compositional models, where the total size reading population has been normalized to 1."
K-means clustering,"k-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells. k-means clustering minimizes within-cluster variances (squared Euclidean distances), but not regular Euclidean distances, which would be the more difficult Weber problem: the mean optimizes squared errors, whereas only the geometric median minimizes Euclidean distances. For instance, better Euclidean solutions can be found using k-medians and k-medoids.
The problem is computationally difficult (NP-hard); however, efficient heuristic algorithms converge quickly to a local optimum. These are usually similar to the expectation-maximization algorithm for mixtures of Gaussian distributions via an iterative refinement approach employed by both k-means and Gaussian mixture modeling. They both use cluster centers to model the data; however, k-means clustering tends to find clusters of comparable spatial extent, while the Gaussian mixture model allows clusters to have different shapes.
The unsupervised k-means algorithm has a loose relationship to the k-nearest neighbor classifier, a popular supervised machine learning technique for classification that is often confused with k-means due to the name. Applying the 1-nearest neighbor classifier to the cluster centers obtained by k-means classifies new data into the existing clusters. This is known as nearest centroid classifier or Rocchio algorithm.

"
Spectral clustering,"In multivariate statistics, spectral clustering techniques make use of the spectrum (eigenvalues) of the similarity matrix of the data to perform dimensionality reduction before clustering in fewer dimensions. The similarity matrix is provided as an input and consists of a quantitative assessment of the relative similarity of each pair of points in the dataset.
In application to image segmentation, spectral clustering is known as segmentation-based object categorization."
Human genetic clustering,"Human genetic clustering refers to patterns of relative genetic similarity among human individuals and populations, as well as the wide range of scientific and statistical methods used to study this aspect of human genetic variation.
Clustering studies are thought to be valuable for characterizing the general structure of genetic variation among human populations, to contribute to the study of ancestral origins, evolutionary history, and precision medicine. Since the mapping of the human genome, and with the availability of increasingly powerful analytic tools, cluster analyses have revealed a range of ancestral and migratory trends among human populations and individuals. Human genetic clusters tend to be organized by geographic ancestry, with divisions between clusters aligning largely with geographic barriers such as oceans or mountain ranges. Clustering studies have been applied to global populations, as well as to population subsets like post-colonial North America. Notably, the practice of defining clusters among modern human populations is largely arbitrary and variable due to the continuous nature of human genotypes; although individual genetic markers can be used to produce smaller groups, there are no models that produce completely distinct subgroups when larger numbers of genetic markers are used.Many studies of human genetic clustering have been implicated in discussions of race, ethnicity, and scientific racism, as some have controversially suggested that genetically derived clusters may be understood as proof of genetically determined races. Although cluster analyses invariably organize humans (or groups of humans) into subgroups, debate is ongoing on how to interpret these genetic clusters with respect to race and its social and phenotypic features. And, because there is such a small fraction of genetic variation between human genotypes overall, genetic clustering approaches are highly dependent on the sampled data, genetic markers, and statistical methods applied to their construction."
Fuzzy clustering,"Fuzzy clustering (also referred to as soft clustering or soft k-means) is a form of clustering in which each data point can belong to more than one cluster.
Clustering or cluster analysis involves assigning data points to clusters such that items in the same cluster are as similar as possible, while items belonging to different clusters are as dissimilar as possible. Clusters are identified via similarity measures. These similarity measures include distance, connectivity, and intensity. Different similarity measures may be chosen based on the data or the application."
Agent-based model,"An agent-based model (ABM) is a computational model for simulating the actions and interactions of autonomous agents (both individual or collective entities such as organizations or groups) in order to understand the behavior of a system and what governs its outcomes. It combines elements of game theory, complex systems, emergence, computational sociology, multi-agent systems, and evolutionary programming. Monte Carlo methods are used to understand the stochasticity of these models.  Particularly within ecology, ABMs are also called individual-based models (IBMs). A review of recent literature on individual-based models, agent-based models, and multiagent systems shows that ABMs are used in many scientific domains including biology, ecology and social science. Agent-based modeling is related to, but distinct from, the concept of multi-agent systems or multi-agent simulation in that the goal of ABM is to search for explanatory insight into the collective behavior of agents obeying simple rules, typically in natural systems, rather than in designing agents or solving specific practical or engineering problems.Agent-based models are a kind of microscale model that simulate the simultaneous operations and interactions of multiple agents in an attempt to re-create and predict the appearance of complex phenomena. The process is one of emergence, which some express as ""the whole is greater than the sum of its parts"". In other words, higher-level system properties emerge from the interactions of lower-level subsystems. Or, macro-scale state changes emerge from micro-scale agent behaviors. Or, simple behaviors (meaning rules followed by agents) generate complex behaviors (meaning state changes at the whole system level).
Individual agents are typically characterized as boundedly rational, presumed to be acting in what they perceive as their own interests, such as reproduction, economic benefit, or social status, using heuristics or simple decision-making rules. ABM agents may experience ""learning"", adaptation, and reproduction.Most agent-based models are composed of: (1) numerous agents specified at various scales (typically referred to as agent-granularity); (2) decision-making heuristics; (3) learning rules or adaptive processes; (4) an interaction topology; and (5) an environment. ABMs are typically implemented as computer simulations, either as custom software, or via ABM toolkits, and this software can be then used to test how changes in individual behaviors will affect the system's emerging overall behavior."
Hierarchical clustering,"In data mining and statistics, hierarchical clustering (also called hierarchical cluster analysis or HCA) is a method of cluster analysis that seeks to build a hierarchy of clusters. Strategies for hierarchical clustering generally fall into two categories:

Agglomerative: This is a ""bottom-up"" approach: Each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.
Divisive: This is a ""top-down"" approach: All observations start in one cluster, and splits are performed recursively as one moves down the hierarchy.In general, the merges and splits are determined in a greedy manner. The results of hierarchical clustering are usually presented in a dendrogram.
The standard algorithm for hierarchical agglomerative clustering (HAC) has a time complexity of 
  
    
      
        
          
            O
          
        
        (
        
          n
          
            3
          
        
        )
      
    
    {\displaystyle {\mathcal {O}}(n^{3})}
   and requires 
  
    
      
        Ω
        (
        
          n
          
            2
          
        
        )
      
    
    {\displaystyle \Omega (n^{2})}
   memory, which makes it too slow for even medium data sets. However, for some special cases, optimal efficient agglomerative methods (of complexity 
  
    
      
        
          
            O
          
        
        (
        
          n
          
            2
          
        
        )
      
    
    {\displaystyle {\mathcal {O}}(n^{2})}
  ) are known: SLINK for single-linkage and CLINK for complete-linkage clustering. With a heap, the runtime of the general case can be reduced to 
  
    
      
        
          
            O
          
        
        (
        
          n
          
            2
          
        
        log
        ⁡
        n
        )
      
    
    {\displaystyle {\mathcal {O}}(n^{2}\log n)}
  , an improvement on the aforementioned bound of 
  
    
      
        
          
            O
          
        
        (
        
          n
          
            3
          
        
        )
      
    
    {\displaystyle {\mathcal {O}}(n^{3})}
  , at the cost of further increasing the memory requirements. In many cases, the memory overheads of this approach are too large to make it practically usable.
Except for the special case of single-linkage, none of the algorithms (except exhaustive search in 
  
    
      
        
          
            O
          
        
        (
        
          2
          
            n
          
        
        )
      
    
    {\displaystyle {\mathcal {O}}(2^{n})}
  ) can be guaranteed to find the optimum solution.
Divisive clustering with an exhaustive search is 
  
    
      
        
          
            O
          
        
        (
        
          2
          
            n
          
        
        )
      
    
    {\displaystyle {\mathcal {O}}(2^{n})}
  , but it is common to use faster heuristics to choose splits, such as k-means.
Hierarchical clustering has the distinct advantage that any valid measure of distance can be used. In fact, the observations themselves are not required: all that is used is a matrix of distances."
Single-linkage clustering,"In statistics, single-linkage clustering is one of several methods of hierarchical clustering. It is based on grouping clusters in bottom-up fashion (agglomerative clustering), at each step combining two clusters that contain the closest pair of elements not yet belonging to the same cluster as each other.
A drawback of this method is that it tends to produce long thin clusters in which nearby elements of the same cluster have small distances, but elements at opposite ends of a cluster may be much farther from each other than two elements of other clusters. This may lead to difficulties in defining classes that could usefully subdivide the data."
centroids,"In mathematics and physics, the centroid, also known as geometric center or center of figure, of a plane figure or solid figure is the arithmetic mean position of all the points in the surface of the figure. The same definition extends to any object in n-dimensional Euclidean space.In geometry, one often assumes uniform mass density, in which case the barycenter or center of mass coincides with the centroid. Informally, it can be understood as the point at which a cutout of the shape (with uniformly distributed mass) could be perfectly balanced on the tip of a pin.In physics, if variations in gravity are considered, then a center of gravity can be defined as the weighted mean of all points weighted by their specific weight.
In geography, the centroid of a radial projection of a region of the Earth's surface to sea level is the region's geographical center."
Centroid,"In mathematics and physics, the centroid, also known as geometric center or center of figure, of a plane figure or solid figure is the arithmetic mean position of all the points in the surface of the figure. The same definition extends to any object in n-dimensional Euclidean space.In geometry, one often assumes uniform mass density, in which case the barycenter or center of mass coincides with the centroid. Informally, it can be understood as the point at which a cutout of the shape (with uniformly distributed mass) could be perfectly balanced on the tip of a pin.In physics, if variations in gravity are considered, then a center of gravity can be defined as the weighted mean of all points weighted by their specific weight.
In geography, the centroid of a radial projection of a region of the Earth's surface to sea level is the region's geographical center."
List of centroids,"The following is a list of centroids of various two-dimensional and three-dimensional objects.  The centroid of an object 
  
    
      
        X
      
    
    {\displaystyle X}
   in 
  
    
      
        n
      
    
    {\displaystyle n}
  -dimensional space is the intersection of all hyperplanes that divide 
  
    
      
        X
      
    
    {\displaystyle X}
   into two parts of equal moment about the hyperplane. Informally, it is the ""average"" of all points of 
  
    
      
        X
      
    
    {\displaystyle X}
  . For an object of uniform composition, the centroid of a body is also its center of mass. In the case of two-dimensional objects shown below, the hyperplanes are simply lines."
Spectral centroid,"The spectral centroid is a measure used in digital signal processing to characterise a spectrum. It indicates where the center of mass of the spectrum is located. Perceptually, it has a robust connection with the impression of brightness of a sound. It is sometimes called center of spectral mass."
K-means clustering,"k-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells. k-means clustering minimizes within-cluster variances (squared Euclidean distances), but not regular Euclidean distances, which would be the more difficult Weber problem: the mean optimizes squared errors, whereas only the geometric median minimizes Euclidean distances. For instance, better Euclidean solutions can be found using k-medians and k-medoids.
The problem is computationally difficult (NP-hard); however, efficient heuristic algorithms converge quickly to a local optimum. These are usually similar to the expectation-maximization algorithm for mixtures of Gaussian distributions via an iterative refinement approach employed by both k-means and Gaussian mixture modeling. They both use cluster centers to model the data; however, k-means clustering tends to find clusters of comparable spatial extent, while the Gaussian mixture model allows clusters to have different shapes.
The unsupervised k-means algorithm has a loose relationship to the k-nearest neighbor classifier, a popular supervised machine learning technique for classification that is often confused with k-means due to the name. Applying the 1-nearest neighbor classifier to the cluster centers obtained by k-means classifies new data into the existing clusters. This is known as nearest centroid classifier or Rocchio algorithm.

"
Pappus's centroid theorem,"In mathematics, Pappus's centroid theorem (also known as  the Guldinus theorem, Pappus–Guldinus theorem or Pappus's theorem) is either of two related theorems dealing with the surface areas and volumes of surfaces and solids of revolution.
The theorems are attributed to Pappus of Alexandria and Paul Guldin. Pappus's statement of this theorem appears in print for the first time in 1659, but it was known before, by Kepler in 1615 and by Guldin in 1640."
List of second moments of area,"The following is a list of second moments of area of some shapes. The second moment of area, also known as area moment of inertia, is a geometrical property of an area which reflects how its points are distributed with respect to an arbitrary axis. The unit of dimension of the second moment of area is length to fourth power, L4, and should not be confused with the mass moment of inertia. If the piece is thin, however, the mass moment of inertia equals the area density times the area moment of inertia."
Lloyd's algorithm,"In electrical engineering and computer science, Lloyd's algorithm, also known as Voronoi iteration or relaxation, is an algorithm named after Stuart P. Lloyd for finding evenly spaced sets of points in subsets of Euclidean spaces and partitions of these subsets into well-shaped and uniformly sized convex cells.  Like the closely related k-means clustering algorithm, it repeatedly finds the centroid of each set in the partition and then re-partitions the input according to which of these centroids is closest. In this setting, the mean operation is an integral over a region of space, and the nearest centroid operation results in Voronoi diagrams.
Although the algorithm may be applied most directly to the Euclidean plane, similar algorithms may also be applied to higher-dimensional spaces or to spaces with other non-Euclidean metrics. Lloyd's algorithm can be used to construct close approximations to centroidal Voronoi tessellations of the input, which can be used for quantization, dithering, and stippling. Other applications of Lloyd's algorithm include smoothing of triangle meshes in the finite element method."
